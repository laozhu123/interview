年龄不是打败你的桎梏，没有足以对抗的优势才是
* 现在的努力，在未来看来都是值得的
** 总结
** DONE 19	架构设计&分布式&数据结构与算法面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/105870730
   CLOSED: [2021-01-28 Thu 08:51]
*** 算法
**** 快排
***** firstSort(int[] arrays, int start, int end){
          if(end - start < 1) return;
          int eindex = end;
          for (int i = start+1;i <= eindex; i++){
              if (arrays[i] > arrays[start]){
                  for (int j = eindex; j > i; j--){
                      if (arrays[j] < arrays[start]){
                          swap(arrays, i , j);
                          eindex = j - 1;
                          break;
                      }
                  }
                  if (arrays[i] > arrays[start]){
                      eindex = i - 1;
                      break;
                  }
              }
          }
          swap(arrays, start, eindex);
          firstSort(arrays, start,eindex-1);
          firstSort(arrays, eindex+1, end);
      }
** 重点
*** 多线程（创建方式、线程池使用）、锁机制(Synchronize[monitor｜对象头中的mark word]、ReentrantLock[unsafe.park])、AQS(同步器sync、fairsync、unfairsync)、CAS
*** 数据库索引、事务隔离级别、锁机制
*** spring容器、aop(动态代理)、ioc
*** jvm内存(内存分布、回收策略)
*** 
** 面试问题
*** 老板让你最最小的代价去设计每日签到和UV、PV统计你就会接触到：位图和HyperLogLog，高速的过滤你就会考虑到：布隆过滤器 (Bloom Filter) ，
    附近的人就会使用到：GeoHash 他的大容量存储有问题，你可能需要去了解Pika
*** lru算法
  LRU 算法实现：1.通过双向链表来实现，新数据插入到链表头部；2.每当缓存命中（即缓存数据被访问），则将数据移到链表头部；3.当链表满的时候，将链表尾部的数据丢弃。
  #+BEGIN_SRC java  
    class LRUCache<K,V> extends LinkedHashMap<K,V>{
        private final int CACHE_SIZE;
        public LRUCache(int cacheSize){
            super((int)Math.ceil(cacheSize/0.75), 0.75f, true); //true:按访问顺序排序 false:按插入顺序排序
            CACHE_SIZE = cacheSize;
        }

        @Override
        protected boolean removeEldestEntry(Map.Entry<K,V> eldest){
            return size() > CACHE_SIZE;
        }
    }
  #+END_SRC 
*** 秒杀系统
**** 业务隔离、活动热场、优惠券、红包、问卷调查、历史数据分析
**** 数据隔离，使用单独的数据库(活动结束后数据合并)
**** redis cluster + 哨兵
**** 库存预热
**** 动态url，每次请求秒杀之前，先传递goodid从服务器获取一个加密过的pathid，该pathid存储在redis缓存中，而后请求秒杀接口+pathid
**** 微服务单一指责
**** 恶意请求在网关层过滤，使用bloomfilter
**** lua脚本实现原子性cas
**** 风控团队介入
***** 在请求到达后端之前，⻛控可以根据账号⾏为分析出这个账号机器⼈的概率⼤不⼤，我现在负责公司的某些特殊系统，每个⽤户的⾏为都是会送到我们⼤数据团队进⾏分析处理，给你打上对应标签的。
**** 压力测试
**** 总结
***** 客户端 90% 静态 HTML+10% 动态 JS；配合 CDN 做好缓存工作。
***** 接入层专注于过滤和限流。nginx+session缓存+限流模块
***** 应用层利用缓存+队列+分布式处理好订单。
***** 做好数据的预估，隔离，合并。
***** 上线之前记得进行压力测试。
*** 单例模式
**** （1）饿汉式：线程安全
public class Singleton{
    private static Singleton instance = new Singleton();
    private Singleton(){}
    public static Singleton newInstance(){
        return instance;
    }
}
**** TODO （2）懒汉式：非线程安全
public class Singleton{
    private static Singleton instance = null;
    private Singleton(){}
    public static synchronized Singleton newInstance(){ //若不添加synchronized，不安全，但添加了之后又影响效率
        if(null == instance){  // Single Checked
            instance = new Singleton();
        }
        return instance;
    }
}
**** （3）双检锁：线程安全
public class Singleton {
    private static volatile Singleton instance = null;
    private Singleton(){}
    public static Singleton getInstance() {
        if (instance == null) { // Single Checked
            synchronized (Singleton.class) {
                if (instance == null) { // Double checked
                    instance = new Singleton();
                }
 
            }
        }
        return instance;
    }
}
**** （4）静态内部类：线程安全(推荐：利用了类加载机制，只要内部类没有被使用到，就不会加载该类)(饿汉模式放到静态内部类中)
public class Singleton{
    private static class SingletonHolder{
        public static Singleton instance = new Singleton();
    }
    private Singleton(){}
    public static Singleton newInstance(){
        return SingletonHolder.instance;
    }
}
**** （5）枚举：线程安全(每个枚举实例都是static final类型的，也就表明只能被实例化一次)(枚举被首次调用时执行private构造方法[创建引用对象]，而后使用public getInstance方法获取)
class Resource{
}
public enum SomeThing {
    INSTANCE;
    private Resource instance;
    private SomeThing() {
        instance = new Resource();
    }
    public Resource getInstance() {
        return instance;
    }
}
SomeThing.INSTANCE.getInstance() 

** DONE 1	Java基础知识面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104390612
   CLOSED: [2021-04-14 Wed 14:51]
*** 数据结构
**** 队列、集合、链表、数组、字典、栈
***** 集合扩充方式
****** 创建时如果不指定容量初始值，Hashtable 默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap 默认的初始化大小为16。之后
         每次扩充，容量变为原来的2倍。②创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为2的幂次方大
         小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）
****** vector 2*n  arraylist 1.5*n
****** arraylist初始容量是10，或者用户设置
****** jdk1.8中hashmap当链表长度大于8时，使用红黑树存储链表数据
*** Java语言有哪些特点
**** 面向对象（封装，继承，多态）
*** Java和C++的区别
**** 都是面向对象的语言，都支持封装、继承和多态
**** Java不提供指针来直接访问内存，程序内存更加安全
**** Java的类是单继承的，C++支持多重继承；虽然Java的类不可以多继承，但是接口可以多继承。
**** Java有自动内存管理机制，不需要程序员手动释放无用内存
*** 面向对象五大基本原则是什么
**** 单一职责原则SRP(Single Responsibility Principle)
***** 类的功能要单一，不能包罗万象，跟杂货铺似的。
**** 开放封闭原则OCP(Open－Close Principle)
***** 一个模块对于拓展是开放的，对于修改是封闭的，想要增加功能热烈欢迎，想要修改，哼，一万个不乐意。
**** 里式替换原则LSP(the Liskov Substitution Principle LSP)
***** 子类可以替换父类出现在父类能够出现的任何地方。比如你能代表你爸去你姥姥家干活。哈哈~~
**** 依赖倒置原则DIP(the Dependency Inversion Principle DIP)
***** 高层次的模块不应该依赖于低层次的模块，他们都应该依赖于抽象。抽象不应该依赖于具体实现，具体实现应该依赖于抽象。就是你出国要说你是中国人，而不能说你是哪个村子的。比如说中国人是抽象的，下面有具体的xx省，xx市，xx县。你要依赖的抽象是中国人，而不是你是xx村的。
**** 接口分离原则ISP(the Interface Segregation Principle ISP)
***** 设计时采用多个与特定客户类有关的接口比采用一个通用的接口要好。就比如一个手机拥有打电话，看视频，玩游戏等功能，把这几个功能拆分成不同的接口，比在一个接口里要好的多。
*** JDK 中常用的包有哪些
**** java.lang：这个是系统的基础类；
**** java.io：这里面是所有输入输出有关的类，比如文件操作等；
**** java.nio：为了完善 io 包中的功能，提高 io 包中性能而写的一个新包；
**** java.net：这里面是与网络有关的类；
**** java.util：这个是系统辅助类，特别是集合类；
**** java.sql：这个是数据库操作的类。
*** java 中 IO 流分为几种?
**** 按照流的流向分，可以分为输入流和输出流；
**** 按照操作单元划分，可以划分为字节流和字符流；
**** 按照流的角色划分为节点流和处理流。
**** InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。
**** OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。
*** 访问范围
**** 访问级别  访问控制修饰符	同类	同包	子类	不同包
**** 公开	public	    √	√	√	√
**** 受保护	protected	√	√	√	×
**** 默认	没有修饰符	√	√	×	×
**** 私有	private     √	×	×	×
*** Collections 和 Arrays工具类常见方法：
**** Collections：reverse、sort、shuffle、swap、rotate、synchronizedCollection、singletonXxx
**** Arrays: sort、binarySearch、equals、fill、asList、toString、copyOf
*** object有方法
**** getclass、hashcode、equals、clone、tostring、notify、notifyAll、wait（3个）、finalize
*** 同步异步（buffer-tv是线程安全的）(lambd8 = arraylist + hashmap + stringbuilder)
**** StringBuffer线程安全、StringBuilder线程不安全
**** HashMap线程不安全，可以用collections.synchronizeMap()来构建线程安全的，HashTable线程安全，但是基于字典较慢
****  HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。。但是在 HashTable 中 put 进的键值只要有一
       个 null，直接抛出 NullPointerException。
**** Vector是线程安全(增长速度100%)，ArrayList线程不安全（增长速度50%），查询用Arraylist、删除和插入用LinkedList,linkedlist是线程不安
       全的
*** 线程、进程、程序
**** 程序是含有指令和数据的文件，被存储在磁盘或其他的数据存储设备中，也就是说程序是静态的代码。 
**** 线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一
       块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量
       级进程
**** 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单
       来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，
       文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。 线程是进程划分成的更小的运行单位。线程和进程最
       大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，
       主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。
*** 线程的几种状态 
**** 初始(NEW)：新创建了一个线程对象，但还没有调用start()方法。
**** 运行(RUNNABLE)：Java线程中将就绪（ready）和运行中（running）两种状态笼统的称为“运行”。线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，
     获取CPU的使用权，此时处于就绪状态（ready）。就绪状态的线程在获得CPU时间片后变为运行中状态（running）。
**** 阻塞(BLOCKED)：表示线程阻塞于锁。
**** 等待(WAITING)：进入该状态的线程需要等待其他线程做出一些特定动作（通知或中断）。
**** 超时等待(TIMED_WAITING)：该状态不同于WAITING，它可以在指定的时间后自行返回。
**** 终止(TERMINATED)：表示该线程已经执行完毕。
*** 一些简单的知识点：
**** 对于不想进行序列化的变量，使用transient关键字修饰。
*** nio、bio、aio
**** 两者都是socket.getInputStream()时线程堵塞的概念
**** bio是blocking io
**** nio是new io，也是 no-blocking io
***** 可以应对高量级的应用请求
***** 在Java 1.4 中引入了 NIO 框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。
***** 它支持面向缓冲的，基于通道的I/O操作方法。 
***** NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。
***** IO 面向流(Stream oriented)，而 NIO 面向缓冲区(Buffer oriented)。
****** 在NIO厍中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的; 在写入数据时，写入到缓冲区中。任何时候访问NIO中的数据，都是通过缓冲区进行操作。
****** 最常用的缓冲区是 ByteBuffer,一个 ByteBuffer 提供了一组功能用于操作 byte 数组。除了ByteBuffer,还有其他的一些缓冲区，事实上，每一种Java基本类型（除了Boolean类型）都对应有一种缓冲区。
***** channel
****** 通道是双向的，可读也可写，而流的读写是单向的。无论读写，通道只能和Buffer交互。因为 Buffer，通道可以异步地读写。
***** Selector (选择器)
****** 选择器用于使用单个线程处理多个通道。因此，它需要较少的线程来处理这些通道。线程之间的切换对于操作系统来说是昂贵的。 因此，为了提高系统效率选择器是有用的。
***** NIO 读数据和写数据方式
****** 通常来说NIO中的所有IO都是从 Channel（通道） 开始的。
****** 从通道进行数据读取 ：创建一个缓冲区，然后请求通道读取数据。
****** 从通道进行数据写入 ：创建一个缓冲区，填充数据，并要求通道写入数据。
***** Netty 的出现很大程度上改善了 JDK 原生 NIO 所存在的一些让人难以忍受的问题
**** AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。
**** Java中提供的IO有关的API，在文件处理的时候，其实依赖操作系统层面的IO操作实现的。
***** 比如在Linux 2.6以后，Java中NIO和AIO都是通过epoll来实现的，而在Windows上，AIO是通过IOCP来实现的。
**** 五种IO模型
***** 阻塞IO模型、非阻塞IO模型、IO复用模型、信号驱动IO模型以及异步IO模型。

** DONE 2	Java集合容器面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104588551
   CLOSED: [2021-04-14 Wed 15:50]
*** Map
**** HashMap： JDK1.8之前HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8以后在解决哈希冲突时有了较大的变化，
     当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间
**** LinkedHashMap：LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，
     使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。
**** HashTable： 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的
**** TreeMap： 红黑树（自平衡的排序二叉树）
*** 哪些集合类是线程安全的？
**** vector：就比arraylist多了个同步化机制（线程安全），因为效率较低，现在已经不太建议使用。在web应用中，特别是前台页面，往往效率（页面响应速度）是优先考虑的。
**** statck：堆栈类，先进后出。
**** hashtable：就比hashmap多了个线程安全。
**** enumeration：枚举，相当于迭代器。
*** Java集合的快速失败机制 “fail-fast”？
**** 假设存在两个线程（线程1、线程2），线程1通过Iterator在遍历集合A中的元素，在某个时候线程2修改了集合A的结构（是结构上面的修改，而不是简单的修改集合元素的内容），
     那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生fail-fast机制。
**** 原因：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，
     都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。
**** 解决办法：
***** 在遍历过程中，所有涉及到改变modCount值得地方全部加上synchronized。
***** 使用CopyOnWriteArrayList来替换ArrayList
*** 怎么确保一个集合不能被修改？
**** 可以使用 Collections. unmodifiableCollection(Collection c) 方法来创建一个只读集合，这样改变集合的任何操作都会抛出 Java. lang. UnsupportedOperationException 异常。
*** ArrayList 和 Vector 的区别是什么？
**** 线程安全：Vector 使用了 Synchronized 来实现线程同步，是线程安全的，而 ArrayList 是非线程安全的。
**** 性能：ArrayList 在性能方面要优于 Vector。
**** 扩容：ArrayList 和 Vector 都会根据实际的需要动态的调整容量，只不过在 Vector 扩容每次会增加 1 倍，而 ArrayList 只会增加 50%。
*** List 和 Set 的区别(set是无序的)
**** List 特点：一个有序（元素存入集合的顺序和取出的顺序一致）容器，元素可以重复，可以插入多个null元素，元素都有索引。常用的实现类有 ArrayList、LinkedList 和 Vector。
**** Set 特点：一个无序（存入和取出顺序有可能不一致）容器，不可以存储重复元素，只允许存入一个null元素，必须保证元素唯一性。Set 接口常用实现类是 HashSet、LinkedHashSet 以及 TreeSet。
**** 另外 List 支持for循环，也就是通过下标来遍历，也可以用迭代器，但是set只能用迭代，因为他无序，无法用下标来取得想要的值。
**** 对比
***** Set：检索元素效率低下，删除和插入效率高，插入和删除不会引起元素位置改变。
***** List：和数组类似，List可以动态增长，查找元素效率高，插入删除元素效率低，因为会引起其他元素位置改变
*** HashSet 的实现原理
**** HashSet 是基于 HashMap 实现的，HashSet的值存放于HashMap的key上，HashMap的value统一为PRESENT，因此 HashSet 的实现比较简单，相关 HashSet 的操作，
     基本上都是直接调用底层 HashMap 的相关方法来完成，HashSet 不允许重复的值
**** HashSet如何检查重复
***** 先比较hashcode 再比较equals 
***** ((k = e.key) == key || (key != null && key.equals(k))))
***** equals方法被覆盖过，则hashCode方法也必须被覆盖
***** hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。
*** HashSet与HashMap的区别
**** HashMap	HashSet
**** 实现了Map接口	| 实现Set接口
**** 存储键值对	|  仅存储对象
**** 调用put（）向map中添加元素	 |  调用add（）方法向Set中添加元素
**** HashMap使用键（Key）计算Hashcode	|  HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性，如果两个对象不同的话，那么返回false
**** HashMap相对于HashSet较快，因为它是使用唯一的键获取对象	|   HashSet较HashMap来说比较慢
*** 在 Queue 中 poll()和 remove()有什么区别？
**** 相同点：都是返回第一个元素，并在队列中删除返回的对象。
**** 不同点：如果没有元素 poll()会返回 null，而 remove()会直接抛出 NoSuchElementException 异常。
*** Hashtable(散列表) 与 HashMap 的简单比较
**** HashTable 基于 Dictionary 类，而 HashMap 是基于 AbstractMap。Dictionary 是任何可将键映射到相应值的类的抽象父类，而 AbstractMap 是基于 Map 接口的实现，它以最大限度地减少实现此接口所需的工作。
**** HashMap 的 key 和 value 都允许为 null，而 Hashtable 的 key 和 value 都不允许为 null。HashMap 遇到 key 为 null 的时候，调用 putForNullKey 方法进行处理，
     而对 value 没有处理；Hashtable遇到 null，直接返回 NullPointerException。
**** Hashtable 方法是同步，而HashMap则不是。我们可以看一下源码，Hashtable 中的几乎所有的 public 的方法都是 synchronized 的，而有些方法也是在内部通过 synchronized 代码块来实现。
     所以有人一般都建议如果是涉及到多线程同步时采用 HashTable，没有涉及就采用 HashMap，但是在 Collections 类中存在一个静态方法：synchronizedMap()，该方法创建了一个线程安全的 Map 对象，
     并把它作为一个封装的对象来返回。
**** **初始容量大小和每次扩充容量大小的不同 **： ①创建时如果不指定容量初始值，Hashtable 默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap 默认的初始化大小为16。之后每次扩充，
     容量变为原来的2倍。②创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为2的幂次方大小。也就是说 HashMap 总是使用2的幂作为哈希表的大小，
     后面会介绍到为什么是2的幂次方。
**** DK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。
*** HashMap 1.7和1.8区别
**** 不同	JDK 1.7	JDK 1.8
**** 存储结构	数组 + 链表	数组 + 链表 + 红黑树
**** 初始化方式	单独函数：inflateTable()  |	直接集成到了扩容函数resize()中
**** hash值计算方式	扰动处理 = 9次扰动 = 4次位运算 + 5次异或运算	|  扰动处理 = 2次扰动 = 1次位运算 + 1次异或运算
**** 存放数据的规则	无冲突时，存放数组；冲突时，存放链表	无冲突时，存放数组； | 冲突 & 链表长度 < 8：存放单链表；冲突 & 链表长度 > 8：树化并存放红黑树
**** 插入数据方式	头插法（先讲原位置的数据移到后1位，再插入数据到该位置） |	尾插法（直接插入到链表尾部/红黑树）
**** 扩容后存储位置的计算方式	全部按照原来方法进行计算（即hashCode ->> 扰动函数 ->> (h&length-1)）|	按照扩容后的规律计算（即扩容后的位置=原位置 or 原位置 + 旧容量）
**** 这比在JDK 1.7中，更为简洁，相比在1.7中的4次位运算，5次异或运算（9次扰动），在1.8中，只进行了1次位运算和1次异或运算（2次扰动）；
**** 1.8 (h = key.hashCode()) ^ (h >>> 16)  
*** HashMap1.8总结
**** 使用链地址法（使用散列表）来链接拥有相同hash值的数据；
**** 使用2次扰动函数（hash函数）来降低哈希冲突的概率，使得数据分布更平均；
**** 引入红黑树进一步降低遍历的时间复杂度，使得遍历更快
*** 能否使用任何类作为 Map 的 key？(若重写了equals方法，必须重写hashcode方法，不然同一个实例在不同时刻获取到到hashcode不同，在hash时得到的不是相同的值就会到不同的数组中)
**** 可以使用任何类作为 Map 的 key，然而在使用之前，需要考虑以下几点：
**** 如果类重写了 equals() 方法，也应该重写 hashCode() 方法。
**** 类的所有实例需要遵循与 equals() 和 hashCode() 相关的规则。
**** 如果一个类没有使用 equals()，不应该在 hashCode() 中使用它。
**** 用户自定义 Key 类最佳实践是使之为不可变的，这样 hashCode() 值可以被缓存起来，拥有更好的性能。不可变的类也可以确保 hashCode() 和 equals() 在未来不会改变，这样就会解决与可变相关的问题了。
*** 为什么HashMap中String、Integer这样的包装类适合作为K
**** String、Integer等包装类的特性能够保证Hash值的不可更改性和计算准确性，能够有效的减少Hash碰撞的几率
**** 都是final类型，即不可变性，保证key的不可更改性，不会存在获取hash值不同的情况
**** 内部已重写了equals()、hashCode()等方法，遵守了HashMap内部的规范（不清楚可以去上面看看putValue的过程），不容易出现Hash值计算错误的情况；
*** 如果使用Object作为HashMap的Key，应该怎么办呢？
**** 重写hashCode()和equals()方法
**** 重写hashCode()是因为需要计算存储数据的存储位置，需要注意不要试图从散列码计算中排除掉一个对象的关键部分来提高性能，这样虽然能更快但可能会导致更多的Hash碰撞；
**** 重写equals()方法，需要遵守自反性、对称性、传递性、一致性以及对于任何非null的引用值x，x.equals(null)必须返回false的这几个特性，目的是为了保证key在哈希表中的唯一性；
*** HashMap 的长度为什么是2的幂次方
**** hash%length==hash&(length-1) 其中length必须是2的次方,不然会存在0在里面
*** 那为什么是两次扰动呢？ (h = key.hashCode()) ^ (h >>> 16)
**** 这样就是加大哈希值低位的随机性，使得分布更均匀，从而提高对应数组存储下标位置的随机性&均匀性，最终减少Hash冲突，两次就够了，已经达到了高位低位同时参与运算的目的；
*** 如何决定使用 HashMap 还是 TreeMap
**** 对于在Map中插入、删除和定位元素这类操作，HashMap是最好的选择。然而，假如你需要对一个有序的key集合进行遍历，TreeMap是更好的选择。基于你的collection的大小，也许向HashMap中添加元素会更快，
     将map换为TreeMap进行有序key的遍历。
*** TreeMap 和 TreeSet 在排序时如何比较元素？Collections 工具类中的 sort()方法如何比较元素
**** TreeSet 要求存放的对象所属的类必须实现 Comparable 接口，该接口提供了比较元素的 compareTo()方法，当插入元素时会回调该方法比较元素的大小。
     TreeMap 要求存放的键值对映射的键必须实现 Comparable 接口从而根据键对元素进 行排 序。
**** Collections 工具类的 sort 方法有两种重载的形式
***** 第一种要求传入的待排序容器中存放的对象比较实现 Comparable 接口以实现元素的比较；
***** 第二种不强制性的要求容器中的元素必须可比较，但是要求传入第二个参数，参数是Comparator 接口的子类型（需要重写 compare 方法实现元素的比较），相当于一个临时定义的排序规则，
      其实就是通过接口注入比较元素大小的算法，也是对回调模式的应用（Java 中对函数式编程的支持）。
*** HashMap 和 ConcurrentHashMap 的区别
**** ConcurrentHashMap对整个桶数组进行了分割分段(Segment)，然后在每一个分段上都用lock锁进行保护，相对于HashTable的synchronized锁的粒度更精细了一些，并发性能更好，
     而HashMap没有锁机制，不是线程安全的。（JDK1.8之后ConcurrentHashMap启用了一种全新的方式实现,利用CAS算法。）
**** HashMap的键值对允许有null，但是ConCurrentHashMap都不允许。
*** concurrenthashmap实现线程安全的方式
**** ① 在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。
     （默认分配16个Segment，比Hashtable效率提高16倍。） 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。
     （JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；
**** ② Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，
     另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。
*** ConcurrentHashMap 底层具体实现知道吗
**** JDK1.7
***** 首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。
***** 在JDK1.7中，ConcurrentHashMap采用Segment + HashEntry的方式进行实现
***** 一个 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和HashMap类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，
      每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个HashEntry数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment的锁。
***** 该类包含两个静态内部类 HashEntry 和 Segment ；前者用来封装映射表的键值对，后者用来充当锁的角色；
***** Segment 是一种可重入的锁 ReentrantLock，每个 Segment 守护一个HashEntry 数组里得元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment 锁。
**** JDK1.8
***** 在JDK1.8中，放弃了Segment臃肿的设计，取而代之的是采用Node + CAS + Synchronized来保证并发安全进行实现，synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，
      就不会产生并发，效率又提升N倍。
***** 如果该节点是TreeBin类型的节点，说明是红黑树结构，则通过putTreeVal方法往红黑树中插入节点；如果binCount不为0，说明put操作对数据产生了影响，如果当前链表的个数达到8个，
      则通过treeifyBin方法转化为红黑树，如果oldVal不为空，说明是一次更新操作，没有对元素个数产生影响，则直接返回旧值；
***** 如果插入的是一个新节点，则执行addCount()方法尝试更新元素个数baseCount；

** DONE 3	Java异常面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104390689
   CLOSED: [2021-04-14 Wed 15:07]
*** Error 和 Exception 区别是什么
**** Error 类型的错误通常为虚拟机相关错误，如系统崩溃，内存不足，堆栈溢出等，编译器不会对这类错误进行检测，JAVA 应用程序也不应对这类错误进行捕获，
     一旦这类错误发生，通常应用程序会被终止，仅靠应用程序本身无法恢复；
**** Exception 类的错误是可以在应用程序中进行捕获并处理的，通常遇到这种错误，应对其进行处理，使应用程序可以继续正常运行。
*** 运行时异常和一般异常(受检异常)区别是什么？
**** 运行时异常包括 RuntimeException 类及其子类，表示 JVM 在运行期间可能出现的异常。 Java 编译器不会检查运行时异常。
**** 受检异常是Exception 中除 RuntimeException 及其子类之外的异常。 Java 编译器会检查受检异常。
**** RuntimeException异常和受检异常之间的区别：是否强制要求调用者必须处理此异常，如果强制要求调用者必须进行处理，那么就使用受检异常，否则就选择非受检异常(RuntimeException)。
     一般来讲，如果没有特殊的要求，我们建议使用RuntimeException异常。
*** JVM 是如何处理异常的？
**** 在一个方法中如果发生异常，这个方法会创建一个异常对象，并转交给 JVM，该异常对象包含异常名称，异常描述以及异常发生时应用程序的状态。创建异常对象并转交给 JVM 的过程称为抛出异常。可能有一系列的方法调用，
     最终才进入抛出异常的方法，这一系列方法调用的有序列表叫做调用栈。
**** JVM 会顺着调用栈去查找看是否有可以处理异常的代码，如果有，则调用异常处理代码。当 JVM 发现可以处理异常的代码时，会把发生的异常传递给它。如果 JVM 没有找到可以处理该异常的代码块，
     JVM 就会将该异常转交给默认的异常处理器（默认处理器为 JVM 的一部分），默认异常处理器打印出异常信息并终止应用程序。
*** NoClassDefFoundError 和 ClassNotFoundException 区别？
**** NoClassDefFoundError 是一个 Error 类型的异常，是由 JVM 引起的，不应该尝试捕获这个异常。
**** 引起该异常的原因是 JVM 或 ClassLoader 尝试加载某类时在内存中找不到该类的定义，该动作发生在运行期间，即编译时该类存在，但是在运行时却找不到了，可能是编译后被删除了等原因导致；
**** ClassNotFoundException 是一个受查异常，需要显式地使用 try-catch 对其进行捕获和处理，或在方法签名中用 throws 关键字进行声明。当使用 Class.forName,
     ClassLoader.loadClass 或 ClassLoader.findSystemClass 动态加载类到内存的时候，通过传入的类路径参数没有找到该类，就会抛出该异常；另一种抛出该异常的可能原因是某个类已经由一
     个类加载器加载至内存中，另一个加载器又尝试去加载它。
*** 常见的 RuntimeException 有哪些
**** ClassCastException(类转换异常)
**** IndexOutOfBoundsException(数组越界)
**** NullPointerException(空指针)
**** ArrayStoreException(数据存储异常，操作数组时类型不一致)
**** 还有IO操作的BufferOverflowException异常
*** 常见异常
**** 字符串、数组、索引（列表） 越界
**** 类、数据 转换异常
**** 内存、堆栈
**** 空指针、算数
**** 属性、方法不存在
**** 类找不到
*** Java异常处理最佳实践
**** 使用 finally 代码块
***** 资源在finally中关闭
**** Java 7 的 try-with-resource 语法
***** 如果你的资源实现了 AutoCloseable 接口，你可以使用这个语法。大多数的 Java 标准资源都继承了这个接口。当你在 try 子句中打开资源，资源会在 try 代码块执行后或异常处理后自动关闭。
**** 优先明确的异常
***** 抛出一个 NumberFormatException 来替换一个 IllegalArgumentException 。避免抛出一个不明确的异常。
**** 对异常进行文档说明
***** 当在方法上声明抛出异常时，也需要进行文档说明。目的是为了给调用者提供尽可能多的信息，从而可以更好地避免或处理异常。
***** 在 Javadoc 添加 @throws 声明，并且描述抛出异常的场景。
**** 使用描述性消息抛出异常
***** 在抛出异常时，需要尽可能精确地描述问题和相关信息，这样无论是打印到日志中还是在监控工具中，都能够更容易被人阅读，从而可以更好地定位具体错误信息、错误的严重程度等。
**** 优先捕获最具体的异常
**** 不要捕获 Throwable 类
***** Throwable 是所有异常和错误的超类。你可以在 catch 子句中使用它，但是你永远不应该这样做！
**** 不要忽略异常
***** 很多时候，开发者很有自信不会抛出异常，因此写了一个catch块，但是没有做任何处理或者记录日志
***** 但现实是经常会出现无法预料的异常，或者无法确定这里的代码未来是不是会改动(删除了阻止异常抛出的代码)，而此时由于异常被捕获，使得无法拿到足够的错误信息来定位问题。
***** 合理的做法是至少要记录异常的信息。
**** 不要记录并抛出异常
***** 这可能是本文中最常被忽略的最佳实践。可以发现很多代码甚至类库中都会有捕获异常、记录日志并再次抛出的逻辑。
***** 这个处理逻辑看着是合理的。但这经常会给同一个异常输出多条日志。
***** 如果想要提供更加有用的信息，那么可以将异常包装为自定义异常。
**** 包装异常时不要抛弃原始的异常
***** 捕获标准异常并包装为自定义异常是一个很常见的做法。这样可以添加更为具体的异常信息并能够做针对的异常处理。
***** 在你这样做时，请确保将原始异常设置为原因（注：参考下方代码 NumberFormatException e 中的原始异常 e ）。Exception 类提供了特殊的构造函数方法，它接受一个 Throwable 作为参数。
      否则，你将会丢失堆栈跟踪和原始异常的消息，这将会使分析导致异常的异常事件变得困难。
**** 异常会影响性能
***** 异常处理的性能成本非常高，每个 Java 程序员在开发时都应牢记这句话。创建一个异常非常慢，抛出一个异常又会消耗1~5ms，当一个异常在应用的多个层级之间传递时，会拖累整个应用的性能。
**** 尽管使用异常有利于 Java 开发，但是在应用中最好不要捕获太多的调用栈，因为在很多情况下都不需要打印调用栈就知道哪里出错了。因此，异常消息应该提供恰到好处的信息。
*** 异常处理-阿里巴巴Java开发手册
**** 【强制】Java 类库中定义的可以通过预检查方式规避的RuntimeException异常不应该通过catch 的方式来处理，比如：NullPointerException，IndexOutOfBoundsException等等。
     说明：无法通过预检查的异常除外，比如，在解析字符串形式的数字时，可能存在数字格式错误，不得不通过catch NumberFormatException来实现。
     正例：if (obj != null) {…} 反例：try { obj.method(); } catch (NullPointerException e) {…}
**** 【强制】异常不要用来做流程控制，条件控制。 说明：异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。
**** 【强制】catch时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的catch尽可能进行区分异常类型，再做对应的异常处理。
     说明：对大段代码进行try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利于定位问题，这是一种不负责任的表现。 正例：用户注册的场景中，如果用户输入非法字符，
     或用户名称已存在，或用户输入密码过于简单，在程序上作出分门别类的判断，并提示给用户。
**** 【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。
**** 【强制】有try块放到了事务代码中，catch异常后，如果需要回滚事务，一定要注意手动回滚事务。
**** 【强制】finally块必须对资源对象、流对象进行关闭，有异常也要做try-catch。 说明：如果JDK7及以上，可以使用try-with-resources方式。
**** 【强制】不要在finally块中使用return。 说明：try块中的return语句执行成功后，并不马上返回，而是继续执行finally块中的语句，如果此处存在return语句，则在此直接返回，无情丢弃掉try块中的返回点。
**** 【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。 说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。(里氏替换原则)
**** 【强制】在调用RPC、二方包、或动态生成类的相关方法时，捕捉异常必须使用Throwable类来进行拦截。 说明：通过反射机制来调用方法，如果找不到方法，抛出NoSuchMethodException。
     什么情况会抛出NoSuchMethodError呢？二方包在类冲突时，仲裁机制可能导致引入非预期的版本使类的方法签名不匹配，或者在字节码修改框架（比如：ASM）动态创建或修改类时，修改了相应的方法签名。
     这些情况，即使代码编译期是正确的，但在代码运行期时，会抛出NoSuchMethodError。
**** 【推荐】方法的返回值可以为null，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回null值。 说明：本手册明确防止NPE是调用者的责任。即使被调用方法返回空集合或者空对象，
     对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回null的情况。
**** 【推荐】防止NPE，是程序员的基本修养，注意NPE产生的场景： 1） 返回类型为基本数据类型，return包装数据类型的对象时，自动拆箱有可能产生NPE。
     反例：public int f() { return Integer对象}， 如果为null，自动解箱抛NPE。 2） 数据库的查询结果可能为null。 3） 集合里的元素即使isNotEmpty，取出的数据元素也可能为null。
     4） 远程调用返回对象时，一律要求进行空指针判断，防止NPE。 5） 对于Session中获取的数据，建议进行NPE检查，避免空指针。 6） 级联调用obj.getA().getB().getC()；一连串调用，易产生NPE。
     正例：使用JDK8的Optional类来防止NPE问题。
**** 【推荐】定义时区分unchecked / checked 异常，避免直接抛出new RuntimeException()，更不允许抛出Exception或者Throwable，应使用有业务含义的自定义异常。
     推荐业界已定义过的自定义异常，如：DAOException / ServiceException等。
**** 【参考】对于公司外的http/api开放接口必须使用“错误码”；而应用内部推荐异常抛出；跨应用间RPC调用优先考虑使用Result方式，封装isSuccess()方法、“错误码”、“错误简短信息”。
     说明：关于RPC方法返回方式使用Result方式的理由： 1）使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。 2）如果不加栈信息，只是new自定义异常，加入自己的理解的error message，
     对于调用端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输的性能损耗也是问题。
**** 【参考】避免出现重复的代码（Don’t Repeat Yourself），即DRY原则。 说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副本，容易遗漏。
     必要时抽取共性方法，或者抽象公共类，甚至是组件化。 正例：一个类中有多个public方法，都需要进行数行相同的参数校验操作，这个时候请抽取：private boolean checkParam(DTO dto) {…}

** DONE 4	并发编程面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104863992
   CLOSED: [2021-04-15 Thu 16:43]
*** 并发编程有什么缺点(内存泄漏、上下文切换、线程安全、死锁)
**** 并发编程的目的就是为了能提高程序的执行效率，提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如**：内存泄漏、上下文切换、线程安全、死锁**等问题
*** 并发编程三要素是什么？在 Java 程序中怎么保证多线程的运行安全？(原子性、可见行、有序性)
**** 原子性：原子，即一个不可再被分割的颗粒。原子性指的是一个或多个操作要么全部执行成功要么全部执行失败。
**** 可见性：一个线程对共享变量的修改,另一个线程能够立刻看到。（synchronized,volatile）
**** 有序性：程序执行的顺序按照代码的先后顺序执行。（处理器可能会对指令进行重排序）
**** 出现线程安全问题的原因：
***** 线程切换带来的原子性问题
***** 缓存导致的可见性问题
***** 编译优化带来的有序性问题
**** 解决办法：
***** JDK Atomic开头的原子类、synchronized、LOCK，可以解决原子性问题
***** synchronized、volatile、LOCK，可以解决可见性问题
***** Happens-Before 规则可以解决有序性问题
*** 守护线程和用户线程有什么区别呢？(在后台、用户线程结束其就结束、使用setDaemon(true)，且需要在start之前、其子线程也是守护线程、io和计算不能使用守护线程、finally不能确保执行)
**** 用户 (User) 线程：运行在前台，执行具体的任务，如程序的主线程、连接网络的子线程等都是用户线程
**** 守护 (Daemon) 线程：运行在后台，为其他前台线程服务。也可以说守护线程是 JVM 中非守护线程的 “佣人”。一旦所有用户线程都结束运行，守护线程会随 JVM 一起结束工作
**** 注意事项：
***** setDaemon(true)必须在start()方法前执行，否则会抛出 IllegalThreadStateException 异常
***** 在守护线程中产生的新线程也是守护线程
***** 不是所有的任务都可以分配给守护线程来执行，比如读写操作或者计算逻辑
***** 守护 (Daemon) 线程中不能依靠 finally 块的内容来确保执行关闭或清理资源的逻辑。因为我们上面也说过了一旦所有用户线程都结束运行，守护线程会随 JVM 一起结束工作，所以守护 (Daemon) 线程中的
      finally 语句块可能无法被执行。
*** 死锁的四个必要条件(互斥、持有且等待、不可剥夺、循环等待)
**** 互斥条件：线程(进程)对于所分配到的资源具有排它性，即一个资源只能被一个线程(进程)占用，直到被该线程(进程)释放
**** 请求与保持条件：一个线程(进程)因请求被占用资源而发生阻塞时，对已获得的资源保持不放。
**** 不剥夺条件：线程(进程)已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。
**** 循环等待条件：当发生死锁时，所等待的线程(进程)必定会形成一个环路（类似于死循环），造成永久阻塞
*** 如何避免线程死锁(不可剥夺[主动释放]、持有等待[一次申请所有资源]、循环等待[有序申请资源])
**** 破坏互斥条件
***** 这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。
**** 破坏请求与保持条件
***** 一次性申请所有的资源。
**** 破坏不剥夺条件
***** 占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。
**** 破坏循环等待条件
***** 靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。
*** 防止死锁(使用tryLock(time)、使用concurrent、降低锁粒度、减少同步代码)
**** 尽量使用 tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。
**** 尽量使用 Java. util. concurrent 并发类代替自己手写锁。
**** 尽量降低锁的使用粒度，尽量不要几个功能用同一把锁。
**** 尽量减少同步的代码块。
*** 创建线程的四种方式(继承Thread[run()]、实现Runnable[run()]、实现Callable[call()实现，new FutureTask(callable)，new Thread(futureTask)]、使用ExecutorService = Executors.newFixedThreadPool(5))
**** 继承 Thread 类；
**** 实现 Runnable 接口；
**** 实现 Callable 接口；
***** 创建实现Callable接口的类myCallable
***** 以myCallable为参数创建FutureTask对象(也可以是runnable作为参数，只是FutureTask会使用RunnableAdapter（有call方法调用runnable的run方法）对runnable进行适配)
***** 将FutureTask(继承RunnableFuture接口,RunnableFuture继承runnable和future)作为参数创建Thread对象
***** 调用线程对象的start()方法
***** FutureTask.get（）获取相应的结果，awaitdone（）-> 结果的存入使用的unsafe.compareandset()
**** 使用 Executors
***** callable和runnable的区别是一个有返回值一个没有，在executorService中都是转化为futureTask，用submit方法是返回future，用execute返回void。
***** 《阿里巴巴Java开发手册》中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险
***** Executors 返回线程池对象的弊端如下：
****** FixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE ，可能堆积大量的请求，从而导致OOM。
****** CachedThreadPool 和  ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致OOM。
***** Executors创建三种类型的ThreadPoolExecutor
****** FixedThreadPool ： 该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空
           闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。
****** SingleThreadExecutor： 方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线
           程空闲，按先入先出的顺序执行队列中的任务。
****** CachedThreadPool： 该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优
           先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池
           进行复用。
****** ScheduledThreadPool内部使用的阻塞队列是DelayQueue，这是一个无界、带延迟的阻塞队列，只有当延迟时间过了才能从这个阻塞队列中取出当中的元素。
***** ThreadPoolExecutor构造方法
****** public ThreadPoolExecutor(int corePoolSize,
                            int maximumPoolSize,
                            long keepAliveTime,
                            TimeUnit unit,
                            BlockingQueue<Runnable> workQueue,
                            ThreadFactory threadFactory,
                            RejectedExecutionHandler handler)
****** 饱和策略
******* ThreadPoolExecutor.AbortPolicy：抛出 RejectedExecutionException来拒绝新任务的处理。
******* ThreadPoolExecutor.CallerRunsPolicy：调用当前执行线程运行任务。您不会任务请求。但是这种策略会降低对于新任务提交速度，影响
             程序的整体性能。另外，这个策略喜欢增加队列容量。如果您的应用程序可以承受此延迟并且你不能容忍丢弃任何一个任务请求的话，你可以
             选择这个策略。
******* ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。
******* ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。
***** corePoolSize设置
****** I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，
           这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。
****** CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是
           为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的
           一个线程就可以充分利用 CPU 的空闲时间。
*** ThreadPoolExecutor参数(corePoolSize、maximumPoolSize、keepAliveTime、unit、workQueue、threadFactory、handler[拒绝策略])
*** 阻塞队列(Fixed+single[LinkedBlockingQueue(无界)]、cached[Synchronous{无容量}]、scheduled[DelayQueue{通过compareto排序}内部封装PriorityQueue{最大堆}]、这些队列基本使用ReentrantLock + Condition实现线程安全)
*** BlockingQueue
**** 分类
***** ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。在多线程环境下不保证“公平性”
***** LinkedBlockingQueue ：一个由链表结构组成的无界阻塞队列。
***** PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。默认情况下元素采用自然顺序升序排序，可以通过指定Comparator来对元素进行排序，最大堆父节点的键值总是大于或等于任何一个子节点的键值，添加操作则是不断“上冒”，而删除操作则是不断“下掉”
***** DelayQueue：一个使用优先级队列实现的无界阻塞队列。ReentrantLock + Condition实现，根据Delay时间排序的优先级队列：PriorityQueue
***** SynchronousQueue：一个不存储元素的阻塞队列。交换工作，生产者的线程和消费者的线程同步以传递某些信息、事件或者任务
***** LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。相当于ConcurrentLinkedQueue、SynchronousQueue (公平模式下)、无界的LinkedBlockingQueues等的超集
***** LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。
**** 使用场景
***** 阻塞队列使用最经典的场景就是 socket 客户端数据的读取和解析，读取数据的线程不断将数据放入队列，然后解析线程不断从队列取数据解析。
*** 线程池都有哪些状态？(running、shutdown[不接收任务]、stop[终止当前任务、不接收、不运行]、tidying[销毁所有任务、workcount设为0、调用terminated()]、terminated)
**** RUNNING：这是最正常的状态，接受新的任务，处理等待队列中的任务。
**** SHUTDOWN：不接受新的任务提交，但是会继续处理等待队列中的任务。
**** STOP：不接受新的任务提交，不再处理等待队列中的任务，中断正在执行任务的线程。
**** TIDYING：所有的任务都销毁了，workCount 为 0，线程池的状态在转换为 TIDYING 状态时，会执行钩子方法 terminated()。
**** TERMINATED：terminated()方法结束后，线程池的状态就会变成这个。
*** 什么是线程组，为什么在 Java 中不推荐使用(threadGroup用于管理线程、线程池管理线程生命周期，thread创建后会加入到线程主中，可以为null)
**** ThreadGroup 类，可以把线程归属到某一个线程组中，线程组中可以有线程对象，也可以有线程组，组中还可以有线程，这样的组织结构有点类似于树的形式。
**** 线程组和线程池是两个不同的概念，他们的作用完全不同，前者是为了方便线程的管理，后者是为了管理线程的生命周期，复用线程，减少创建销毁线程的开销。
**** 为什么不推荐使用线程组？因为使用有很多的安全隐患吧，没有具体追究，如果需要使用，推荐使用线程池。
*** Java 中用到的线程调度算法是什么(分时调度模型和抢占式调度模型)
**** 计算机通常只有一个 CPU，在任意时刻只能执行一条机器指令，每个线程只有获得CPU 的使用权才能执行指令。所谓多线程的并发运行，其实是指从宏观上看，各个线程轮流获得 CPU 的使用权，分别执行各自的任务。在运行池中，
     会有多个处于就绪状态的线程在等待 CPU，JAVA 虚拟机的一项任务就是负责线程的调度，线程调度是指按照特定机制为多个线程分配 CPU 的使用权。
**** 有两种调度模型：分时调度模型和抢占式调度模型。
***** 分时调度模型是指让所有的线程轮流获得 cpu 的使用权，并且平均分配每个线程占用的 CPU 的时间片这个也比较好理解。
***** Java虚拟机采用抢占式调度模型，是指优先让可运行池中优先级高的线程占用CPU，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用CPU。处于运行状态的线程会一直运行，直至它不得不放弃 CPU。
*** [#A] 线程的调度策略(yield、sleep、wait、io操作、优先级更高到抢占、时间片用完)
**** 线程调度器选择优先级最高的线程运行，但是，如果发生以下情况，就会终止线程的运行：
***** （1）线程体中调用了 yield 方法让出了对 cpu 的占用权利
***** （2）线程体中调用了 sleep 方法使线程进入睡眠状态
***** （3）线程由于 IO 操作受到阻塞
***** （4）另外一个更高优先级线程出现
***** （5）在支持时间片的系统中，该线程的时间片用完
*** 请说出与线程同步以及线程调度相关的方法。(wait、sleep、notify、notifyAll)
**** （1） wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁；
**** （2）sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理 InterruptedException 异常；
**** （3）notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由 JVM 确定唤醒哪个线程，而且与优先级无关；
**** （4）notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态；
*** 线程的 sleep()方法和 yield()方法有什么区别？(yield让给优先级更高到线程)
**** （1） sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会；
**** （2） 线程执行 sleep()方法后转入阻塞（blocked）状态，而执行 yield()方法后转入就绪（ready）状态；
**** （3）sleep()方法声明抛出 InterruptedException，而 yield()方法没有声明任何异常；
**** （4）sleep()方法比 yield()方法（跟操作系统 CPU 调度相关）具有更好的可移植性，通常不建议使用yield()方法来控制并发线程的执行。
*** join使用方法，thread.join(1000)，join底层使用的是wait方法
*** interrupted()相关
**** interrupt()不会去真正意义上的打断一个正在运行的线程，而是修改这个线程的中断状态码，若是等待状态会抛出异常，若是运行状态则只是改变中断标志位。
**** interrupted()返回当前线程的中断标志位，同时会重置中断标志位(默认位false)。
**** isInterrupted()这个方法会返回线程的中断标志位。
*** 如何停止一个正在运行的线程？(使用退出标志[自定义exit标志]、使用stop方法强行终止[危险、如同断电、ThreadDeatherror]、使用interrupt方法中断线程)
**** 在java中有以下3种方法可以终止正在运行的线程：
***** 使用退出标志，使线程正常退出，也就是当run方法完成后线程终止。
public class ThreadSafe extends Thread {
    public volatile boolean exit = false; 
        public void run() { 
        while (!exit){
            //do something
        }
    } 
}
***** 使用stop方法强行终止，但是不推荐这个方法，因为stop和suspend及resume一样都是过期作废的方法。
***** 使用interrupt方法中断线程。（需要考虑阻塞和非阻塞两种状态）
****** 为什么要区分进入阻塞状态和和非阻塞状态两种情况了，是因为当阻塞状态时，如果有interrupt()发生，系统除了会抛出InterruptedException异常外，还会调用interrupted()函数，
       调用时能获取到中断状态是true的状态，调用完之后会复位中断状态为false，所以异常抛出之后通过isInterrupted()是获取不到中断状态是true的状态，从而不能退出循环，
       因此在线程未进入阻塞的代码段时是可以通过isInterrupted()来判断中断是否发生来控制循环，在进入阻塞状态后要通过捕获异常来退出循环。
public class ThreadSafe extends Thread {
    public void run() { 
        while (!isInterrupted()){ //非阻塞过程中通过判断中断标志来退出
            try{
                Thread.sleep(5*1000);//阻塞过程捕获中断异常来退出
            }catch(InterruptedException e){
                e.printStackTrace();
                break;//捕获到异常之后，执行break跳出循环。
            }
        }
    } 
}
*** 什么是阻塞式方法？(一直等待不做别的事[serversocket.accept()])
**** 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket 的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。
     此外，还有异步和非阻塞式方法在任务完成前就返回。
*** Java 中你怎样唤醒一个阻塞的线程？(wait|notify方法在同步块中使用，锁定到同一对象，唤醒后还需要竞争到锁才能继续执行)
**** 首先 ，wait()、notify() 方法是针对对象的，调用任意对象的 wait()方法都将导致线程阻塞，阻塞的同时也将释放该对象的锁，相应地，调用任意对象的 notify()方法则将随机解除该对象阻塞的线程，
     但它需要重新获取该对象的锁，直到获取成功才能往下执行；
**** 其次，wait、notify 方法必须在 synchronized 块或方法中被调用，并且要保证同步块或方法的锁对象与调用 wait、notify 方法的对象是同一个，
     如此一来在调用 wait 之前当前线程就已经成功获取某对象的锁，执行 wait 阻塞后当前线程就将之前获取的对象锁释放。
*** 如果你提交任务时，线程池队列已满，这时会发生什么(先添加workqueue[array有界，linked无界]->maximumPoolSize是否达到->拒绝策略)
**** （1）如果使用的是无界队列 LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为 LinkedBlockingQueue 可以近乎认为是一个无穷大的队列，可以无限存放任务
**** （2）如果使用的是有界队列比如 ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue 中，ArrayBlockingQueue 满了，会根据maximumPoolSize 的值增加线程数量，
     如果增加了线程数量还是处理不过来，ArrayBlockingQueue 继续满，那么则会使用拒绝策略RejectedExecutionHandler 处理满了的任务，默认是 AbortPolicy
*** 饱和策略(Abort[抛出rejectexecution异常]、callerruns[当前执行线程运行]、discard[直接抛弃]、discardoldest[抛弃最老])
*** [#A] 锁(synchronized使用对象头中mark word，1.6之后引入锁优化技术、reentrantlock使用locksupport.park)
**** Synchronize是可重入锁的原理
***** synchronize 监视monitor锁，monitorenter、monitorexit
***** Synchronized的语义底层是通过一个monitor的对象来完成，其实wait/notify等方法也依赖于monitor对象，这就是为什么只有在同步的块或者方法中才能调用wait/notify等方法，
      否则会抛出java.lang.IllegalMonitorStateException的异常的原因。
***** synchronized底层的实现原理是利用计算机系统的mutex Lock实现。每一个可重入锁都会关联一个线程ID和一个锁状态status。
***** 当一个线程请求方法时，会去检查锁状态，如果锁状态是0，代表该锁没有被占用，直接进行CAS操作获取锁，将线程ID替换成自己的线程ID。如果锁状态不是0，代表有线程在访问该方法。
      此时，如果线程ID是自己的线程ID，如果是可重入锁，会将status自增1，然后获取到该锁，进而执行相应的方法。如果是非重入锁，就会进入阻塞队列等待。
**** 在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。
     如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，
     这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。
     JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。
**** synchronized 和 ReentrantLock 区别是什么？
***** synchronized 是和 if、else、for、while 一样的关键字，ReentrantLock 是类
***** synchronized 早期的实现比较低效，对比 ReentrantLock，大多数场景性能都相差较大，但是在 Java 6 中对 synchronized 进行了非常多的改进。
***** 相同点：两者都是可重入锁
***** 主要区别如下
****** ReentrantLock 使用起来比较灵活，但是必须有释放锁的配合动作；
****** ReentrantLock 必须手动获取与释放锁，而 synchronized 不需要手动释放和开启锁；
****** ReentrantLock 只适用于代码块锁，而 synchronized 可以修饰类、方法、变量等。
****** 二者的锁机制其实也是不一样的。ReentrantLock 底层调用的是 Unsafe 的park 方法加锁，synchronized 操作的应该是对象头中 mark word
*** [#A] volatile 关键字的作用(保证可见性[对象修改后立写回主存，对象读取从主存获取新值{先将本地设为无效}]、禁止指令重排、和TLAB相关[线程私有内存空间]、和cas结合如atomic包下到类)
**** 对于可见性，Java 提供了 volatile 关键字来保证可见性和禁止指令重排。 volatile 提供 happens-before 的保证，确保一个线程的修改能对其他线程是可见的。当一个共享变量被 volatile 修饰时，
     它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。
**** 从实践角度而言，volatile 的一个重要作用就是和 CAS 结合，保证了原子性，详细的可以参见 java.util.concurrent.atomic 包下的类，比如 AtomicInteger。
**** Java内存模型(JMM)规定了所有的变量都存储在主内存中，主内存中的变量为共享变量，而每条线程都有自己的工作内存，线程的工作内存保存了从主内存拷贝的变量，所有对变量的操作都在自己的工作内存中进行，
     完成后再刷新到主内存中
**** 当变量被声明成volatile类型后，线程对该变量进行修改后会立即刷新回主内存，而其他线程读取该变量时会先将自己工作内存中的变量置为无效，再从主内存重新读取变量到自己的工作内存，这样就避免发生线程可见性问题。
*** volatile如何保证线程间可见和避免指令重排(将write-store和load-read成为原子操作保证可见性，禁止指令重排通过两个内存屏障实现：编译器屏障+cpu屏障)
**** volatile可见性是有指令原子性保证的，在jmm中定义了8类原子性指令，比如write，store，read，load。而volatile就要求write-store，load-read成为一个原子性操作，
     这样子可以确保在读取的时候都是从主内存读入，写入的时候会同步到主内存中（准确来说也是内存屏障），指令重排则是由内存屏障来保证的，由两个内存屏障:
**** 一个是编译器屏障：阻止编译器重排，保证编译程序时在优化屏障之前的指令不会在优化屏障之后执行。
**** 第二个是cpu屏障：sfence保证写入，lfence保证读取，lock类似于锁的方式。java多执行了一个“load addl $0x0, (%esp)”操作，这个操作相当于一个lock指令，就是增加一个完全的内存屏障指令。
*** 内存屏障
**** 主要包括了loadFence、storeFence、fullFence等方法, 用于定义内存屏障，避免代码重排序，与Java内存模型相关
//在该方法之前的所有读操作，一定在load屏障之前执行完成
public native void loadFence();
//在该方法之前的所有写操作，一定在store屏障之前执行完成
public native void storeFence();
//在该方法之前的所有读写操作，一定在full屏障之前执行完成，这个内存屏障相当于上面两个的合体功能
public native void fullFence();
*** volatile 变量和 atomic 变量有什么不同？(volatile只保持可见性，不保证原子性)
**** volatile 变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用 volatile 修饰 count 变量，那么 count++ 操作就不是原子性的。
**** 而 AtomicInteger 类提供的 atomic 方法可以让这种操作具有原子性如getAndIncrement()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。
*** 说一下 atomic 的原理(使用volatile和cas结合，循环尝试修改值，直到成功)
**** Atomic包中的类基本的特性就是在多线程环境下，当有多个线程同时对单个（包括基本类型及引用类型）变量进行操作时，具有排他性，即当多个线程同时对该变量的值进行更新时，仅有一个线程能成功，
     而未成功的线程可以像自旋锁一样，继续尝试，一直等到执行成功。
**** java调用
**** do {
****        var5 = this.getIntVolatile(var1, var2); // var2是valueoffset
****    } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));
**** 
**** jni调用
**** UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x))
****   UnsafeWrapper("Unsafe_CompareAndSwapInt");
****   oop p = JNIHandles::resolve(obj);
****   jint* addr = (jint *) index_oop_from_field_offset_long(p, offset);
****   return (jint)(Atomic::cmpxchg(x, addr, e)) == e;
**** UNSAFE_END
**** 
**** inline jint _Atomic_cmpxchg(jint exchange_value, volatile jint* dest, jint compare_value, int mp) {
****     __asm__ volatile (LOCK_IF_MP(%4) "cmpxchgl %1,(%3)"
****                : "=a" (exchange_value)
****                : "r" (exchange_value), "a" (compare_value), "r" (dest), "r" (mp)
****                : "cc", "memory");
****     return exchange_value;
**** }
     
*** CAS (内存位置（V）、预期原值（A）和新值(B)，自旋尝试直到成功，atomic包下的类大多使用cas+volatile实现)
**** 包含三个操作数：内存位置（V）、预期原值（A）和新值(B)。如果内存地址里面的值和 A 的值是一样的，那么就将内存里面的值更新成 B。
**** CAS是通过无限循环来获取数据的，若果在第一轮循环中，a 线程获取地址里面的值被b 线程修改了，那么 a 线程需要自旋，到下次循环才有可能机会执行。
**** java.util.concurrent.atomic 包下的类大多是使用 CAS 操作来实现的
*** ABA问题(加版本号)
**** CAS在操作值的时候检查值是否已经变化，没有变化的情况下才会进行更新。但是如果一个值原来是A，变成B，又变成A，那么CAS进行检查时会认为这个值没有变化，但是实际上却变化了。ABA问题的解决方法是使用版本号。
     在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么A－B－A 就变成1A-2B－3A。从Java1.5开始JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。
*** 什么叫线程安全？servlet 是线程安全吗?
**** 线程安全是编程中的术语，指某个方法在多线程环境中被调用时，能够正确地处理多个线程之间的共享变量，使程序功能正确完成。
**** Servlet 不是线程安全的，servlet 是单实例多线程的，当多个线程同时访问同一个方法，是不能保证共享变量的线程安全性的。
**** Struts2 的 action 是多实例多线程的，是线程安全的，每个请求过来都会 new 一个新的 action 分配给这个请求，请求完成后销毁。
**** SpringMVC 的 Controller 是线程安全的吗？不是的，和 Servlet 类似的处理流程。
**** Struts2 好处是不用考虑线程安全问题；Servlet 和 SpringMVC 需要考虑线程安全问题，但是性能可以提升不用处理太多的 gc，可以使用 ThreadLocal 来处理多线程的问题。
*** 你对线程优先级的理解是什么？(操作系统相关)
**** 每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实现，这个实现是和操作系统相关的(OS dependent)。我们可以定义线程的优先级，
     但是这并不能保证高优先级的线程会在低优先级的线程前执行。线程优先级是一个 int 变量(从 1-10)，1 代表最低优先级，10 代表最高优先级。
**** Java 的线程优先级调度会委托给操作系统去处理，所以与具体的操作系统优先级有关，如非特别需要，一般无需设置线程优先级。
*** 线程异常捕获（UnCaughtExceptionHandler使用，线程池中使用Thread.currentThread.setUncaughtExceptionHandler(new ExceptionHandler())，但是只有通过submit才能获取到，使用excute将异常放入future.get中返回）
**** 使用thread时
***** Thread thread = new Thread(new Task());
		 thread.setUncaughtExceptionHandler(new ExceptionHandler());
		 thread.start();
**** 使用线程池时（线程池出现异常）
***** 这时需要将异常的捕获封装到Runnable或者Callable中
***** Thread.currentThread().setUncaughtExceptionHandler(new ExceptionHandler());
***** 只有通过execute提交的任务，才能将它抛出的异常交给UncaughtExceptionHandler，而通过submit提交的任务，无论是抛出的未检测异常还是已检查异常，都将被认为是任务返回状态的一部分。
      如果一个由submit提交的任务由于抛出了异常而结束，那么这个异常将被Future.get封装在ExecutionException中重新抛出。
*** finalize()方法什么时候被调用？析构函数(finalization)的目的是什么？(垃圾回收时调用finalize，只有在调用native方法时才会调用finalization方法)
**** 1）垃圾回收器（garbage colector）决定回收某对象时，就会运行该对象的finalize()方法；
**** 2）GC本来就是内存回收了，应用还需要在finalization做什么呢？ 答案是大部分时候，什么都不用做(也就是不需要重载)。只有在某些很特殊的情况下，比如你调用了一些native的方法(一般是C写的)，
     可以要在finaliztion里去调用C的释放函数。
*** 为什么代码会重排序？(提高性能，要求：单线程下不改变结果、有数据依赖关系到不允许重排)
**** 为了提高性能
**** 重排要求
***** 在单线程环境下不能改变程序运行的结果；
***** 存在数据依赖关系的不允许重排序
*** as-if-serial规则和happens-before规则的区别(as保证单线程执行结果不变，happen保证正确同步到执行结果不变，都是为了提高程序执行的并行度)
**** as-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。
**** as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。
*** happens-before规则(同一线程中操作的顺序性、锁的解锁在加锁之前、volatile中的写在任意后续的读之前、具有传递性)
**** 1、程序顺序规则：一个线程中的每个操作，happens-before于线程中的任意后续操作。
**** 2、监视器锁规则：一个锁的解锁，happens-before于随后对这个锁的加锁。
**** 3、volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。
**** 4、传递性：如果A happens-before B，且Bhappens-before C，那么Ahappens-before C。
*** as-if-serial语义(单线程执行结果不变、如果存在数据依赖关系不可指令重排)
**** 不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不会改变。编译器、runtime和处理器都必须遵守as-if-serial语义。
**** 为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。
**** 但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 
*** synchronized、volatile、CAS 比较
**** （1）synchronized 是悲观锁，属于抢占式，会引起其他线程阻塞。是非公平锁
**** （2）volatile 提供多线程共享变量可见性和禁止指令重排序优化。
**** （3）CAS 是基于冲突检测的乐观锁（非阻塞）
*** synchronized 和 Lock 有什么区别？(sync是关键字是jvm层面的、给类+方法+代码块加锁，lock是类、给代码加锁、需手动释放锁、可以直到释放获取了锁)
**** 首先synchronized是Java内置关键字，在JVM层面，Lock是个Java类；
**** synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。
**** synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而 lock 需要自己加锁和释放锁，如果使用不当没有 unLock()去释放锁就会造成死锁。
**** 通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。
*** 乐观锁和悲观锁(cas[乐观锁]和普通锁+synchronize对比)
**** 悲观锁：表锁、行锁、读锁、写锁、synchronize
**** 乐观锁：cas
*** [#A] 多线程锁的升级原理是什么(无状态锁，偏向锁，轻量级锁和重量级锁状态)
**** 在Java中，锁共有4种状态，级别从低到高依次为：无状态锁，偏向锁，轻量级锁和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级。
**** 初始无锁状态(001)，线程直接获取锁，通过cas将状态改为101、同时修改锁对象头(mark word)中的线程id为自己
**** 另一个线程要获取锁，查看对象头状态，是101【偏向锁】，查看对象头中线程id是否是自己，若是将标记位设置位101，若否则通过cas修改对象头线程id，若成功则获取到锁，若失败则进入偏向锁撤销过程
**** 竞争线程在原地等待持有锁线程进入安全点，而后查看持有锁线程状态
**** 1.若是不活跃状态，则将对象头锁状态改为001，而后竞争进程通过cas修改对象头状态为101，同时线程id设置为自己的，从而获取到偏向锁
**** 2.若是活跃状态，则升级为轻量级锁，将对象头中到mark word复制到持有锁到线程栈的锁记录中，而后将对象头的指针指向该锁记录，同时状态改为00，唤醒持有锁线程进入安全点继续执行
**** 此后竞争锁进入自旋状态，自旋结束后，将对象头到mark word复制到线程栈的锁记录中，而后通过cas修改对象头中锁指针执行本线程栈中锁记录，若成功则获取到锁，若失败则升级为重量级锁
**** 重量级锁直接将对象头中的指针指向monitor指针【互斥量】，而后竞争线程进入挂起状态
**** 轻量级锁解锁时，持有锁线程通过cas来对比对象头中的markword和本线程栈中的mark word是否相同，而后再对比对象头中锁指针是否指向本线程栈中的锁对象，若都相同则解除锁，若有不相同的【已升级为重量级锁】，则去唤醒挂起的线程
*** [#A] synchronized原理，monitor依赖于底层的操作系统的Mutex Lock来实现【重量级锁】，而操作系统实现线程之间的切换时需要从用户态转换到核心态，对象头中根据锁类型指向不同的东西，当是重量级锁时指向互斥量的指针
**** 使用的锁对象存储在对象头里，对象头包含（mark word【hashCode、锁信息或分代年龄或GC标志】、Class Metadata Address【对象的类元数据】）
**** 每个对象都存在着一个 monitor 与之关联，对象与其 monitor 之间的关系有存在多种实现方式，如monitor可以与对象一起创建销毁或当线程试图获取对象锁时自动生成，但当一个 monitor 被某个线程持有后，它便处于锁定状态
**** ObjectMonitor中有两个队列，_WaitSet 和 _EntryList，用来保存ObjectWaiter对象列表( 每个等待锁的线程都会被封装成ObjectWaiter对象)，_owner指向持有ObjectMonitor对象的线程,
     当多个线程同时访问一段同步代码时，首先会进入 _EntryList 集合，当线程获取到对象的monitor 后进入 _Owner 区域并把monitor中的owner变量设置为当前线程同时monitor中的计数器count加1，若线程调用 wait() 方法，
     将释放当前持有的monitor，owner变量恢复为null，count自减1，同时该线程进入 WaitSe t集合中等待被唤醒。若当前线程执行完毕也将释放monitor(锁)并复位变量的值，以便其他线程进入获取monitor(锁)。
**** 字节码中会有两个monitorexit，一个是正常退出，一个是报错时异常退出【异常捕获监听所有异常】
**** 除了显式使用monitorenter的方式，若sychronized标记方法，则会隐式引用ACC_SYNCHRONIZED来标记同步
**** 不同锁类型
***** 偏向锁：如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word 的结构也变为偏向锁结构，当这个线程再次请求锁时，无需再做任何同步操作，即获取锁的过程
***** 轻量级锁：轻量级锁能够提升程序性能的依据是“对绝大部分的锁，在整个同步周期内都不存在竞争”，注意这是经验数据。需要了解的是，轻量级锁所适应的场景是线程交替执行同步块的场合，如果存在同一时间访问同一锁的场合，就会导致轻量级锁膨胀为重量级锁。
***** 自旋锁：轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。【大多数情况下，线程持有锁的时间都不会太长】
***** 锁消除:消除锁是虚拟机另外一种锁的优化，这种优化更彻底，Java虚拟机在JIT编译时(可以简单理解为当某段代码即将第一次被执行时进行编译，又称即时编译)，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁
*** [#A] ConcurrentHashMap(1.6使用segment继承reentrantlock、1.8使用cas+synchronized，底层数组加链表/红黑树(链表大于等于8))
**** 线程安全实现方式
***** JDK 1.6版本关键要素：
****** segment继承了ReentrantLock充当锁的角色，为每一个segment提供了线程安全的保障；
****** segment维护了哈希散列表的若干个桶，每个桶由HashEntry构成的链表。
***** JDK1.8后，ConcurrentHashMap抛弃了原有的Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。
****** ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，
       HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，
       每个HashEntry是一个链表结构的元素， 每个Segment守护者一个HashEntry数组里的元素,当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。
**** SynchronizedMap 和 ConcurrentHashMap 有什么区别？
***** SynchronizedMap 一次锁住整张表来保证线程安全，所以每次只能有一个线程来访为 map。
***** ConcurrentHashMap 使用分段锁来保证在多线程下的性能。
***** ConcurrentHashMap 中则是一次锁住一个桶。ConcurrentHashMap 默认将hash 表分为 16 个桶，诸如 get，put，remove 等常用操作只锁当前需要用到的桶。
***** 这样，原来只能一个线程进入，现在却能同时有 16 个写线程执行，并发性能的提升是显而易见的。
***** 另外 ConcurrentHashMap 使用了一种不同的迭代方式。在这种迭代方式中，当iterator 被创建后集合再发生改变就不再是抛出ConcurrentModificationException，
      取而代之的是在改变时 new 新的数据从而不影响原有的数据，iterator 完成后再将头指针替换为新的数据 ，这样 iterator线程可以使用原来老的数据，而写线程也可以并发的完成改变。
*** [#A] AQS(AbstractQueuedSynchronizer)详解与源码分析(tryacquire|tryrelease|tryacquireshared|isHeldexclusively[当用到condition采取实现]，volatile state是锁状态，独占reentrantlock，共享readwritlock)
*** [#B] ReentrantReadWriteLock -> (state【reentrantlock中记录进入次数，此处分成两部分，高16位读读次数，低16位写次数】、exclusiveOwnerThread、nofairsync、fairsync 一般实现tryacquire方法，不进行CLH等待) -> sync(implement AQS) -> (实现上面的几种方法) 
*** AQS中底层通过locksupport.park实现，locksupport.park和unpark的使用顺序没有限制，只是设置一个signal的状态，多次unpark后只能执行一次park，在执行又是阻塞，
**** 使用方式 locksupport.park()在当前线程运行，locksupport.unpark(thread) 参数thread指向需要解除阻塞状态的线程
*** CLH是(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列，[用于解决同步状态管理]，AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node）来实现锁的分配。
*** 每个使用LockSupport的线程都会与一个许可关联，如果该许可可用，并且可在进程中使用，则调用park()将会立即返回，否则可能阻塞。如果许可尚不可用，则可以调用 unpark 使其可用
*** [#A] AQS详细
**** 简介
***** AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，
      SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。
***** Sync extends AbstractQueuedSynchronizer   （sync是同步器）
***** 1.FairSync extends Sync   2.NonfairSync extends Sync
***** readerLock = new ReadLock(sync);
***** 唤醒使用locksupport.unpark(s.thread);  -> unsafe.unpark(thread); 调用native，使用到来mutex
**** AQS底层使用了模板方法模式
***** 使用者继承AbstractQueuedSynchronizer并重写指定的方法。（这些重写方法很简单，无非是对于共享资源state的获取和释放）
***** 将AQS组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。
***** 自定义同步器时需要重写下面几个AQS提供的模板方法：
****** isHeldExclusively()//该线程是否正在独占资源。只有用到condition才需要去实现它。
****** tryAcquire(int)//独占方式。尝试获取资源，成功则返回true，失败则返回false。
****** tryRelease(int)//独占方式。尝试释放资源，成功则返回true，失败则返回false。
****** tryAcquireShared(int)//共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。
****** tryReleaseShared(int)//共享方式。尝试释放资源，成功则返回true，失败则返回false。
**** AQS 原理概览
***** AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，
      这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。
***** CLH(Craig,Landin,and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS是将每条请求共享资源的线程封装成一个CLH锁队列的一个结点（Node）来实现锁的分配。
***** AQS使用一个int成员变量来表示同步状态，通过内置的FIFO队列来完成获取资源线程的排队工作。AQS使用CAS对该同步状态进行原子操作实现对其值的修改。
****** private volatile int state;//共享变量，使用volatile修饰保证线程可见性
***** 状态信息通过protected类型的getState，setState，compareAndSetState进行操作
**** AQS 对资源的共享方式
***** Exclusive（独占）：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁：
****** 公平锁：按照线程在队列中的排队顺序，先到者先拿到锁
****** 非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的
***** Share（共享）：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatch、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。
**** 自定义同步器
***** 不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。
**** condition使用(与object的wait相比Condition则可通过多个Condition实例对象建立更加精细的线程控制)
***** condition.await线程进入等待队列
***** condition.signal被通知线程进入同步队列
*** reentrantlock
**** 以ReentrantLock为例，state初始化为0，表示未锁定状态。A线程lock()时，会调用tryAcquire()独占该锁并将state+1。此后，其他线程再tryAcquire()时就会失败，
     直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A线程自己是可以重复获取此锁的（state会累加），这就是可重入的概念。
     但要注意，获取多少次就要释放多么次，这样才能保证state是能回到零态的。
*** CountDownLatch(只能使用一次)
**** 任务分为N个子线程去执行，state也初始化为N（注意N要与线程个数一致）。这N个子线程是并行执行的，每个子线程执行完后countDown()一次，state会CAS(Compare and Swap)减1。
     等到所有子线程都执行完后(即state=0)，会unpark()主调用线程，然后主调用线程就会从await()函数返回，继续后余动作。
**** 用给定的计数 初始化 CountDownLatch。由于调用了 countDown() 方法，所以在当前计数到达零之前，await 方法会一直受阻塞。之后，会释放所有等待的线程，await 的所有后续调用都将立即返回。
     这种现象只出现一次——计数无法被重置。如果需要重置计数，请考虑使用 CyclicBarrier。
*** CyclicBarrier(可以循环使用)
**** 多线程结果合并的操作，用于多线程计算数据，最后合并计算结果的应用场景
**** 通俗讲：让一组线程到达一个屏障时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活
**** 底层采用ReentrantLock + Condition实现
*** Semaphore(获取资源 semaphore-1，若semaphore = 0则表示资源没有了)
**** 信号量Semaphore是一个非负整数（>=1）。当一个线程想要访问某个共享资源时，它必须要先获取Semaphore，当Semaphore >0时，获取该资源并使Semaphore – 1。如果Semaphore值 = 0，
     则表示全部的共享资源已经被其他线程全部占用，线程必须要等待其他线程释放资源。当线程释放资源时，Semaphore则+1
*** Exchanger
**** 可以在对中对元素进行配对和交换的线程的同步点允许在并发任务之间交换数据。具体来说，Exchanger类允许在两个线程之间定义同步点。当两个线程都到达同步点时，他们交换数据结构，因此第一个线程的数据结构进入到第二个线程中，第二个线程的数据结构进入到第一个线程中
*** CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待
*** [#A] ThreadLocal(线程私有、threadlocals是threadlocalmap对象、弱引用entry(k,v)、使用完需要调用remove清除数据、map中的key为当前ThreadLocal对象，value则是对应线程的变量副本、ThreadLocal实例本身是不存储值，它只是提供了一个在当前线程中找到副本值得key)
**** ThreadLocal 是一个本地线程副本变量工具类，在每个线程中都创建了一个 ThreadLocalMap 对象，简单说 ThreadLocal 就是一种以空间换时间的做法，
     每个线程可以访问自己内部 ThreadLocalMap 对象内的 value。通过这种方式，避免资源在多线程间共享。
**** ThreadLocal内存泄漏分析与解决方案
***** 每次使用完ThreadLocal，都调用它的remove()方法，清除数据。
***** 在使用线程池的情况下，没有及时清理ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。
*** Threadlocal使用场景(代替参数的显式传递、全局存储用户信息、解决线程安全问题[spring中dao层中装配的connection，原因：@Autowired注解默认使用单例模式])
**** 全局存储用户信息(拦截器的业务中， 获取到保存的用户信息，然后存入ThreadLocal)
**** 在Spring的Web项目中，我们通常会将业务分为Controller层，Service层，Dao层， 我们都知道@Autowired注解默认使用单例模式，那么不同请求线程进来之后，由于Dao层使用单例，那么负责数据库连接的Connection也只有一个，
     如果每个请求线程都去连接数据库，那么就会造成线程不安全的问题，Spring是如何解决这个问题的呢？
*** Fork/Join(大任务分割成若干小任务、最终汇总小任务结果，fork分解任务｜join收集数据)
**** 工作窃取
***** 某个线程从其他队列里窃取任务来执行
***** 执行块的线程帮助执行慢的线程执行任务，提升整个任务效率
***** 队列要采用双向队列
**** 核心类
***** ForkJoinPool	执行任务的线程池
***** ForkJoinTask	表示任务，用于ForkJoinPool的任务抽象
***** ForkJoinWorkerThread 执行任务的工作线程
*** CopyOnWriteArrayList(读多写少、最终一致性、会导致full gc和young gc)
**** CopyOnWriteArrayList(免锁容器)的好处之一是当多个迭代器同时遍历和修改这个列表时，不会抛出 ConcurrentModificationException。在CopyOnWriteArrayList 中，
     写入将导致创建整个底层数组的副本，而源数组将保留在原地，使得复制的数组在被修改时，读取操作可以安全地执行。
**** CopyOnWriteArrayList 的使用场景
***** 通过源码分析，我们看出它的优缺点比较明显，所以使用场景也就比较明显。就是合适读多写少的场景。
**** CopyOnWriteArrayList 的缺点
***** 由于写操作的时候，需要拷贝数组，会消耗内存，如果原数组的内容比较多的情况下，可能导致 young gc 或者 full gc。
***** 不能用于实时读的场景，像拷贝数组、新增元素都需要时间，所以调用一个 set 操作后，读取到数据可能还是旧的，虽然CopyOnWriteArrayList 能做到最终一致性,但是还是没法满足实时性要求。
***** 由于实际使用中可能没法保证 CopyOnWriteArrayList 到底要放置多少数据，万一数据稍微有点多，每次 add/set 都要重新复制数组，这个代价实在太高昂了。在高性能的互联网应用中，这种操作分分钟引起故障。
**** CopyOnWriteArrayList 的设计思想
***** 读写分离，读和写分开
***** 最终一致性
***** 使用另外开辟空间的思路，来解决并发冲突
** DONE 5	JVM面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104390752
   CLOSED: [2021-04-15 Thu 17:38]
*** class文件结构
****  4 个字节称为魔数 cafebabe
**** 第 5 和第 6 个字节是次版本号（MinorVersion），第 7 和第 8 个字节是主版本号（Major Version）
**** Java 的版本号是从45 开始的，JDK 1.1 之后的每个 JDK 大版本发布主版本号向上加 1 高版本的 JDK 能向下兼容以前版本的 Class 文件，但不能运行以后版本的 Class 文件，即使文件格式并未发生
     任何变化，虚拟机也必须拒绝执行超过其版本号的 Class 文件。
**** 常量池中主要存放两大类常量：字面量（Literal）和符号引用（Symbolic References）。
***** 字面量比较接近于 Java 语言层面的常量概念，如文本字符串、声明为 final 的常量值等。
***** 符号引用则属于编译原理方面的概念，包括了下面三类常量：
****** 类和接口的全限定名（Fully Qualified Name）
****** 字段的名称和描述符（Descriptor）
****** 方法的名称和描述符
**** javap -c class查看命令代码
*** 永久代是hotspot的概念、方法区是jvm的概念、对于Java8， HotSpots取消了永久代，使用元空间。
    存储位置不同，永久代物理是是堆的一部分，和新生代，老年代地址是连续的，而元空间属于本地内存；
    存储内容不同，元空间存储类的元信息，静态变量和常量池等并入堆中。相当于永久代的数据被分到了堆和元空间中。
*** 方法区主要用来存储已被虚拟机加载的类的信息、常量、静态变量和即时编译器编译后的代码等数据。
*** 方法区里有一个运行时常量池，用于存放静态编译产生的字面量和符号引用。该常量池具有动态性，也就是说常量并不一定是编译时确定，运行时生成的常量也会存在这个常量池中。
*** 参数
**** -XX
     标准选择（Standard Options）
     这些是 JVM 的所有实现都支持的最常用的选项。
***** -XX:SurvivorRatio=8	年轻代中Eden区与Survivor区的容量比例值，默认为8，即8:1
***** -XX:NewRatio=老年代/新生代	设置老年代和新生代的大小比例
***** -XX:PermSize	非堆内存初始大小，一般应用设置初始化200m，最大1024m就够了
***** -XX:MaxPermSize	非堆内存最大允许大小
***** -XX:NewSize（-Xns）	年轻代内存初始大小
***** -XX:MaxNewSize（-Xmn）	年轻代内存最大允许大小
***** -XX:+PrintGC	jvm启动后，只要遇到GC就会打印日志
***** -XX:+PrintGCDetails	查看GC详细信息，包括各个区的情况
***** -XX:MaxDirectMemorySize	在NIO中可以直接访问直接内存，这个就是设置它的大小，不设置默认就是最大堆空间的值-Xmx
***** -XX:+DisableExplicitGC	关闭System.gc()
***** -XX:MaxTenuringThreshold	垃圾可以进入老年代的年龄
***** -XX:TLABWasteTargetPercent	TLAB占eden区的百分比，默认是1%
***** -XX:+CollectGen0First	FullGC时是否先YGC，默认false
***** TLAB使用
***** -Xx:+UseTLAB	使用TLAB
***** -XX:+TLABSize	设置TLAB大小
***** -XX:TLABRefillWasteFraction	设置维护进入TLAB空间的单个对象大小，他是一个比例值，默认为64，即如果对象大于整个空间的1/64，则在堆创建
***** -XX:+PrintTLAB	查看TLAB信息
***** -Xx:ResizeTLAB	自调整TLABRefillWasteFraction阀值。
***** 回收器设置
****** 新生代
******* -XX:+UseSerialGC参数可以设置新生代使用这个串行回收器
******* -XX:+UseParNewGC参数可以设置新生代使用这个并行回收器
******* XX:ParallelGCThreads来设置线程数（ParNew，ParallelOld）
******* ParallelGC
******** -XX:+UseParallelGC参数可以设置新生代使用这个并行回收器
******** -XX:MaxGCPauseMillis：设置最大垃圾收集停顿时间，可用把虚拟机在GC停顿的时间控制在MaxGCPauseMillis范围内，如果希望减少GC停顿时间可以将MaxGCPauseMillis设置的很小，
         但是会导致GC频繁，从而增加了GC的总时间，降低了吞吐量。所以需要根据实际情况设置该值。
******** -Xx:GCTimeRatio：设置吞吐量大小，它是一个0到100之间的整数，默认情况下他的取值是99，那么系统将花费不超过1/(1+n)的时间用于垃圾回收，也就是1/(1+99)=1%的时间。
******** -XX:+UseAdaptiveSizePolicy打开自适应模式，在这种模式下，新生代的大小、eden、from/to的比例，以及晋升老年代的对象年龄参数会被自动调整，以达到在堆大小、吞吐量和停顿时间之间的平衡点。
****** 老年代
******* SerialOld 垃圾回收器
******* ParallelOldGC 回收器
******** -XX:+UseParallelOldGc进行设置老年代使用该回收器
******** -XX:+ParallelGCThreads也可以设置垃圾收集时的线程数量。 
******* CMS 回收器(concurrent mark sweep 同步标记清除算法)
******** 使用-XX:+UseConcMarkSweepGC进行设置老年代使用该回收器。
******** 使用-XX:ConcGCThreads设置并发线程数量。
******** CMS不会等到应用程序饱和的时候才去回收垃圾，而是在某一阀值的时候开始回收，回收阀值可用指定的参数进行配置：-XX:CMSInitiatingoccupancyFraction来指定，默认为68，
         也就是说当老年代的空间使用率达到68%的时候，会执行CMS回收。
******** 如果内存使用率增长的很快，在CMS执行的过程中，已经出现了内存不足的情况，此时CMS回收就会失败，虚拟机将启动老年代串行回收器；SerialOldGC进行垃圾回收，这会导致应用程序中断，直到垃圾回收完成后才会正常工作。
******** 这个过程GC的停顿时间可能较长，所以-XX:CMSInitiatingoccupancyFraction的设置要根据实际的情况。
******** 之前我们在学习算法的时候说过，标记清除法有个缺点就是存在内存碎片的问题，那么CMS有个参数设置-XX:+UseCMSCompactAtFullCollecion可以使CMS回收完成之后进行一次碎片整理。
******** -XX:CMSFullGCsBeforeCompaction参数可以设置进行多少次CMS回收之后，对内存进行一次压缩。
**** -X
     非标准选择（Non-Standard Options）
     这些选项是特定于 Java HotSpot 虚拟机的通用选项。
***** -Xms	堆内存初始大小，单位m、g
***** -Xmx	堆内存最大允许大小，一般不要大于物理内存的80%
***** -Xss	堆栈内存大小
***** -Xnoclassgc	禁用垃圾回收
*** SafePoint 是什么
**** 比如 GC 的时候必须要等到 Java 线程都进入到 safepoint 的时候 VMThread 才能开始执行 GC
**** safepoint位置
***** 1- 循环的末尾 (防止大循环的时候一直不进入 safepoint，而其他线程在等待它进入safepoint)
***** 2- 方法返回前
***** 3- 调用方法的 call 之后
***** 4- 抛出异常的位置
*** [#B] JVM包含两个子系统和两个组件，两个子系统为Class loader(类装载)、Execution engine(执行引擎)；两个组件为Runtime data area(运行时数据区)、Native Interface(本地接口)。
**** Class loader(类装载)：根据给定的全限定名类名(如：java.lang.Object)来装载class文件到Runtime data area中的method area。
**** Execution engine（执行引擎）：执行classes中的指令。
**** Native Interface(本地接口)：与native libraries交互，是其它编程语言交互的接口。
**** Runtime data area(运行时数据区域)：这就是我们常说的JVM的内存。
*** Java程序运行机制步骤(java -> class -> java命令执行)
**** 首先利用IDE集成开发工具编写Java源代码，源文件的后缀为.java；
**** 再利用编译器(javac命令)将源代码编译成字节码文件，字节码文件的后缀名为.class；
**** 运行字节码的工作是由解释器(java命令)来完成的。
*** [#B] 运行时数据区(方法区、java堆、虚拟机栈、本地方法栈、pc程序计数器)
**** 方法区（类定义、string常量）
***** 用于存储已被虚拟机加载的类信息（包括类的名称、方法信息、字段信息）、常量、静态变量、即时编译后的代码等数据。
***** 在 Class 文件中除了类的字段、方法、接口等描述信息外，还有一项信息是常量池，用来存储编译期间生成的字面量和符号引用。
***** 运行时常量池，它是每一个类或接口的常量池的运行时表示形式，在类和接口被加载到 JVM 后，对应的运行时常量池就被创建出来。在运行期间也可将新的常量放入运行时常量池中，比如 String 的 intern 方法。
**** java堆
***** Java 虚拟机中内存最大的一块，是被所有线程共享的，几乎所有的对象实例都在这里分配内存；
**** 虚拟机栈
***** 用于存储局部变量表(基础数据、对象引用)、操作数栈(涉及计算相关)、动态链接、方法出口等信息；
***** 指向运行时常量池的引用，因为在方法执行的过程中有可能需要用到类中的常量，所以必须要有一个引用指向运行时常量
***** 栈深度大于已有深度：StackOverflowError[根据xss设置的大小来决定需要迭代多少次才会溢出]
***** 可扩展深度大于能够申请的内存：OutOfMemoryError
**** 本地方法栈
***** 与虚拟机栈的作用是一样的，只不过虚拟机栈是服务 Java 方法的，而本地方法栈是为虚拟机调用 Native 方法服务的；
***** HotSopt 虚拟机中直接就把本地方法栈和 Java 栈合二为一
**** pc程序计数器
***** 当前线程所执行的字节码的行号指示器，字节码解析器的工作是通过改变这个计数器的值，来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能，都需要依赖这个计数器来完成；
**** 直接内存（线程共享）
***** NIO,使用 native 函数库直接分配堆外内存，不经过 JVM 内存直接访问系统物理内存的类——DirectBuffer。
****** 1. 直接内存申请空间耗费更高的性能，当频繁申请到一定量时尤为明显
****** 2. 直接内存 IO 读写的性能要优于普通的堆内存，在多次读写操作的情况下差异明显
*** 说一下堆栈的区别(堆线程共享、堆内存不连续、大小不固定、需要各种回收算法、存放对象实例，栈存放局部变量、操作数栈)
**** 物理地址
***** 堆的物理地址分配对对象是不连续的。因此性能慢些。在GC的时候也要考虑到不连续的分配，所以有各种算法。比如，标记-消除，复制，标记-整理，分代（即新生代使用复制算法，老年代使用标记——整理）
***** 栈使用的是数据结构中的栈，先进后出的原则，物理地址分配是连续的。所以性能快。
**** 内存分别
***** 堆因为是不连续的，所以分配的内存是在运行期确认的，因此大小不固定。一般堆大小远远大于栈。
***** 栈是连续的，所以分配的内存大小要在编译期就确认，大小是固定的。
**** 存放的内容
***** 堆存放的是对象的实例和数组。因此该区更关注的是数据的存储
***** 栈存放：局部变量，操作数栈，返回结果。该区更关注的是程序方法的执行。
**** PS：
***** 静态变量放在方法区
***** 静态的对象还是放在堆。
**** 程序的可见度
***** 堆对于整个应用程序都是共享、可见的。
***** 栈只对于线程是可见的。所以也是线程私有。他的生命周期和线程相同。
*** [#B] 对象的创建(new、class.newInstance、Constructor.newInstance、clone、反序列化、unsafe的allocateInstance方法)
**** 使用new关键字	调用了构造函数
**** 使用Class的newInstance方法	调用了构造函数
**** 使用Constructor类的newInstance方法	调用了构造函数
**** 使用clone方法	没有调用构造函数
**** 使用反序列化	没有调用构造函数
*** 为对象分配内存(指针碰撞、空闲列表)
**** 类加载完成后，接着会在Java堆中划分一块内存分配给对象。内存分配根据Java堆是否规整，有两种方式：
***** 指针碰撞：如果Java堆的内存是规整，即所有用过的内存放在一边，而空闲的的放在另一边。分配内存时将位于中间的指针指示器向空闲的内存移动一段与对象大小相等的距离，这样便完成分配内存工作。
***** 空闲列表：如果Java堆的内存不是规整的，则需要由虚拟机维护一个列表来记录那些内存是可用的，这样在分配的时候可以从列表中查询到足够大的内存分配给对象，并在分配后更新列表记录。
*** 内存分配处理并发安全问题(1.cas+失败重试 2.每个线程预先分配TLAB)
**** 对分配内存空间的动作进行同步处理（采用 CAS + 失败重试来保障更新操作的原子性）；
**** 把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在 Java 堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer, TLAB）。哪个线程要分配内存，
     就在哪个线程的 TLAB 上分配。只有 TLAB 用完并分配新的 TLAB 时，才需要同步锁。通过-XX:+/-UserTLAB参数来设定虚拟机是否使用TLAB。
*** 对象的访问定位(指针[直接指向对象，需考虑如何放置类型数据，hotspot中使用]、句柄[存放对象类型和对象实例地址，句柄地址稳定，需两次指针定位])
**** Java程序需要通过 JVM 栈上的引用访问堆中的具体对象。对象的访问方式取决于 JVM 虚拟机的实现。目前主流的访问方式有 句柄 和 直接指针 两种方式。
**** 句柄访问
***** Java堆中划分出一块内存来作为句柄池，引用中存储对象的句柄地址，而句柄中包含了对象实例数据与对象类型数据各自的具体地址信息
***** 优势：引用中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而引用本身不需要修改。
**** 直接指针
***** 如果使用直接指针访问，引用 中存储的直接就是对象地址，那么Java堆对象内部的布局中就必须考虑如何放置访问类型数据的相关信息。
***** 优势：速度更快，节省了一次指针定位的时间开销。由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是非常可观的执行成本。HotSpot 中采用的就是这种方式。
*** 内存泄漏(长生命周期持有短生命周期、hashmap等容器)
**** ①长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收。
**** ②集合中的内存泄漏，比如 HashMap、ArrayList 等，这些对象经常会发生内存泄露。比如当它们被声明为静态对象时，它们的生命周期会跟应用程序的生命周期一样长，很容易造成内存不足。
*** Java 中都有哪些引用类型？(强软弱虚)
**** 强引用：发生 gc 的时候不会被回收。
**** 软引用：有用但不是必须的对象，在发生内存溢出之前会被回收。
**** 弱引用：有用但不是必须的对象，在下一次GC时会被回收。
**** 虚引用（幽灵引用/幻影引用）：无法通过虚引用获得对象，用 PhantomReference 实现虚引用，虚引用的用途是在 gc 时返回一个通知。
*** JVM中的永久代中会发生垃圾回收吗(一般不会，当永久代满了会有一次full gc，会回收方法区里的常量和字节码)
**** 垃圾回收不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。如果你仔细查看垃圾收集器的输出信息，就会发现永久代也是被回收的。
     这就是为什么正确的永久代大小对避免Full GC是非常重要的原因。请参考下Java8：从永久代到元数据区
**** (译者注：Java8中已经移除了永久代，新加了一个叫做元数据区的native内存区)
*** [#B] JVM 有哪些垃圾回收算法(标记-清除算法、复制算法、标记-整理算法、分代算法)
**** 标记-清除算法：标记无用对象，然后进行清除回收。缺点：效率不高，无法清除垃圾碎片。
**** 复制算法：按照容量划分二个大小相等的内存区域，当一块用完的时候将活着的对象复制到另一块上，然后再把已使用的内存空间一次清理掉。缺点：内存使用率不高，只有原来的一半。
**** 标记-整理算法：标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。
**** 分代算法：根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代，新生代基本采用复制算法，老年代采用标记整理算法。
*** [#B] 说一下 JVM 有哪些垃圾回收器(Serial[单线程]、ParNew[多线程]、Parallel Scavenge[多线程吞吐量，即用户代码cpu耗时比例]、Serial Old、Parallel Old、CMS[多线程停顿时间]、G1)
**** Serial 收集器
***** 大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工
           作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ "Stop The World" ），直到它收集结束。
***** 新生代采用复制算法，老年代采用标记-整理算法
**** ParNew 收集器
***** ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）
           和 Serial 收集器完全一样。
***** 新生代采用复制算法，老年代采用标记-整理算法。
**** Parallel Scavenge 收集器
***** Parallel Scavenge 收集器也是使用复制算法的多线程收集器，它看上去几乎和ParNew都一样。 那么它有什么特别之处呢？
***** Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体
           验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值
***** 新生代采用复制算法，老年代采用标记-整理算法。
**** Serial Old 收集器
***** Serial 收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在 JDK1.5 以及以前的版本中与 Parallel
           Scavenge 收集器搭配使用，另一种用途是作为 CMS 收集器的后备方案。
**** Parallel Old 收集器(1.8默认)
***** Parallel Scavenge 收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及 CPU 资源的场合，都可以优先考虑
      Parallel Scavenge 收集器和 Parallel Old 收集器(jdk1.8默认)。
**** CMS 收集器
***** CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。
***** CMS（Concurrent Mark Sweep）收集器是 HotSpot 虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程
           （基本上）同时工作。
***** CMS 收集器是一种 “标记-清除”算法实现的,整个过程分为四个步骤：
****** 初始标记： 暂停所有的其他线程，并记录下直接与 root 相连的对象，速度很快 ；
****** 并发标记： 同时开启 GC 和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的
             可达对象。因为用户线程可能会不断的更新引用域，所以 GC 线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更
             新的地方。
****** 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段
             的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短
****** 并发清除： 开启用户线程，同时 GC 线程开始对为标记的区域做清扫。
**** G1 收集器（1.9默认）
***** G1 (Garbage-First) 是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足 GC 停顿时间要求的
           同时,还具备高吞吐量性能特征.
***** 并行与并发：G1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分
           其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行。
***** 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。
***** 空间整合：与 CMS 的“标记--清理”算法不同，G1 从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。
***** 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可
           预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。
***** 4步
****** 初始标记
****** 并发标记
****** 最终标记
****** 筛选回收
*** [#B] 类加载过程(加载[二进制字节流存入方法区，主存生成class对象]-验证[文件格式、元数据、字节码、符号引用]-准备[类变量分配内存及初始化零值]-解析[常量池符号引用{接口、方法、字段、方法类型、方法句柄}替换为直接引用]-初始化[调用clinit方法])
**** 类加载过程(加载-连接-初始化)
***** 系统加载 Class 类型的文件主要三步:加载->连接->初始化，连接过程又可分为三步:验证->准备->解析。
***** 加载
****** 通过全类名获取定义此类的二进制字节流
****** 将字节流所代表的静态存储结构转换为方法区的运行时数据结构
****** 在内存中生成一个代表该类的 Class 对象,作为方法区这些数据的访问入口
***** 验证
****** 文件格式验证：验证字节流是否符合 Class 文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.
****** 元数据验证:对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。
****** 字节码验证：是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。
****** 符号引用验证：这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。
***** 准备
****** 准备阶段是正式为类静态变量分配内存并设置类变量初始值的阶段
***** 解析
****** 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行。
***** 初始化
****** 初始化是类加载的最后一步，也是真正执行类中定义的 Java 程序代码(字节码)，初始化阶段是执行类构造器 <clinit> ()方法的过程。
**** 类加载器
***** BootstrapClassLoader(启动类加载器)
****** 最顶层的加载类，由C++实现，负责加载 %JAVA_HOME%/lib目录下的jar包和类或者或被 -Xbootclasspath参数指定的路径中的所有类。
***** ExtensionClassLoader(扩展类加载器)
****** ExtensionClassLoader(扩展类加载器) ：主要负责加载目录 %JRE_HOME%/lib/ext 目录下的jar包和类，或被 java.ext.dirs 系统变量所指定
          的路径下的jar包。
***** AppClassLoader(应用程序类加载器)
****** AppClassLoader(应用程序类加载器) :面向我们用户的加载器，负责加载当前应用classpath下的所有jar包和类。
***** 双亲委派模型
****** 每一个类都有一个对应它的类加载器。系统中的 ClassLoder 在协同工作的时候会默认使用 双亲委派模型 。即在类加载的时候，系统会首先判
          断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。加载的时候，首先会把该请求委派该父类加载器的 loadClass() 处理，
          因此所有的请求最终都应该传送到顶层的启动类加载器 BootstrapClassLoader 中。当父类加载器无法处理时，才由自己来处理。当父类加载器为
          null时，会使用启动类加载器 BootstrapClassLoader 作为父类加载器。
***** 自定义类加载器
****** 除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader。如果我们要自定义自己的类加载器，很明显
       需要继承 ClassLoader。
*** 如何打破双亲委派模型？(SPI[线程上下文类加载器]、自定义类加载器重写loadClass或findClass方法)
**** （1）：自定义类加载器，继承ClassLoader类重写loadClass方法；
**** （2）：SPI (Service Provider interface）[线程上下文类加载器就是使用的这个原理]
***** （1）：服务提供接口（服务发现机制）：
***** （2）：通过加载ClassPath下META_INF/services，自动加载文件里所定义的类
***** （3）：通过ServiceLoader.load/Service.providers方法通过反射拿到实现类的实例
***** SPI应用?
****** （1）：应用于JDBC获取数据库驱动连接过程就是应用这一机制
****** （2）：apache最早提供的common-logging只有接口.没有实现..发现日志的提供商通过SPI来具体找到日志提供商实现类
*** tomcat是如何打破双亲委派模型
**** tomcat有着特殊性，它需要容纳多个应用，需要做到应用级别的隔离，而且需要减少重复性加载，所以划分为：/common 容器和应用共享的类信息，/server容器本身的类信息，/share应用通用的类信息,
     /WEB-INF/lib应用级别的类信息。整体可以分为：boostrapClassLoader->ExtensionClassLoader->ApplicationClassLoader->CommonClassLoader->
     CatalinaClassLoader（容器本身的加载器）/ShareClassLoader（共享的）->WebAppClassLoader。虽然第一眼是满足双亲委派模型的，但是不是的，因为双亲委派模型是要先提交给父类装载，
     而tomcat是优先判断是否是自己负责的文件位置，进行加载的。
*** 双亲委派机制缺陷?(父加载器加载当类无法调用子加载器加载的类，无法实现代码热替换和模块热部署)
**** （1）：双亲委派核心是越基础的类由越上层的加载器进行加载， 基础的类总是作为被调用代码调用的API，无法实现基础类调用用户的代码….
**** ⼦类类加载器加载的类能访问⽗类类加载器加载的类，⽽⽗类类加载器加载的类⽆法访问⼦类类加载器加载的类，从而引入了线程上下⽂类加载器
**** 当⽤户需要程序的动态性，⽐如代码热替换、模块热部署等时，双亲委派模型就不再适⽤，类加载器会发展为更为复杂的⽹状结构。
**** （2）：JNDI服务它的代码由启动类加载器去加载，但是他需要调独立厂商实现的应用程序，如何解决? 线程上下文件类加载器（Thread Context ClassLoader），
     JNDI服务使用这个线程上下文类加载器去加载所需要的SPI代码，也就是父类加载器请求子类加载器去完成类加载动作Java中所有涉及SPI的加载动作基本上都采用这种方式，例如JNDI，JDBC
*** 对象创建过程(类加载、内存分配、初始化零值、设置对系头、执行init方法)
**** 类加载检查： 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代
       表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。
**** 分配内存： 对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方
       式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理
       功能决定。
**** 初始化零值: 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代
       码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。
**** 设置对象头: 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希
       码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置
       方式。
**** 执行 init 方法: 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，
       <init> 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 <init> 方法，把对象按照程序员的意愿进行初始化，
       这样一个真正可用的对象才算完全产生出来。
*** 对象的内存布局(对象头、实例数据、对齐填充)
**** 对象头、实例数据和对齐填充
**** 对象头： Hotspot 虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的自身运行时数据（哈希码、GC 分代年龄、锁状态标志等等），
       另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。
**** 实例数据：该部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。
**** 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。
*** 堆外内存的优缺点(不受gc控制、内存泄漏难排查、速度快、内存可以超出jvm设置大小)
**** Ehcache中的一些版本，各种 NIO 框架，Dubbo，Memcache 等中会用到，NIO包下ByteBuffer来创建堆外内存 堆外内存，其实就是不受JVM控制的内存。
**** 相比于堆内内存有几个优势：
***** 减少了垃圾回收的工作，因为垃圾回收会暂停其他的工作。加快了复制的速度。因为堆内在 flush 到远程时，会先复制到直接内存（非堆内存），然后在发送；而堆外内存相当于省略掉了复制这项工作。
      可以扩展至更大的内存空间。比如超过 1TB 甚至比主存还大的空间。
**** 缺点总结如下：
***** 堆外内存难以控制，如果内存泄漏，那么很难排查，通过-XX：MaxDirectMemerySize来指定，当达到阈值的时候，调用system.gc来进行一次full gc 堆外内存相对来说，不适合存储很复杂的对象。
      一般简单的对象或者扁平化的比较适合 jstat查看内存回收概况，实时查看各个分区的分配回收情况， jmap查看内存栈，查看内存中对象占用大小， jstack查看线程栈，死锁，性能瓶颈
*** java是根据什么来执行可达性分析的(GC roots[虚拟机栈和本地方法栈的对象应用、方法区中的类变量引用{static}、方法区中的常量引用])
**** 根据GC ROOTS。GC ROOTS可以的对象有：虚拟机栈中的引用对象，方法区的类变量的引用，方法区中的常量引用，本地方法栈中的对象引用。
*** java 内存模型(JMM是线程间通信的控制机制，为解决线程工作内存与主内存数据不一致问题，线程将本地数据写回主内存、从主内存读取数据到本地缓存中,原因是：cpu多级缓存导致的)
**** 线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM 的一个抽象概念，
     并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。
*** JMM指令重排 
**** 编译器优化的重排(编译器重排)
***** 编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。(as-if-serial)
**** 处理器重排
***** 处理器指令重排是对CPU的性能优化，从指令的执行角度来说一条指令可以分为多个步骤完成 
****** 取指 IF
****** 译码和取寄存器操作数 ID
****** 执行或者有效地址计算 EX
****** 存储器访问 MEM
****** 写回 WB
***** 指令并行的重排(处理器重排)
****** 现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性(即后一个执行的语句无需依赖前面执行的语句的结果)，处理器可以改变语句对应的机器指令的执行顺序(happen-before)
***** 内存系统的重排(处理器重排)
****** 由于处理器使用缓存和读写缓存冲区，这使得加载(load)和存储(store)操作看上去可能是在乱序执行，因为三级缓存的存在，导致内存与缓存的数据同步存在时间差。
**** happen-before原则
***** 程序顺序原则，即在一个线程内必须保证语义串行性，也就是说按照代码顺序执行。
***** 锁规则 解锁(unlock)操作必然发生在后续的同一个锁的加锁(lock)之前，也就是说，如果对于一个锁解锁后，再加锁，那么加锁的动作必须在解锁动作之后(同一个锁)。
***** volatile规则 volatile变量的写，先发生于读，这保证了volatile变量的可见性，简单的理解就是，volatile变量在每次被线程访问时，都强迫从主内存中读该变量的值，而当该变量发生变化时，又会强迫将最新的值刷新到主内存，
      任何时刻，不同的线程总是能够看到该变量的最新值。
***** 线程启动规则 线程的start()方法先于它的每一个动作，即如果线程A在执行线程B的start方法之前修改了共享变量的值，那么当线程B执行start方法时，线程A对共享变量的修改对线程B可见
***** 传递性 A先于B ，B先于C 那么A必然先于C
***** 线程终止规则 线程的所有操作先于线程的终结，Thread.join()方法的作用是等待当前执行的线程终止。假设在线程B终止之前，修改了共享变量，线程A从线程B的join方法成功返回后，线程B对共享变量的修改将对线程A可见。
***** 线程中断规则 对线程 interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测线程是否中断。
***** 对象终结规则 对象的构造函数执行，结束先于finalize()方法
*** finalize() 的工作就是回收JNI产生的这部分内存(非java)。
*** 什么是分布式垃圾回收（DGC）？它是如何工作的？
**** DGC 叫做分布式垃圾回收。RMI 使用 DGC 来做自动垃圾回收。因为 RMI 包含了跨虚拟机的远程对象的引用，垃圾回收是很困难的。DGC 使用引用计数算法来给远程对象提供自动内存管理。
** DONE 6	Spring面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104397516 CLOSED: [2021-04-14 Wed 11:08]
*** aop(日志，事务管理，权限控制,性能监测， 核心业务功能和切面功能分别独立进行开发 ，然后把切面功能和核心业务功能 "编织" 在一起，这就叫AOP)
*** DI中@Autowired优先byType, @Resource优先byName
*** @Configuration 作用于类上，相当于一个xml配置文件； @Bean 作用于方法上，相当于xml配置中的<bean>；
*** 模块划分图 https://img-blog.csdnimg.cn/2019102923475419.png
*** bean的生命周期 https://img-blog.csdnimg.cn/201911012343410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RoaW5rV29u,size_16,color_FFFFFF,t_70
*** 两个核心特性，依赖注入（dependency injection，DI）和面向切面编程（aspect-oriented programming，AOP）
*** 为了降低Java开发的复杂性，Spring采取了以下4种关键策略:
**** 基于POJO的轻量级和最小侵入性编程；
**** 通过依赖注入和面向接口实现松耦合；
**** 基于切面和惯例进行声明式编程；
**** 通过切面和模板减少样板式代码。
*** 几大模块(core container、aop、web、test、jdbc、jms)
**** 核心容器（Core Container） 
**** AOP（Aspect Oriented Programming）
**** 设备支持（Instrmentation） 数据访问与集成（Data Access/Integeration）  
**** Web 
**** 消息（Messaging）  
**** Test
*** [#A] 具体介绍
**** spring core：提供了框架的基本组成部分，包括控制反转（Inversion of Control，IOC）和依赖注入（Dependency Injection，DI）功能。
**** spring beans：提供了BeanFactory，是工厂模式的一个经典实现，Spring将管理对象称为Bean。
**** spring context：构建于 core 封装包基础上的 context 封装包，提供了一种框架式的对象访问方法。
**** spring jdbc：提供了一个JDBC的抽象层，消除了烦琐的JDBC编码和数据库厂商特有的错误代码解析， 用于简化JDBC。
**** spring aop：提供了面向切面的编程实现，让你可以自定义拦截器、切点等。
**** spring Web：提供了针对 Web 开发的集成特性，例如文件上传，利用 servlet listeners 进行 ioc 容器初始化和针对 Web 的 ApplicationContext。
**** spring test：主要为测试提供支持的，支持使用JUnit或TestNG对Spring组件进行单元测试和集成测试
*** 用到了哪些设计模式
**** 工厂模式：BeanFactory就是简单工厂模式的体现，用来创建对象的实例；
**** 单例模式：Bean默认为单例模式。
**** 代理模式：Spring的AOP功能用到了JDK的动态代理和CGLIB字节码生成技术；
**** 模板方法：用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。
**** 观察者模式：定义对象键一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都会得到通知被制动更新，如Spring中listener的实现–ApplicationListener。
*** 详细讲解一下核心容器（spring context应用上下文) 模块
**** 这是基本的Spring模块，提供spring 框架的基础功能，BeanFactory 是 任何以spring为基础的应用的核心。Spring 框架建立在此模块之上，它使Spring成为一个容器。
**** Bean 工厂是工厂模式的一个实现，提供了控制反转功能，用来把应用的配置和依赖从真正的应用代码中分离。最常用的就是org.springframework.beans.factory.xml.XmlBeanFactory ，
    它根据XML文件中的定义加载beans。该容器从XML 文件读取配置元数据并用它去创建一个完全配置的系统或应用。
*** Spring框架中有哪些不同类型的事件(如果一个bean实现了ApplicationListener接口，当一个ApplicationEvent 被发布以后，bean会自动被通知。)
**** 上下文更新事件（ContextRefreshedEvent）：在调用ConfigurableApplicationContext 接口中的refresh()方法时被触发。
**** 上下文开始事件（ContextStartedEvent）：当容器调用ConfigurableApplicationContext的Start()方法开始/重新开始容器时触发该事件。
**** 上下文停止事件（ContextStoppedEvent）：当容器调用ConfigurableApplicationContext的Stop()方法停止容器时触发该事件。
**** 上下文关闭事件（ContextClosedEvent）：当ApplicationContext被关闭时触发该事件。容器被关闭时，其管理的所有单例Bean都被销毁。
**** 请求处理事件（RequestHandledEvent）：在Web应用中，当一个http请求（request）结束触发该事件。
*** Spring 应用程序有哪些不同组件？ (接口、bean类、bean配置文件、spring aop、用户程序)
**** 接口 - 定义功能。
**** Bean 类 - 它包含属性，setter 和 getter 方法，函数等。
**** Bean 配置文件 - 包含类的信息以及如何配置它们。
**** Spring 面向切面编程（AOP） - 提供面向切面编程的功能。
**** 用户程序 - 它使用接口。
*** 使用 Spring 有哪些方式?(web应用程序、第三方web框架、企业级java bean、远程使用)
**** 作为一个成熟的 Spring Web 应用程序。
**** 作为第三方 Web 框架，使用 Spring Frameworks 中间层。
**** 作为企业级 Java Bean，它可以包装现有的 POJO（Plain Old Java Objects）。
**** 用于远程使用。
*** 什么是Spring IOC 容器？(将对象调用权交给容器、负责创建、配置、管理【生命周期】对象)
**** 控制反转即IoC (Inversion of Control)，它把传统上由程序代码直接操控的对象的调用权交给容器，通过容器来实现对象组件的装配和管理。
    所谓的“控制反转”概念就是对组件对象控制权的转移，从程序代码本身转移到了外部容器。
**** Spring IOC 负责创建对象，管理对象（通过依赖注入（DI），装配对象，配置对象，并且管理这些对象的整个生命周期。
*** Spring 容器提供配置元数据(xml、注解、java配置)
**** XML配置文件
**** 基于注解的配置
**** 基于java的配置
*** Spring基于xml注入bean的几种方式(set、构造器、静态工厂、实例工厂)
**** Set方法注入；
**** 构造器注入：①通过index设置参数的位置；②通过type设置参数类型；
**** 静态工厂注入；
**** 实例工厂；
*** bean的作用域(singleton、prototype、request、session、global-session)
**** singleton : bean在每个Spring ioc 容器中只有一个实例。
**** prototype：一个bean的定义可以有多个实例。
**** request：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。
**** session：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。
**** global-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。
*** Spring框架中的单例bean是线程安全的吗？(不安全，没有进行多线程封装，但是多数是无状态的，也可以认为是安全的，可以改变作用域为prototype)
**** 不是，Spring框架中的单例bean不是线程安全的。
**** spring 中的 bean 默认是单例模式，spring 框架并没有对单例 bean 进行多线程的封装处理
**** 实际上大部分时候 spring bean 无状态的（比如 dao 类），所有某种程度上来说 bean 也是安全的，但如果 bean 有状态的话（比如 view model 对象），那就要开发者自己去保证线程安全了，
    最简单的就是改变 bean 的作用域，把“singleton”变更为“prototype”，这样请求 bean 相当于 new Bean()了，所以就可以保证线程安全了。
*** Spring如何处理线程并发问题(单例bean无状态、bean中非线程安全的使用threadlocal进行处理)
**** 在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域，因为Spring对一些Bean中非线程安全状态采用ThreadLocal进行处理，解决线程安全问题。
*** 控制反转(IoC)有什么作用(解耦、管理对象的创建和依赖关系的维护、托管类实例过程)
**** 管理对象的创建和依赖关系的维护。
**** 解耦，由容器去维护具体的对象
**** 托管了类的产生过程，比如我们需要在类的产生过程中做一些处理，最直接的例子就是代理，如果有容器程序可以把这部分处理交给容器，应用程序则无需去关心类是如何完成代理的
*** IOC的优点是什么？(降低代码量、易于测试、解耦、懒加载)
**** IOC 或 依赖注入把应用的代码量降到最低。
**** 它使应用容易测试，单元测试不再需要单例和JNDI查找机制。
**** 最小的代价和最小的侵入性使松散耦合得以实现。
**** IOC容器支持加载服务时的饿汉式初始化和懒加载。
*** Spring IoC 的实现机制(工厂模式+反射机制, 接口注入【spring4已废弃】、setter注入、构造器注入)
**** Spring 中的 IoC 的实现原理就是工厂模式加反射机制。
**** 依赖注入是IoC最理想的实现
**** 依赖注入分为接口注入（Interface Injection），Setter方法注入（Setter Injection）和构造器注入（Constructor Injection）三种方式
*** Spring 的 IoC支持哪些功能  (对于 IoC 来说，最重要的就是容器。容器管理着 Bean 的生命周期，控制着 Bean 的依赖注入。)
**** 依赖注入
***** 接口注入(spring4已废弃)、 Setter 注入和构造器注入
***** 两种依赖方式都可以使用，构造器注入和Setter方法注入。最好的解决方案是用构造器参数实现强制依赖，setter方法实现可选依赖。
**** 依赖检查
**** 自动装配
**** 支持集合
**** 指定初始化方法和销毁方法
**** 支持回调某些方法（但是需要实现 Spring 接口，略有侵入）
*** [#A] BeanFactory 和 ApplicationContext有什么区别？ 
   继承图 https://img-blog.csdnimg.cn/20191105111441363.png
**** BeanFactory和ApplicationContext是Spring的两大核心接口，都可以当做Spring的容器。其中ApplicationContext是BeanFactory的子接口。
**** BeanFactory：是Spring里面最底层的接口，包含了各种Bean的定义，读取bean配置文档，管理bean的加载、实例化，控制bean的生命周期，维护bean之间的依赖关系。
**** ApplicationContext接口作为BeanFactory的派生，除了提供BeanFactory所具有的功能外，还提供了更完整的框架功能：
***** 继承MessageSource，因此支持国际化。【继承MessageSource】
***** 统一的资源文件访问方式。【继承ResourcePatternResolver->ResourceLoader】
***** 提供在监听器中注册bean的事件。【继承ApplicationEventPublisher】
***** 同时加载多个配置文件。
***** 载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层。
**** 加载方式(applicationcontext启动时全部加载，可以检查配置错误，就是比较消耗内存)
***** BeanFactroy采用的是延迟加载形式来注入Bean的，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化。(配置错误不容易发现)
***** ApplicationContext，它是在容器启动时，一次性创建了所有的Bean。这样，在容器启动时，我们就可以发现Spring中存在的配置错误，这样有利于检查所依赖属性是否注入。
***** 相对于基本的BeanFactory，ApplicationContext 唯一的不足是占用内存空间。当应用程序配置Bean较多时，程序启动较慢。
**** 创建方式
***** BeanFactory通常以编程的方式被创建
***** ApplicationContext还能以声明的方式创建, 如使用ContextLoader。
**** 注册方式(都支持beanpostprocessor，但区别是beanfactory需要手动注册)
***** BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册。
**** bean获取方式(hashmap使用)
***** BeanFactory 简单粗暴，可以理解为就是个 HashMap，Key 是 BeanName，Value 是 Bean 实例。通常只提供注册（put），获取（get）这两个功能。我们可以称之为 “低级容器”。
***** ApplicationContext 可以称之为 “高级容器”。因为他比 BeanFactory 多了更多的功能。他继承了多个接口。因此具备了更多的功能。例如资源的获取，支持多种消息（例如 JSP tag 的支持），
     对 BeanFactory 多了工具级别的支持等待。所以你看他的名字，已经不是 BeanFactory 之类的工厂了，而是 “应用上下文”， 代表着整个大容器的所有功能。该接口定义了一个 refresh 方法，
     此方法是所有阅读 Spring 源码的人的最熟悉的方法，用于刷新整个容器，即重新加载/刷新所有的 bean。
**** 什么是Spring的内部bean(仅被另一个bean当属性用，用setter和构造器注入实现，一般是匿名的，使用scope是prototype)
***** 在Spring框架中，当一个bean仅被用作另一个bean的属性时，它能被声明为一个内部bean。内部bean可以用setter注入“属性”和构造方法注入“构造参数”的方式来实现，内部bean通常是匿名的，
     它们的Scope一般是prototype。
**** 在 Spring中如何注入一个java集合(<list><set><map><props>)
***** 1、<list>注入列值-允许重复
***** 2、<set>注入组值-不允许重复
***** 3、<map>注入键值对-任意类型
***** 4、<props>名称和值都是字符串类型
**** 什么是bean装配(在Spring 容器中把bean组装到一起，前提是容器需要知道bean的依赖关系，如何通过依赖注入来把它们装配到一起)
**** [#A] 使用@Autowired注解自动装配的过程是怎样的
***** 在启动spring IoC时，容器自动装载了一个AutowiredAnnotationBeanPostProcessor后置处理器，当容器扫描到@Autowied、@Resource或@Inject时，就会在IoC容器自动查找需要的bean，
     并装配给该对象的属性。在使用@Autowired时，首先在容器中查询对应类型的bean：
****** 如果查询结果刚好为一个，就将该bean装配给@Autowired指定的数据；
****** 如果查询的结果不止一个，那么@Autowired会根据名称来查找；
****** 如果上述查找的结果为空，那么会抛出异常。解决方法时，使用required=false。
**** 自动装配有哪些局限性(重写、基本数据类型无法装配、模糊特性)
***** 重写：你仍需用 和 配置来定义依赖，意味着总要重写自动装配。
***** 基本数据类型：你不能自动装配简单的属性，如基本数据类型，String字符串，和类。
***** 模糊特性：自动装配不如显式装配精确，如果有可能，建议使用显式装配。
**** 你可以在Spring中注入一个null 和一个空字符串吗(可以)
*** [#A] 事务(required、supports、mandatory、requires_new、not_supported、never、nested【和require_new区别是nested外层失败会进行回滚】)
**** Spring支持的事务管理类型， spring事务实现方式有哪些
***** 编程式事务管理：这意味你通过编程的方式管理事务，给你带来极大的灵活性，但是难维护。
***** 声明式事务管理：这意味着你可以将业务代码和事务管理分离，你只需用注解和XML配置来管理事务。
**** Spring事务的实现方式和实现原理
***** Spring事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring是无法提供事务功能的。真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的
**** 说一下Spring的事务传播行为
***** ① PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。
***** ② PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。
***** ③ PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。
***** ④ PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。
***** ⑤ PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。
***** ⑥ PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。
***** ⑦ PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。(和require_new区别是nested外层失败会进行回滚)
****** 假设都是在一个REQUIRED类型的事务里调用这些事务，就像上面的例子，该REQUIRED类型方法调用抛出异常，REQUIRED_NEW的方法仍然可以提交，但是NESTED还要受到REQUIRED事务回滚而被迫回滚 
**** 说一下 spring 的事务隔离
***** ISOLATION_DEFAULT：用底层数据库的设置隔离级别，数据库设置的是什么我就用什么；
***** ISOLATION_READ_UNCOMMITTED：未提交读，最低隔离级别、事务未提交前，就可被其他事务读取（会出现幻读、脏读、不可重复读）；
***** ISOLATION_READ_COMMITTED：提交读，一个事务提交后才能被其他事务读取到（会造成幻读、不可重复读），SQL server 的默认级别；
***** ISOLATION_REPEATABLE_READ：可重复读，保证多次读取同一个数据时，其值都和事务开始时候的内容是一致，禁止读取到别的事务未提交的数据（会造成幻读），MySQL 的默认级别；
***** ISOLATION_SERIALIZABLE：序列化，代价最高最可靠的隔离级别，该隔离级别能防止脏读、不可重复读、幻读。
**** 脏读、幻读、不可重复读
***** 脏读 ：表示一个事务能够读取另一个事务中还未提交的数据。比如，某个事务尝试插入记录 A，此时该事务还未提交，然后另一个事务尝试读取到了记录 A。
***** 不可重复读 ：是指在一个事务内，多次读同一数据, 读取结果不相同。
***** 幻读 ：指同一个事务内多次查询返回的结果集不一样。比如同一个事务 A 第一次查询时候有 n 条记录，但是第二次同等条件下查询却有 n+1 条记录，这就好像产生了幻觉
*** [#A] AOP
**** 什么是AOP (用于将那些与业务无关，但却对[多个对象]产生影响的公共行为和逻辑，抽取并封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），减少系统中的重复代码，降低了模块间的耦合度，同时提高了系统的可维护性)
***** OOP(Object-Oriented Programming)面向对象编程，允许开发者定义纵向的关系，但并适用于定义横向的关系，导致了大量代码的重复，而不利于各个模块的重用。
***** AOP(Aspect-Oriented Programming)，一般称为面向切面编程，作为面向对象的一种补充，用于将那些与业务无关，但却对多个对象产生影响的公共行为和逻辑，
     抽取并封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），减少系统中的重复代码，降低了模块间的耦合度，同时提高了系统的可维护性。可用于权限认证、日志、事务处理等。
**** Spring AOP and AspectJ AOP 有什么区别？AOP 有哪些实现方式？(AspectJ是静态代理的增强,在编译期生产增强类，spring aop是动态代理)
***** AOP实现的关键在于 代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为AspectJ；动态代理则以Spring AOP为代表。
***** （1）AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强，他会在编译阶段将AspectJ(切面)织入到Java字节码中，运行的时候就是增强之后的AOP对象。
***** （2）Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，
     并且在特定的切点做了增强处理，并回调原对象的方法。
***** 静态代理与动态代理区别在于生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的性能，但是AspectJ需要特定的编译器进行处理，而Spring AOP则无需特定的编译器处理。
**** [#A] JDK动态代理和CGLIB动态代理的区别(jdk只支持接口代理,使用proxy和invocationhandler、cglib通过继承方式做动态代理，生成指定类的子类对象)
***** JDK动态代理只提供接口的代理，不支持类的代理。核心InvocationHandler接口和Proxy类，InvocationHandler 通过invoke()方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起；
     接着，Proxy利用 InvocationHandler动态创建一个符合某一接口的的实例, 生成目标类的代理对象。
***** 如果代理类没有实现 InvocationHandler 接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，
     可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并添加增强代码，从而实现AOP。CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。
**** (切片)Spring通知有哪些类型？(before、after、around、after-returning、after-throwing)
***** 前置通知（Before）：在目标方法被调用之前调用通知功能；
***** 后置通知（After）：在目标方法完成之后调用通知，此时不会关心方法的输出是什么；
***** 返回通知（After-returning ）：在目标方法成功执行之后调用通知；
***** 异常通知（After-throwing）：在目标方法抛出异常后调用通知；
***** 环绕通知（Around）：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为。
**** 动态代理代码实现
***** jdk动态代理
 public class JDKProxy<T> implements InvocationHandler{

    private Object target;

    public T getProxy(Object t){
        this.target = t;
        // 创建代理对象
        return (T) Proxy.newProxyInstance(t.getClass().getClassLoader(), t.getClass().getInterfaces(), this);
    }

    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        System.out.println("dynamic proxy start");
        method.invoke(target, args);
        System.out.println("dynamic proxy end");
        return null;
    }
 }
 //调用
 IWorkerService workerProxy = new JDKProxy<IWorkerService>().getProxy(new WorkerServiceImpl());
 workerProxy.doSomething();
***** cglib
public class CglibProxy implements MethodInterceptor{

   public <T> T getProxy(Class<T> clz){

       // 通过生成子类的方式代理
       Enhancer enhancer = new Enhancer();
       enhancer.setSuperclass(clz);
       enhancer.setCallback(this);

       T proxy = (T) enhancer.create();
       return proxy;
   }

   public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable {

       System.out.print("cglib proxy start");
       Object proxy = methodProxy.invokeSuper(o, objects);
       System.out.print("cglib proxy end");
       return proxy;
   }
}
//调用
IWorkerService cglibProxy = new CglibProxy().getProxy(WorkerServiceImpl.class);
cglibProxy.doSomething();
**** 三种代理区别
***** 静态代理类只能为一个被代理类服务，如果需要代理的类比较多，需要为每一个类实现代理类。静态代理类可以直接使用，效率较高。
***** jdk动态代理需要被代理类实现代理接口通过反射实现代理方法，消耗一定系统性能，但是无需编写过多代理类，避免代码重复，更加灵活。
***** Cglib实现原理是通过动态生成子类字节码来实现，比反射要快。由于cglib生成代理相当于被代理类的子类，所以被代理类不能是final类型，被代理的方法也不能是final类型。
** DONE 7	Spring MVC面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104397427
   CLOSED: [2021-04-14 Wed 11:12]
*** 什么是Spring MVC？简单介绍下你对Spring MVC的理解？(模型-视图-控制器分离)
**** Spring MVC是一个基于Java的实现了MVC设计模式的请求驱动类型的轻量级Web框架，通过把模型-视图-控制器分离，将web层进行职责解耦，把复杂的web应用分成逻辑清晰的几部分，
    简化开发，减少出错，方便组内开发人员之间的配合。
*** Spring MVC的优点(dispatcherServlet、handlerMapping、handlerAdapter【包含handler和intercept】、viewResolver【接收modelandview进行数据填充】、dispatcherServlet进行view渲染)
**** （1）可以支持各种视图技术,而不仅仅局限于JSP；
**** （2）与Spring框架集成（如IoC容器、AOP等）；
**** （3）清晰的角色分配：前端控制器(dispatcherServlet) , 请求到处理器映射（handlerMapping), 处理器适配器（HandlerAdapter), 视图解析器（ViewResolver）。
**** （4） 支持各种请求资源的映射策略。
*** [#A] 请描述Spring MVC的工作流程？描述一下 DispatcherServlet 的工作流程？
   图片： https://img-blog.csdnimg.cn/20200208211439106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RoaW5rV29u,size_16,color_FFFFFF,t_70
**** （1）用户发送请求至前端控制器DispatcherServlet；
**** （2） DispatcherServlet收到请求后，调用HandlerMapping处理器映射器，请求获取Handle；
**** （3）处理器映射器根据请求url找到具体的处理器，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet；
**** （4）DispatcherServlet 调用 HandlerAdapter处理器适配器；
**** （5）HandlerAdapter 经过适配调用 具体处理器(Handler，也叫后端控制器【用户的controller，此处使用reflect进行调用，若有切片则调用cglib生成增强类】)；
**** （6）Handler执行完成返回ModelAndView；
**** （7）HandlerAdapter将Handler执行结果ModelAndView返回给DispatcherServlet；
**** （8）DispatcherServlet将ModelAndView传给ViewResolver视图解析器进行解析；
**** （9）ViewResolver解析后返回具体View；
**** （10）DispatcherServlet对View进行渲染视图（即将模型数据填充至视图中）
**** （11）DispatcherServlet响应用户。(进行返回使用的是socketchannel，也就是nio，最后使用fileDispatchImpl.write0()方法调用native方法进行请求返回)
*** 注解原理是什么(继承annotation的接口、通过动态代理类调用)
**** 注解本质是一个继承了Annotation的特殊接口，其具体实现类是Java运行时生成的动态代理类。我们通过反射获取注解时，返回的是Java运行时生成的动态代理对象。
     通过代理对象调用自定义注解的方法，会最终调用AnnotationInvocationHandler的invoke方法。该方法会从memberValues这个Map中索引出对应的值。而memberValues的来源是Java常量池。
*** Spring MVC与Struts2区别
**** 相同点
***** 都是基于mvc的表现层框架，都用于web项目的开发。
**** 不同点
***** 1.前端控制器不一样。Spring MVC的前端控制器是servlet：DispatcherServlet。struts2的前端控制器是filter：StrutsPreparedAndExcutorFilter。
***** 2.请求参数的接收方式不一样。Spring MVC是使用方法的形参接收请求的参数，基于方法的开发，线程安全，可以设计为单例或者多例的开发，推荐使用单例模式的开发（执行效率更高），
     默认就是单例开发模式。struts2是通过类的成员变量接收请求的参数，是基于类的开发，线程不安全，只能设计为多例的开发。
***** 3.Struts采用值栈存储请求和响应的数据，通过OGNL存取数据，Spring MVC通过参数解析器是将request请求内容解析，并给方法形参赋值，将数据和视图封装成ModelAndView对象，
     最后又将ModelAndView中的模型数据通过reques域传输到页面。Jsp视图解析器默认使用jstl。
***** 4.与spring整合不一样。Spring MVC是spring框架的一部分，不需要整合。在企业项目中，Spring MVC使用更多一些。
*** Spring MVC里面拦截器是怎么写的
**** 有两种写法,一种是实现HandlerInterceptor接口，另外一种是继承适配器类，接着在接口方法当中，实现处理逻辑；然后在Spring MVC的配置文件中配置拦截器即可：
*** MVC 是一种设计模式,Spring MVC 是一款很优秀的 MVC 框架。Spring MVC 可以帮助我们进行更简洁的Web层的开发，并且它天生与 Spring 框架集成。Spring MVC 下我们一般把后端项目分为 Service层（处理业务）、
    Dao层（数据库操作）、Entity层（实体类）、Controller层(控制层，返回数据给前台页面)。
*** Spring 框架中用到了哪些设计模式
**** 工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。
**** 代理设计模式 : Spring AOP 功能的实现。
**** 单例设计模式 : Spring 中的 Bean 默认都是单例的。
**** 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。
**** 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。
**** 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。
**** 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。
*** @Component 和 @Bean 的区别是什么？
**** 作用对象不同: @Component 注解作用于类，而@Bean注解作用于方法。
**** @Component通常是通过类路径扫描来自动侦测以及自动装配到Spring容器中（我们可以使用 @ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个 bean,
     @Bean告诉了Spring这是某个类的示例，当我需要用它的时候还给我。
**** @Bean 注解比 Component 注解的自定义性更强，而且很多地方我们只能通过 @Bean 注解来注册bean。比如当我们引用第三方库中的类需要装配到 Spring容器时，则只能通过 @Bean来实现。
*** Spring 管理事务的方式有几种
**** 编程式事务，在代码中硬编码。(不推荐使用)
**** 声明式事务，在配置文件中配置（推荐使用）
***** 基于XML的声明式事务
***** 基于注解的声明式事务
** DONE 8	Spring Boot面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104397299
   CLOSED: [2021-04-14 Wed 11:14]
*** Spring Boot 中的监视器是什么
**** Spring boot actuator 是 spring 启动框架中的重要功能之一。Spring boot 监视器可帮助您访问生产环境中正在运行的应用程序的当前状态。
    有几个指标必须在生产环境中进行检查和监控。即使一些外部应用程序可能正在使用这些服务来向相关人员触发警报消息。监视器模块公开了一组可直接作为 HTTP URL 访问的REST 端点来检查状态。
*** websocket
**** WebSocket 是一种计算机通信协议，通过单个 TCP 连接提供全双工通信信道。
**** 1、WebSocket 是双向的 -使用 WebSocket 客户端或服务器可以发起消息发送。
**** 2、WebSocket 是全双工的 -客户端和服务器通信是相互独立的。
**** 3、单个 TCP 连接 -初始连接使用 HTTP，然后将此连接升级到基于套接字的连接。然后这个单一连接用于所有未来的通信
**** 4、Light -与 http 相比，WebSocket 消息数据交换要轻得多。
*** spring-boot-starter-parent 有什么用
**** 我们都知道，新创建一个 Spring Boot 项目，默认都是有 parent 的，这个 parent 就是 spring-boot-starter-parent ，spring-boot-starter-parent 主要有如下作用：
**** 定义了 Java 编译版本为 1.8 。
**** 使用 UTF-8 格式编码。
**** 继承自 spring-boot-dependencies，这个里边定义了依赖的版本，也正是因为继承了这个依赖，所以我们在写依赖时才不需要写版本号。
**** 执行打包操作的配置。
**** 自动化的资源过滤。
**** 自动化的插件配置。
**** 针对 application.properties 和 application.yml 的资源过滤，包括通过 profile 定义的不同环境的配置文件，例如 application-dev.properties 和 application-dev.yml。
*** Spring Boot 打成的 jar 和普通的 jar 有什么区别
**** Spring Boot 项目最终打包成的 jar 是可执行 jar ，这种 jar 可以直接通过 java -jar xxx.jar 命令来运行，这种 jar 不可以作为普通的 jar 被其他项目依赖，即使依赖了也无法使用其中的类。
**** Spring Boot 的 jar 无法被其他项目依赖，主要还是他和普通 jar 的结构不同。普通的 jar 包，解压后直接就是包名，包里就是我们的代码，而 Spring Boot 打包成的可执行 jar 解压后，
    在 \BOOT-INF\classes 目录下才是我们的代码，因此无法被直接引用。如果非要引用，可以在 pom.xml 文件中增加配置，将 Spring Boot 项目打包成两个 jar ，一个可执行，一个可引用。
*** 运行 Spring Boot 有哪几种方式
**** 1）打包用命令或者放到容器中运行
**** 2）用 Maven/ Gradle 插件运行
**** 3）直接执行 main 方法运行
*** 开启 Spring Boot 特性有哪几种方式
**** 1）继承spring-boot-starter-parent项目
**** 2）导入spring-boot-dependencies项目依赖
*** 如何使用 Spring Boot 实现异常处理
**** Spring 提供了一种使用 ControllerAdvice 处理异常的非常有用的方法。 我们通过实现一个 ControlerAdvice 类，来处理控制器类抛出的所有异常。
*** 如何使用 Spring Boot 实现分页和排序
**** 使用 Spring Boot 实现分页非常简单。使用 Spring Data-JPA 可以实现将可分页的传递给存储库方法
*** Spring Boot 中如何实现定时任务
**** 使用 Spring 中的 @Scheduled 的方式主要通过 @Scheduled 注解来实现。
**** 使用 Quartz ，则按照 Quartz 的方式，定义 Job 和 Trigger 即可。
** DONE 10	MyBatis面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/101292950
   CLOSED: [2021-04-14 Wed 12:20]
*** 底层使用jdk动态代理，使用的是mapperproxy
*** JDBC编程有哪些不足之处，MyBatis是如何解决这些问题的？（连接创建、sql语句维护[经常需要改]、参数传递[数量会变]、结果解析）
**** 1、数据库链接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库连接池可解决此问题。
***** 解决：在mybatis-config.xml中配置数据链接池，使用连接池管理数据库连接。
**** 2、Sql语句写在代码中造成代码不易维护，实际应用sql变化的可能较大，sql变动需要改变java代码。
***** 解决：将Sql语句配置在XXXXmapper.xml文件中与java代码分离。
**** 3、向sql语句传参数麻烦，因为sql语句的where条件不一定，可能多也可能少，占位符需要和参数一一对应。
***** 解决： Mybatis自动将java对象映射至sql语句。
**** 4、对结果集解析麻烦，sql变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成pojo对象解析比较方便。
***** 解决：Mybatis自动将sql执行结果映射至java对象。
*** Mybatis优缺点
**** 优点
***** 基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL写在XML里，解除sql与程序代码的耦合，便于统一管理；提供XML标签，支持编写动态SQL语句，并可重用
***** 与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接
***** 很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持）
***** 提供映射标签，支持对象与数据库的ORM字段关系映射；提供对象关系映射标签，支持对象关系组件维护
***** 能够与Spring很好的集成
**** 缺点
***** SQL语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写SQL语句的功底有一定要求
***** SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库
*** Hibernate 和 MyBatis 的区别
**** 相同点
***** 都是对jdbc的封装，都是持久层的框架，都用于dao层的开发。
**** 不同点
***** 映射关系
****** MyBatis 是一个半自动映射的框架，配置Java对象与sql语句执行结果的对应关系，多表关联关系配置简单
****** Hibernate 是一个全表映射的框架，配置Java对象与数据库表的对应关系，多表关联关系配置复杂
***** SQL优化和移植性
****** Hibernate 对SQL语句封装，提供了日志、缓存、级联（级联比 MyBatis 强大）等特性，此外还提供 HQL（Hibernate Query Language）操作数据库，数据库无关性支持好，但会多消耗性能。如果项目需要支持多种数据库，
      代码开发量少，但SQL语句优化困难。
****** MyBatis 需要手动编写 SQL，支持动态 SQL、处理列表、动态生成表名、支持存储过程。开发工作量相对大些。直接使用SQL语句操作数据库，不支持数据库无关性，但sql语句优化容易。
***** 开发难易程度和学习成本
****** Hibernate 是重量级框架，学习使用门槛高，适合于需求相对稳定，中小型的项目，比如：办公自动化系统
****** MyBatis 是轻量级框架，学习使用门槛低，适合于需求变化频繁，大型的项目，比如：互联网电子商务系统
*** [#A] 请说说MyBatis的工作原理(读取配置文件->加载映射文件->构造会话工厂->创建会话->Executor执行【sql语句生成、缓存维护】->mappedstatement存储sql的id和参数->输入参数映射->输出结构映射)
**** 1）读取 MyBatis 配置文件：mybatis-config.xml 为 MyBatis 的全局配置文件，配置了 MyBatis 的运行环境等信息，例如数据库连接信息。
**** 2）加载映射文件。映射文件即 SQL 映射文件，该文件中配置了操作数据库的 SQL 语句，需要在 MyBatis 配置文件 mybatis-config.xml 中加载。mybatis-config.xml 文件可以加载多个映射文件，
    每个文件对应数据库中的一张表。
**** 3）构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂 SqlSessionFactory。
**** 4）创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。
**** 5）Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。
**** 6）MappedStatement 对象：在 Executor 接口的执行方法中有一个 MappedStatement 类型的参数，该参数是对映射信息的封装，用于存储要映射的 SQL 语句的 id、参数等信息。
**** 7）输入参数映射：输入参数类型可以是 Map、List 等集合类型，也可以是基本数据类型和 POJO 类型。输入参数映射过程类似于 JDBC 对 preparedStatement 对象设置参数的过程。
**** 8）输出结果映射：输出结果类型可以是 Map、 List 等集合类型，也可以是基本数据类型和 POJO 类型。输出结果映射过程类似于 JDBC 对结果集的解析过程。
*** [#A] Mybatis的功能架构分为三层(api接口层【开发人员使用】、数据处理层【sql查找、解析、执行、结果映射】、基础支撑层【连接事务等管理、配置加载和缓存处理】)
**** (1)加载配置：配置来源于两个地方，一处是配置文件，一处是Java代码的注解，将SQL的配置信息加载成为一个个MappedStatement对象（包括了传入参数映射配置、执行的SQL语句、结果映射配置），存储在内存中。
**** (2)SQL解析：当API接口层接收到调用请求时，会接收到传入SQL的ID和传入对象（可以是Map、JavaBean或者基本数据类型），Mybatis会根据SQL的ID找到对应的MappedStatement，
    然后根据传入参数对象对MappedStatement进行解析，解析后可以得到最终要执行的SQL语句和参数。
**** (3)SQL执行：将最终得到的SQL和参数拿到数据库进行执行，得到操作数据库的结果。
**** (4)结果映射：将操作数据库的结果按照映射的配置进行转换，可以转换成HashMap、JavaBean或者基本数据类型，并将最终结果返回。
*** 为什么需要预编译(优化sql，缓存statement再使用)
**** 定义：
***** SQL 预编译指的是数据库驱动在发送 SQL 语句和参数给 DBMS 之前对 SQL 语句进行编译，这样 DBMS 执行 SQL 时，就不需要重新编译。
**** 为什么需要预编译
***** JDBC 中使用对象 PreparedStatement 来抽象预编译语句，使用预编译。预编译阶段可以优化 SQL 的执行。预编译之后的 SQL 多数情况下可以直接执行，DBMS 不需要再次编译，越复杂的SQL，编译的复杂度将越大，
      预编译阶段可以合并多次操作为一个操作。同时预编译语句对象可以重复利用。把一个 SQL 预编译后产生的 PreparedStatement 对象缓存下来，下次对于同一个SQL，可以直接使用这个缓存的 PreparedState 对象。Mybatis默认情况下，将对所有的 SQL 进行预编译。
*** Mybatis都有哪些Executor执行器(simple、reuse、batch)
**** SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。
**** ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map<String, Statement>内，供下一次使用。简言之，就是重复使用Statement对象。
**** BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，
    等待逐一执行executeBatch()批处理。与JDBC批处理相同。
*** Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？(通过代理对象拦截，判断内部的属性是否为空，若为空则调用sql进行获取)
**** 在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。
**** 它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，
    然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。
*** #{}和${}的区别(#占位符进行预编译处理以？代替,$拼接符,#{} 的变量替换是在DBMS 中；${} 的变量替换是在 DBMS 外)
**** #{}是占位符，预编译处理；${}是拼接符，字符串替换，没有预编译处理。
**** Mybatis在处理#{}时，#{}传入参数是以字符串传入，会将SQL中的#{}替换为?号，调用PreparedStatement的set方法来赋值。
**** 变量替换后，#{} 对应的变量自动加上单引号 ‘’；变量替换后，${} 对应的变量不会加上单引号 ‘’
**** #{} 可以有效的防止SQL注入，提高系统安全性；${} 不能防止SQL 注入
**** #{} 的变量替换是在DBMS 中；${} 的变量替换是在 DBMS 外
*** 模糊查询like语句该怎么写
**** CONCAT(’%’,#{question},’%’) 使用CONCAT()函数
*** 在mapper中如何传递多个参数
**** 顺序传参法
**** @Param注解传参法
**** Map传参法
**** Java Bean传参法
*** Dao接口
**** Dao接口，就是人们常说的Mapper接口，接口的全限名，就是映射文件中的namespace的值，接口的方法名，就是映射文件中MappedStatement的id值，接口方法内的参数，就是传递给sql的参数。Mapper接口是没有实现类的，当调用接口方法时，
    接口全限名+方法名拼接字符串作为key值，可唯一定位一个MappedStatement，每一个<select>、<insert>、<update>、<delete>标签，都会被解析为一个MappedStatement对象。
**** Dao接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。
**** Dao接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Dao接口生成代理proxy对象，代理对象proxy会拦截接口方法，转而执行MappedStatement所代表的sql，然后将sql执行结果返回。
*** Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复
**** 不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复；毕竟namespace不是必须的，只是最佳实践而已。
**** 原因就是namespace+id是作为Map<String, MappedStatement>的key使用的，如果没有namespace，就剩下id，那么，id重复会导致数据互相覆盖。有了namespace，自然id就可以重复，namespace不同，namespace+id自然也就不同。
*** 简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系(select->mappedstatement、标签内sql->boundsql、resultMap->ResultMap对象、parametermap->ParameterMap对象)
**** Mybatis将所有Xml配置信息都封装到All-In-One重量级对象Configuration内部。在Xml映射文件中，<parameterMap>标签会被解析为ParameterMap对象，其每个子元素会被解析为ParameterMapping对象。<resultMap>标签会被解析为ResultMap对象，
    其每个子元素会被解析为ResultMapping对象。每一个<select>、<insert>、<update>、<delete>标签均会被解析为MappedStatement对象，标签内的sql会被解析为BoundSql对象。
*** Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？
**** 一种是使用<resultMap>标签，逐一定义列名和对象属性名之间的映射关系。
**** 第二种是使用sql列的别名功能，将列别名书写为对象属性名，比如T_NAME AS NAME，对象属性名一般是name，小写，但是列名不区分大小写，Mybatis会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成T_NAME AS NaMe，Mybatis一样可以正常工作。
*** 标签列表(if、foreach、[choose、when、otherwise]一起用、bind[创建一个变量并将其绑定到上下文]、[set、where、trim]当空时会消除或去除and or这些的)
**** select|insert|updae|delete
**** <resultMap>、<parameterMap>、<sql>、<include>、<selectKey>
**** 动态sql： trim|where|set|foreach|if|choose|when|otherwise|bind
**** 中<sql>为sql片段标签，通过<include>标签引入sql片段，<selectKey>为不支持自增的主键生成策略标签
*** 标签顺序问题(顺序无关)
**** Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？
**** 虽然Mybatis解析Xml映射文件是按照顺序解析的，但是，被引用的B标签依然可以定义在任何地方，Mybatis都可以正确识别。
**** 原理是，Mybatis解析A标签，发现A标签引用了B标签，但是B标签尚未解析到，尚不存在，此时，Mybatis会将A标签标记为未解析状态，然后继续解析余下的标签，包含B标签，待所有标签解析完毕，Mybatis会重新解析那些被标记为未解析的标签，
    此时再解析A标签时，B标签已经存在，A标签也就可以正常解析完成了。
*** Mybatis是否可以映射Enum枚举类(使用Typehandler进行映射)
**** Mybatis可以映射枚举类，不单可以映射枚举类，Mybatis可以映射任何对象到表的一列上。映射方式为自定义一个TypeHandler，实现TypeHandler的setParameter()和getResult()接口方法。
**** TypeHandler有两个作用，一是完成从javaType至jdbcType的转换，二是完成jdbcType至javaType的转换，体现为setParameter()和getResult()两个方法，分别代表设置sql问号占位符参数和获取列查询结果。
*** [#B] 动态sql(原理使用OGNL【对象导航图语言】从sql参数对象中计算表达式的值，对表达式进行拼接)
**** Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能，Mybatis提供了9种动态sql标签trim|where|set|foreach|if|choose|when|otherwise|bind。
**** 其执行原理为，使用OGNL从sql参数对象中计算表达式的值，根据表达式的值动态拼接sql，以此来完成动态sql的功能。
*** 分页(使用rowbounds，针对resultset结果集进行内存分页，而插件和sql书写是物理分页)
**** Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页，可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。
**** 分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数。
*** [#B] Mybatis的插件运行原理，以及如何编写一个插件
**** Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，Mybatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，
    每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。
**** 实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。
*** Mybatis的一级、二级缓存(都是使用PerpetualCache 的 HashMap，一级缓存在session中【默认打开】、二级缓存在namespace中可自定义存储源【默认不打开】，更新机制，当作用域有更新就刷新该作用域下的所有select缓存)
**** 1）一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就将清空，默认打开一级缓存。
**** 2）二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，
    使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态),可在它的映射文件中配置<cache/> ；
**** 3）对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。

*** Mybatis提供了9种动态sql标签： trim|where|set|foreach|if|choose|when|otherwise|bind。
** DONE 16	Tomcat面试题（2020最新版）有jvm+tomcat调优	https://thinkwon.blog.csdn.net/article/details/104397665
   CLOSED: [2021-04-14 Wed 14:31]
*** tomcat 有哪几种Connector 运行模式(优化)？(BIO[一般线程数在400-500，最多最多2000]、NIO[jdk8中linux默认使用,1.4开始有]、APR[1.7使用AIO])
**** BIO：同步并阻塞 一个线程处理一个请求。缺点：并发量高时，线程数较多，浪费资源。Tomcat7或以下，在Linux系统中默认使用这种方式。
**** NIO：同步非阻塞IO
***** 利用Java的异步IO处理，可以通过少量的线程处理大量的请求，可以复用同一个线程处理多个connection(多路复用)。
***** Tomcat8在Linux系统中默认使用这种方式。
***** Tomcat7必须修改Connector配置来启动。
**** APR
***** 即Apache Portable Runtime，从操作系统层面解决io阻塞问题。**AIO方式，**异步非阻塞IO(Java NIO2又叫AIO) 主要与NIO的区别主要是操作系统的底层区别.
     可以做个比喻:比作快递，NIO就是网购后要自己到官网查下快递是否已经到了(可能是多次)，然后自己去取快递；AIO就是快递员送货上门了(不用关注快递进度)。
***** 需在本地服务器安装APR库。Tomcat7或Tomcat8在Win7或以上的系统中启动默认使用这种方式。Linux如果安装了apr和native，Tomcat直接启动就支持apr。
*** Tomcat有几种部署方式？(自动部署[webapps下的文件]、Manager App控制台管理、conf/server.xml增加Context节点)
**** 利用Tomcat的自动部署。
***** 把web应用拷贝到webapps目录。Tomcat在启动时会加载目录下的应用，并将编译后的结果放入work目录下。
**** 使用Manager App控制台部署。
***** 在tomcat主页点击“Manager App” 进入应用管理控制台，可以指定一个web应用的路径或war文件。
**** 修改conf/server.xml文件部署。
***** 修改conf/server.xml文件，增加Context节点可以部署应用。
**** 增加自定义的Web部署文件。
***** 在conf/Catalina/localhost/ 路径下增加 xyz.xml文件，内容是Context节点，可以部署应用。
*** tomcat容器是如何创建servlet类实例？用到了什么原理？(读取所有web.xml文件[servlet注册信息]->通过反射加载servlet类)
**** 当容器启动时，会读取在webapps目录下所有的web应用中的web.xml文件，然后对 xml文件进行解析，并读取servlet注册信息。然后，将每个应用中注册的servlet类都进行加载，
    并通过 反射的方式实例化。（有时候也是在第一次请求时实例化）
**** 在servlet注册时加上1如果为正数，则在一开始就实例化，如果不写或为负数，则第一次请求实例化。
*** Tomcat顶层架构(server->多个service->connector[socket|request请求]+container[封装管理servlet])
**** Tomcat中最顶层的容器是Server，代表着整个服务器，从上图中可以看出，一个Server可以包含至少一个Service，即可以包含多个Service，用于具体提供服务。
**** Service主要包含两个部分：Connector和Container。从上图中可以看出 Tomcat 的心脏就是这两个组件，他们的作用如下：
***** Connector用于处理连接相关的事情，并提供Socket与Request请求和Response响应相关的转化;
***** Container用于封装和管理Servlet，以及具体处理Request请求；
**** 一个Tomcat中只有一个Server，一个Server可以包含多个Service，一个Service只有一个Container，但是可以有多个Connectors，这是因为一个服务可以有多个连接，
    如同时提供Http和Https链接，也可以提供向相同协议不同端口的连接，示意图如下（Engine、Host、Context下面会说到）：
**** 多个 Connector 和一个 Container 就形成了一个 Service，有了 Service 就可以对外提供服务了，但是 Service 还要一个生存的环境，必须要有人能够给她生命、
    掌握其生死大权，那就非 Server 莫属了！所以整个 Tomcat 的生命周期由 Server 控制。
*** Tomcat顶层架构小结
**** Tomcat中只有一个Server，一个Server可以有多个Service，一个Service可以有多个Connector和一个Container；
**** Server掌管着整个Tomcat的生死大权；
**** Service 是对外提供服务的；
**** Connector用于接受请求并将请求封装成Request和Response来具体处理；
**** Container用于封装和管理Servlet，以及具体处理request请求；
*** Connector接收请求过程(使用protocolHandler处理请求->{Endpoint网络请求+Processor将socket封装成request+Adapter将请求提交给container})
**** Connector就是使用ProtocolHandler来处理请求的，不同的ProtocolHandler代表不同的连接类型，比如：Http11Protocol使用的是普通Socket来连接的，Http11NioProtocol使用的是NioSocket来连接的。
**** 其中ProtocolHandler由包含了三个部件：Endpoint、Processor、Adapter。
**** Endpoint用来处理底层Socket的网络连接，Processor用于将Endpoint接收到的Socket封装成Request，Adapter用于将Request交给Container进行具体的处理。
**** Endpoint由于是处理底层的Socket网络连接，因此Endpoint是用来实现TCP/IP协议的，而Processor用来实现HTTP协议的，Adapter将请求适配到Servlet容器进行具体的处理。
**** Endpoint的抽象实现AbstractEndpoint里面定义的Acceptor和AsyncTimeout两个内部类和一个Handler接口。Acceptor用于监听请求，AsyncTimeout用于检查异步Request的超时，
    Handler用于处理接收到的Socket，在内部调用Processor进行处理。
*** Container架构分析(engine[用于管理多个站点]、host[代表一个站点]、context[代表一个应用程序]、wrapper[封装servlet])
**** Container用于封装和管理Servlet，以及具体处理Request请求，在Container内部包含了4个子容器
**** 4个子容器的作用分别是：
***** Engine：引擎，用来管理多个站点，一个Service最多只能有一个Engine；
***** Host：代表一个站点，也可以叫虚拟主机，通过配置Host就可以添加站点；
***** Context：代表一个应用程序，对应着平时开发的一套程序，或者一个WEB-INF目录以及下面的web.xml文件；
***** Wrapper：每一Wrapper封装着一个Servlet；
*** Container如何处理请求的(使用Pipeline-Valve管道来处理)
**** Container处理请求是使用Pipeline-Valve管道来处理的！（Valve是阀门之意）
**** Pipeline-Valve是责任链模式，责任链模式是指在一个请求处理的过程中有很多处理者依次对请求进行处理，每个处理者负责做自己相应的处理，处理完之后将处理后的结果返回，再让下一个处理者继续处理。
**** Pipeline-Valve使用的责任链模式和普通的责任链模式有些不同！区别主要有以下两点：
***** 每个Pipeline都有特定的Valve，而且是在管道的最后一个执行，这个Valve叫做BaseValve，BaseValve是不可删除的；
***** 在上层容器的管道的BaseValve中会调用下层容器的管道。
***** 我们知道Container包含四个子容器，而这四个子容器对应的BaseValve分别在：StandardEngineValve、StandardHostValve、StandardContextValve、StandardWrapperValve。
**** 具体流程
***** Connector在接收到请求后会首先调用最顶层容器的Pipeline来处理，这里的最顶层容器的Pipeline就是EnginePipeline（Engine的管道）；
***** 在Engine的管道中依次会执行EngineValve1、EngineValve2等等，最后会执行StandardEngineValve，在StandardEngineValve中会调用Host管道，然后再依次执行Host的HostValve1、HostValve2等，
     最后在执行StandardHostValve，然后再依次调用Context的管道和Wrapper的管道，最后执行到StandardWrapperValve。
***** 当执行到StandardWrapperValve的时候，会在StandardWrapperValve中创建FilterChain，并调用其doFilter方法来处理请求，这个FilterChain包含着我们配置的与请求相匹配的Filter和Servlet，
     其doFilter方法会依次调用所有的Filter的doFilter方法和Servlet的service方法，这样请求就得到了处理！
***** 当所有的Pipeline-Valve都执行完之后，并且处理完了具体的请求，这个时候就可以将返回的结果交给Connector了，Connector在通过Socket的方式将结果返回给客户端。
*** [#B] 参数server.xml(调优)
**** URIEncoding=”UTF-8″
**** maxSpareThreads：最大空闲线程数
**** minSpareThreads：最小空闲线程数（初始化线程数量）
**** enableLookups: ip反查,为了消除 DNS 查询对性能的影响我们可以关闭DNS 查询
**** connectionTimeout : connectionTimeout 为网络连接超时时间毫秒数
**** maxThreads : maxThreads Tomcat 使用线程来处理接收的每个请求。
**** acceptCount : acceptCount 是当线程数达到 maxThreads 后，后续请求会被放入一个等待队列
**** disableUploadTimeout ：类似于 Apache 中的 keeyalive 一样
**** compression=”on” compressionMinSize=”2048″ 压缩开启，大于多少开始压缩
**** compressableMimeType=”text/html,text/xml,text/JavaScript,text/css,text/plain” 压缩类型
**** noCompressionUserAgents=”gozilla, traviata” 对于以下的浏览器，不启用压缩
**** useURIValidationHack=”false” 关闭uri校验
*** [#B] 参数catalina.sh(内存调优,调整一下 JAVA_OPTS 变量)
**** 如JAVA_OPTS="$JAVA_OPTS -Xmx3550m -Xms3550m -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4"
**** -Xmn2g：设置年轻代大小为 2G(Sun 官方推荐配置为整个堆的 3/8)
**** -XX:NewRatio=4: 年轻代与年老代所占比值为 1：4
**** -XX:SurvivorRatio=4: 两个 Survivor 区与一个 Eden 区的比值为 2:4
**** -XX:MaxPermSize=16m:设置持久代大小为 16m
**** -XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为 0 的话，则年轻代对象不经过 Survivor 区，直接进入年老代
**** -XX:MaxGCPauseMillis=100" 最大回收暂停时间
**** -XX:+UseParallelGC：选择垃圾收集器为并行收集器。此配置仅对年轻代有效。年轻代使用并发收集，而年老代仍旧使用串行收集。
**** -XX:ParallelGCThreads=20：配置并行收集器的线程数
**** -XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集
**** -XX:+UseAdaptiveSizePolicy：并行收集器会自动选择年轻代区大小和相应的 Survivor 区比例
**** -XX:+UseParNewGC: JDK5.0 以上，JVM 会根据系统配置自行设置，所以无需再设置此值。
**** -XX:CMSFullGCsBeforeCompaction：由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此值设置运行多少次 GC 以后对内存空间进行压缩、整理。
**** -XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片
**** -verbose:class 监视加载的类的情况
**** -verbose:gc 在虚拟机发生内存回收时在输出设备显示信息
**** -verbose:jni 输出 native 方法调用的相关情况，一般用于诊断 jni 调用错误信息
**** -Xloggc:filename:与上面几个配合使用，把相关日志信息记录到文件以便分析
*** 共享 session 处理
**** 使用 Tomcat 本身的 Session 复制功能
***** 方案的有点是配置简单，缺点是当集群数量较多时，Session 复制的时间会比较长，影响响应的效率
**** 使用第三方来存放共享 Session
***** 目前用的较多的是使用 memcached 来管理共享 Session，借助于memcached-sesson-manager 来进行 Tomcat 的 Session 管理
**** 使用黏性 session 的策略
***** 对于会话要求不太强（不涉及到计费，失败了允许重新请求下等）的场合，同一个用户的 session 可以由 nginx 或者 apache 交给同一个 Tomcat 来处理，这就是所谓的 session sticky 策略
***** 优点是处理效率高多了，缺点是强会话要求的场合不合适
*** Tomcat 的工作模式分为如下两类
**** Tomcat 作为应用程序服务器：请求来自于前端的 web 服务器，这可能是Apache, IIS, Nginx 等；
**** Tomcat 作为独立服务器：请求来自于 web 浏览器
*** Tomcat 一个请求的完整过程(nginx->connector->service->engine(匹配名称为localhost的host)->host(前缀匹配context)->context->servlet)
**** 首先 dns 解析 wo.de.tian 机器，一般是 nginx 服务器 ip 地址
**** 然后 nginx 根据 server 的配置，寻找路径为 yy/的机器列表，ip 和端口
**** 最后 选择其中一台机器进行访问—->下面为详细过程
**** 请求被发送到本机端口 8080，被在那里侦听的 Coyote HTTP/1.1 Connector 获得
**** Connector 把该请求交给它所在的 Service 的 Engine 来处理，并等待来自Engine 的回应
**** Engine 获得请求 localhost/yy/index.jsp，匹配它所拥有的所有虚拟主机 Host
**** Engine 匹配到名为 localhost 的 Host（即使匹配不到也把请求交给该 Host处理，因为该 Host 被定义为该 Engine 的默认主机）
****  Host 获得请求/yy/index.jsp，匹配它所拥有的所有 Context
**** path=”/yy”的 Context 获得请求/index.jsp，在它的 mapping table 中寻找对应的 servlet
**** Context 匹配到 URL PATTERN 为*.jsp 的 servlet，对应于 JspServlet 类
**** 构造 HttpServletRequest 对象和 HttpServletResponse 对象，作为参数调用JspServlet 的 doGet 或 doPost 方法
**** Context 把执行完了之后的 HttpServletResponse 对象返回给 Host
**** Host 把 HttpServletResponse 对象返回给 Engine
**** Engine 把 HttpServletResponse 对象返回给 Connector
**** Connector 把 HttpServletResponse 对象返回给客户 browser
** TODO 11	Redis面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/103522351
*** 使用场景 (缓存、排行榜、计数器、分布式会话、分布式锁 、社交网络【点赞、踩、关注/被关注、共同好友等是社交网站的基本功能】、最新列表 、消息系统)
*** 类型(字符串、列表、集合、散列表、有序集合)
*** 优点(读写快、单线程事务支持、持久化、数据结构丰富、主从复制)
**** 读写性能优异， Redis能读的速度是110000次/s，写的速度是81000次/s。
**** 支持数据持久化，支持AOF和RDB两种持久化方式。
**** 支持事务，Redis的所有操作都是原子性的，同时Redis还支持对几个操作合并后的原子性执行。
**** 数据结构丰富，除了支持string类型的value外还支持hash、set、zset、list等数据结构。
**** 支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。
*** 缺点(内存限制、无法自动容错、数据不一致、在线扩容困难)
**** 数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。
**** Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。
**** 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。
**** Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。
*** redis module
**** bloomfilter
**** redissearch
**** redis-ml
*** bloomfilter使用场景(bloomfilter+黑名单双重过滤刚好实现)
**** ip、Email黑名单
***** 如果不在黑名单内的，肯定可以通行，如果在的则不允许通过，误判情况增加一个排除名单来进行排除。
***** 误判情况：将正常用户判定为黑名单用户
**** 爬虫重复URL检测
***** 在爬取网站URL时，要检测这条URL是否已经访问过。
**** 字典纠错
***** 检查单词拼写是否正确
***** 误判情况：错误的单词误判为正确。
**** 磁盘文件检测
***** 将磁盘中或者数据库中数据key存入该结构中，检测要访问的数据是否在磁盘或数据库中，然后再发起访问，避免空查询造成磁盘或数据库压力。
***** 误判情况：不存在该数据却误判为有该数据。
*** bloomfilter实现方案
**** 1.原生语言+redis bitmap
**** rebloom
**** pyrebloom
*** Redisearch
**** 是一个高效，功能完备的内存存储的高性能全文检索组件， 十分适合应用在数据量适中， 内存和存储空间有限的环境。借助数据同步手段，我们可以很方便的将redisearch 结合到现有的数据存储中， 进而向产品提供 全文检索，
     自动补全等服务优化功能。
**** 命令（使用中文分词）
***** FT.CREATE SMARTX_VM SCHEMA title TEXT WEIGHT 5.0 desc TEXT
***** FT.ADD SMARTX_VM vm-2019082911110001 1.0 LANGUAGE "chinese" FIELDS title "人工智能" desc "我在北京昌平学习人工智能"
***** FT.SEARCH SMARTX_VM "人工智能" LANGUAGE "chinese"
*** geo使用
**** geoadd key longitude latitude member [longitude latitude member ...]
**** geopos key member [member ...] 获取经纬度
**** geodist key member1 member2 [unit] 获取距离
***** m（meters）代表米。
***** km（kilometers）代表公里。
***** mi（miles）代表英里。
***** ft（feet）代表尺。
**** 两个命令的作用是一样的，都是以一个地理位置为中心算出指定半径内的其他地理信息位置
**** georadius key longitude latitude radiusm|km|ft|mi [withcoord] [withdist] [withhash] [COUNT count] [asc|desc] [store key] [storedist key]
**** georadiusbymember key member radiusm|km|ft|mi [withcoord] [withdist] [withhash] [COUNT count] [asc|desc] [store key] [storedist key]
**** 参数含义
***** withcoord：返回结果中包含经纬度。
***** withdist：返回结果中包含离中心节点位置的距离。
***** withhash：返回结果中包含geohash，有关geohash后面介绍。
***** COUNT count：指定返回结果的数量。
***** asc|desc：返回结果按照离中心节点的距离做升序或者降序。
***** store key：将返回结果的地理位置信息保存到指定键。
***** storedist key：将返回结果离中心节点的距离保存到指定键。
**** geohash key member [member ...] 获取geohash
**** zrem key member 删除地理位置信息
***** GEO没有提供删除成员的命令，但是因为GEO的底层实现是zset，所以可以借用zrem命令实现对地理位置信息的删除。
*** HyperLogLog（做基数统计的算法HLL）
**** 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。
**** 因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。
**** 核心是基数估算算法，主要表现为计算时内存的使用和数据合并的处理。最终数值存在一定误差
**** 数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。
**** 命令
***** PFADD runoobkey "redis"
***** PFCOUNT runoobkey
***** pfmerge 合并多个key
**** 应用场景
***** 基数不大，数据量不大就用不上，会有点大材小用浪费空间
***** 有局限性，就是只能统计基数数量，而没办法去知道具体的内容是什么
***** 和bitmap相比，属于两种特定统计情况，简单来说，HyperLogLog 去重比 bitmap 方便很多
***** 一般可以bitmap和hyperloglog配合使用，bitmap标识哪些用户活跃，hyperloglog计数
**** 统计类型
***** 统计注册 IP 数
***** 统计每日访问 IP 数
***** 统计页面实时 UV 数
***** 统计在线用户数
***** 统计用户每天搜索不同词条的个数
**** 算法进化
***** Linear Counting(LC)：早期的基数估计算法，LC在空间复杂度方面并不算优秀，实际上LC的空间复杂度与简单bitmap方法是一样的（但是有个常数项级别的降低），都是O(Nmax)；
***** LogLog Counting(LLC)：LogLog Counting相比于LC更加节省内存，空间复杂度只有O(log2(log2(Nmax)))
***** HyperLogLog Counting(HLL)：HyperLogLog Counting是基于LLC的优化和改进，在同样空间复杂度情况下，能够比LLC的基数估计误差更小。
*** [#A] Redis为什么这么快
**** 完全基于内存，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是O(1) 
**** 数据结构简单,对数据操作也简单，Redis 中的数据结构是专门进行设计的；
**** 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；
**** 使用多路 I/O 复用模型，非阻塞 IO；
**** 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；
*** 分布式锁实现
**** 在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。可以使用 Redis 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。
*** [#A] Redlock锁实现机制
**** Redlock算法是Antirez在单Redis节点基础上引入的高可用模式。
**** 在Redis的分布式环境中，我们假设有N个完全互相独立的Redis节点，在N个Redis实例上使用与在Redis单实例下相同方法获取锁和释放锁。
**** 现在假设有5个Redis主节点(大于3的奇数个)，这样基本保证他们不会同时都宕掉，获取锁和释放锁的过程中，客户端会执行以下操作:
**** 1.获取当前Unix时间，以毫秒为单位
**** 2.依次尝试从5个实例，使用相同的key和具有唯一性的value获取锁当向Redis请求获取锁时，客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间，这样可以避免客户端死等
**** 3.客户端使用当前时间减去开始获取锁时间就得到获取锁使用的时间。当且仅当从半数以上的Redis节点取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功
**** 4.如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间，这个很重要
**** 5.如果因为某些原因，获取锁失败（没有在半数以上实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁，无论Redis实例是否加锁成功，因为可能服务端响应消息丢失了但是实际成功了，
     毕竟多释放一次也不会有问题
*** 高可用
**** Redis要想实现高可用，主要有以下方面来保证:
***** 数据持久化
***** 主从复制
***** 自动故障恢复
***** 集群化
*** 持久化
**** RDB：产生一个数据快照文件(RDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照)
**** AOF：实时追加命令的日志文件(Append Only File（追加日志文件）存储所有命令，包括读命令)
*** RDB的原理是什么 或 redis如何保证主从一致性，redis是如何导出rdb的
**** 你给出两个词汇就可以了，fork和cow。
**** fork是指redis通过创建⼦进程来进⾏RDB操作。
**** cow指的是copy on write，⼦进程创建后，⽗⼦进程共享数据段，⽗进程继续提供读写服务，写脏的⻚⾯数据会逐渐和⼦进程分离开来。
*** 过期键的删除策略(定时删除、惰性删除、定期删除)
**** 定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。
**** 惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。
**** 定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，
     可以在不同情况下使得CPU和内存资源达到最优的平衡效果。
**** (expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。)
*** 动态扩容（一致性哈希）
**** crc32(key)%(2^32-1)得到相应的值，该值落在相邻的redis节点上。
**** 节点分布不均匀可以使用虚拟节点的方式，多个虚拟节点对应一个物理节点。
*** [#B] Redis的内存淘汰策略有哪些(分全键和有过期时间的)
**** 全键的
**** noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。
**** allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。（这个是最常用的）
**** allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。
**** 有过期时间的
**** volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。
**** volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。
**** volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。
*** Redis回收进程如何工作的？(内存超出后根据回收策略回收)
**** 一个客户端运行了新的命令，添加了新的数据。
**** Redis检查内存使用情况，如果大于maxmemory的限制， 则根据设定好的策略进行回收。
**** 一个新的命令被执行，等等。
**** 所以我们不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下。
*** Redis如何做内存优化(可以用hash就用hash)
**** 可以使用hash就使用hash（将string类型放到hash中）
*** Redis线程模型(Reactor模式开发的网络【文件】事件处理器：多个套字节、、IO多路复用程序、文件事件分派器、事件处理器)
**** Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器（file event handler）。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。因为文件事件分派器队列的消费是单线程的，
     所以Redis才叫单线程模型。
**** 文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。
**** 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。
**** 虽然文件事件处理器以单线程方式运行， 但通过使用 I/O 多路复用程序来监听多个套接字， 文件事件处理器既实现了高性能的网络通信模型， 又可以很好地与 redis 服务器中其他同样以单线程方式运行的模块进行对接，
     这保持了 Redis 内部单线程设计的简单性。
*** [#A] Redis事务(不支持回滚、除非命令错误，否则继续执行命令、MULTI、EXEC、DISCARD和WATCH四个原语)
**** redis 不支持回滚，“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。
**** 如果在一个事务中的命令出现错误，那么所有的命令都不会执行；
**** 如果在一个事务中出现运行错误，那么正确的命令会被执行。
**** Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的
**** 4种命令
***** WATCH 命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。
***** MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。
***** EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。
***** 通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。
***** UNWATCH命令可以取消watch对所有key的监控。
*** ACID(没有原子性、有一致和隔离性、持久性使用aof时有)
**** 原子性（Atomicity）
***** 原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。
**** 一致性（Consistency）没有中间状态
***** 事务前后数据的完整性必须保持一致。
**** 隔离性（Isolation）多个事务互不干扰
***** 多个事务并发执行时，一个事务的执行不应影响其他事务的执行
**** 持久性（Durability）
***** 持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响
**** 相关问题
***** Redis的事务总是具有ACID中的一致性和隔离性，其他特性是不支持的。当服务器运行在AOF持久化模式下，并且appendfsync选项的值为always时，事务也具有耐久性。
***** Redis事务支持隔离性吗
****** Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的。
***** Redis事务保证原子性吗，支持回滚吗
****** Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。
*** Redis与Memcached的区别(持久化、数据类型、单线程、cluster集群、io多路复用、lua脚本使用、redis的数据不全部在内存中、redis支持发布订阅模式)
**** 对比参数	Redis	Memcached
**** 类型	1. 支持内存 2. 非关系型数据库  |	1. 支持内存 2. 键值对形式 3. 缓存形式
**** 数据存储类型	1. String 2. List 3. Set 4. Hash 5. Sort Set 【俗称ZSet】 |	1. 文本型 2. 二进制类型
**** 查询【操作】类型	1. 批量操作 2. 事务支持 3. 每个类型不同的CRUD  |	1.常用的CRUD 2. 少量的其他命令
**** 附加功能	1. 发布/订阅模式 2. 主从分区 3. 序列化支持 4. 脚本支持【Lua脚本】 |	1. 多线程服务支持
**** 网络IO模型	1. 单线程的多路 IO 复用模型  |	1. 多线程，非阻塞IO模式
**** 事件库	自封转简易事件库AeEvent |	贵族血统的LibEvent事件库
**** 持久化支持	1. RDB 2. AOF | 不支持
**** 集群模式	原生支持 cluster 模式，可以实现主从复制，读写分离  |	没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据
**** 内存管理机制	在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘  |	Memcached 的数据则会一直在内存中，
    Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题。但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。
**** 适用场景	复杂数据结构，有持久化，高可用需求，value存储内容较大   |  纯key-value，数据量非常大，并发量非常大的业务
*** 选择 redis 的情况
**** 复杂数据结构
**** 需要进行数据的持久化功能
**** 高可用，redis 支持集群，可以实现主动复制，读写分离
**** 存储的内容比较大，memcache 存储的 value 最大为 1M。
*** 选择 memcache 的场景
**** 纯 KV,数据量非常大的业务，原因如下：
***** memcache 的内存分配采用的是预分配内存池的管理方式，能够省去内存分配的时间，redis 是临时申请空间，可能导致碎片化。
***** 虚拟内存使用，memcache 将所有的数据存储在物理内存里，redis 有自己的 vm 机制，理论上能够存储比物理内存更多的数据，当数据超量时，引发 swap,把冷数据刷新到磁盘上，从这点上，数据量大时，memcache 更快
***** 网络模型，memcache 使用非阻塞的 IO 复用模型，redis 也是使用非阻塞的 IO 复用模型，但是 redis 还提供了一些非 KV 存储之外的排序，聚合功能，复杂的 CPU 计算，会阻塞整个 IO 调度，从这点上由于 redis 提供的功能较多，memcache 更快些
***** 线程模型，memcache 使用多线程，主线程监听，worker 子线程接受请求，执行读写，这个过程可能存在锁冲突。redis 使用的单线程，虽然无锁冲突，但是难以利用多核的特性提升吞吐量。
*** 如何保证缓存与数据库双写时的数据一致性？(先更新数据库，然后再删除缓存)
*** Redis如何做大量数据插入(使用pipe mode执行插入工作)
**** Redis2.6开始redis-cli支持一种新的被称之为pipe mode的新模式用于执行大量数据插入工作。
**** Pipeline有什么好处，为什么要⽤pipeline？
***** 可以将多次IO往返的时间缩减为⼀次，前提是pipeline执⾏的指令之间没有因果相关性。使⽤redisbenchmark进⾏压测的时候可以发现影响redis的QPS峰值的⼀个重要因素是pipeline批次指令的数⽬。
*** 一个字符串类型的值能存储最大容量是多少(2^32bit == 512MB)
**** 512M
*** Redis常见性能问题和解决方案(master不要做持久化，要做也是slave做，采用链表结构进行主从，局域网提速，aof重写cpu消耗)
**** Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化。
**** 如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。
**** 为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内。
**** 尽量避免在压力较大的主库上增加从库
**** Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。
**** 为了Master的稳定性，主从复制不要用图状结构，用单向链表结构更稳定，即主从关系为：Master<–Slave1<–Slave2<–Slave3…，这样的结构也方便解决单点故障问题，实现Slave对Master的替换，
    也即，如果Master挂了，可以立马启用Slave1做Master，其他不变。
*** 假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如果将它们全部找出来？(使用scan代替keys命令获取数据)从完整遍历开始直到完整遍历结束期间，一直存在于数据集内的所有元素都会被完整遍历返回，但是同一个元素可能会被返回多次。
**** 使用keys指令可以扫出指定模式的key列表。
**** 对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？
**** 这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，
    scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。
*** Redis如何实现延时队列(使用sortedset，时间戳做score)
**** 使用sortedset，使用时间戳做score, 消息内容作为key,调用zadd来生产消息，消费者使用zrangbyscore获取n秒之前的数据做轮询处理。
*** 使用Redis做过异步队列吗，是如何实现的(1.list类型保存数据，rpush、lpop操作，没消息lpop sleep等待，或者使用blpop进行获取 2.使用pub\sub订阅模式)
**** 使用list类型保存数据信息，rpush生产消息，lpop消费消息，当lpop没有消息时，可以sleep一段时间，然后再检查有没有信息，如果不想sleep的话，可以使用blpop, 在没有信息的时候，
    会一直阻塞，直到信息的到来。redis可以通过pub/sub主题订阅模式实现一个生产者，多个消费者，当然也存在一定的缺点，当消费者下线时，生产的消息会丢失。
*** 常用工具
**** Redis支持的Java客户端都有哪些？官方推荐用哪个？
***** Redisson、Jedis、lettuce等等，官方推荐使用Redisson。
**** Redis和Redisson有什么关系？
***** Redisson是一个高级的分布式协调Redis客服端，能帮助用户在分布式环境中轻松实现一些Java的对象 (Bloom filter, BitSet, Set, SetMultimap, ScoredSortedSet, SortedSet, Map,
     ConcurrentMap, List, ListMultimap, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, ReadWriteLock, AtomicLong, CountDownLatch, Publish / Subscribe, HyperLogLog)。
***** Jedis与Redisson对比有什么优缺点？
****** Jedis是Redis的Java实现的客户端，其API提供了比较全面的Redis命令的支持；Redisson实现了分布式和可扩展的Java数据结构，和Jedis相比，功能较为简单，不支持字符串操作，
      不支持排序、事务、管道、分区等Redis特性。Redisson的宗旨是促进使用者对Redis的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。
*** 集群
**** 哨兵模式（主从筛选）
***** 主要有以下功能
****** 集群监控：负责监控 redis master 和 slave 进程是否正常工作。
****** 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。
****** 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。
****** 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。
***** 哨兵的核心知识
****** 哨兵至少需要 3 个实例，来保证自己的健壮性。
****** 哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。
****** 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。
**** redis cluster (redis cluster使用16384个槽点，不使用一致性hash算法，只需访问随意一个节点即可获取到数据，内部使用cluster bus通信，使用gossip协议)
***** Redis Cluster是一种服务端Sharding技术,3.0版本开始正式提供。Redis Cluster并没有使用一致性hash，而是采用slot(槽)的概念，一共分成16384个槽。将请求发送到任意节点，接收到请求的节点会将查询请求发送到正确的
      节点上执行
****** 通过哈希的方式，将数据分片，每个节点均分存储一定哈希槽(哈希值)区间的数据，默认分配了16384 个槽位
****** 每份数据分片会存储在多个互为主从的多节点上
****** 数据写入先写主节点，再同步到从节点(支持配置为阻塞同步)
****** 同一分片多个节点间的数据不保持一致性
****** 读取数据时，当客户端操作的key没有分配在该节点上时，redis会返回转向指令，指向正确的节点
****** 扩容时时需要把旧节点的数据迁移一部分到新节点 
***** 新端口
****** 16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的
       数据交换，占用更少的网络带宽和处理时间。
***** 节点间的内部通信机制
****** 集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信
***** 分布式寻址算法
****** hash 算法（大量缓存重建）
****** 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡）(一个圆 2^32-1)
****** redis cluster 的 hash slot 算法 (16384 crc16取余)
***** 优缺点
****** 优点
******* 无中心架构，支持动态扩容，对业务透明
******* 具备Sentinel的监控和自动Failover(故障转移)能力
******* 客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可
******* 高性能，客户端直连redis服务，免去了proxy代理的损耗
****** 缺点
******* 运维也很复杂，数据迁移需要人工干预
******* 只能使用0号数据库
******* 不支持批量操作(pipeline管道操作)
******* 分布式逻辑和存储模块耦合等
**** 主从复制(redis replication，slave node在复制时，不阻塞查询操作，继续使用就数据提供服务，完成时，删除旧数据，加载新数据，停止对外服务)
***** redis replication 的核心机制
****** redis 采用异步方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量；
****** 一个 master node 是可以配置多个 slave node 的；
****** slave node 也可以连接其他的 slave node；
****** slave node 做复制的时候，不会 block master node 的正常工作；
****** slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了；
****** slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。 
**** 复制过程(主库接收到从库到sync命令，保存快照rdb，并将期间接收到的写命令缓存起来，rdb完成后，将rdb和缓存写命令一起发送给从库，之后主库每次的写命令都发送给从库)
***** 当从库和主库建立MS关系后，会向主数据库发送SYNC命令
***** 主库接收到SYNC命令后会开始在后台保存快照(RDB持久化过程)，并将期间接收到的写命令缓存起来
***** 当快照完成后，主Redis会将快照文件和所有缓存的写命令发送给从Redis
***** 从Redis接收到后，会载入快照文件并且执行收到的缓存的命令
***** 之后，主Redis每当接收到写命令时就会将命令发送从Redis，从而保证数据的一致
**** 缺点
***** 所有的slave节点数据的复制和同步都由master节点来处理，会照成master节点压力太大，使用主从从(主从从是对的，从节点复制从节点)结构来解决
**** 生产环境中的 redis 是怎么部署的？
***** redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，
      5 台机器最多是 25 万读写请求/s。
***** 机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。
***** 5 台机器对外提供读写，一共有 50g 内存。
***** 因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。
**** 问题
***** 说说Redis哈希槽的概念？
****** Redis集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽。
***** Redis集群会有写操作丢失吗？为什么？
****** Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。
***** Redis集群之间是如何复制的？
****** 异步复制
***** Redis集群最大节点个数是多少？
****** 16384个
***** Redis集群如何选择数据库？
****** Redis集群目前无法做数据库选择，默认在0数据库。
**** 你知道有哪些Redis分区实现方案
***** 客户端分区就是在客户端就已经决定数据会被存储到哪个redis节点或者从哪个redis节点读取。大多数客户端已经实现了客户端分区。
***** 代理分区 意味着客户端将请求发送给代理，然后代理决定去哪个节点写数据或者读数据。代理根据分区规则决定请求哪些Redis实例，然后根据Redis的响应结果返回给客户端。
      redis和memcached的一种代理实现就是Twemproxy和codis
***** 查询路由(Query routing) 的意思是客户端随机地请求任意一个redis实例，然后由Redis将请求转发给正确的Redis节点。Redis Cluster实现了一种混合形式的查询路由，
      但并不是直接将请求从一个redis节点转发到另一个redis节点，而是在客户端的帮助下直接redirected到正确的redis节点。
**** Redis分区有什么缺点
***** 涉及多个key的操作通常不会被支持。例如你不能对两个集合求交集，因为他们可能被存储到不同的Redis实例（实际上这种情况也有办法，但是不能直接使用交集指令）。
***** 同时操作多个key,则不能使用Redis事务.
***** 分区使用的粒度是key，不能使用一个非常长的排序key存储一个数据集（The partitioning granularity is the key, so it is not possible to shard a dataset with a single huge key like a very big
      sorted set）
***** 当使用分区的时候，数据处理会非常复杂，例如为了备份你必须从不同的Redis实例和主机同时收集RDB / AOF文件。
***** 分区时动态扩容或缩容可能非常复杂。Redis集群在运行时增加或者删除Redis节点，能做到最大程度对用户透明地数据再平衡，但其他一些客户端分区或者代理分区方法则不支持这种特性。然而，有一种预分片的技术也可以较好的
      解决这个问题。
**** 如何解决 Redis 的并发竞争 Key 问题
***** 所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！
***** 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能）
***** 基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，
      只需要判断有序节点中序号最小的一个。当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。
***** 在实践中，当然是从以可靠性为主。所以首推Zookeeper。
*** 缓存异常
**** 缓存雪崩
***** 缓存雪崩是指缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。
***** 解决方案
****** 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。
****** 一般并发量不是特别多的时候，使用最多的解决方案是加锁排队(获取互斥锁，而后进行db查询，回来更新redis，删除互斥锁)。
****** 给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。
**** 缓存穿透
***** 缓存穿透是指缓存和数据库中都没有的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。
***** 解决方案
****** 接口层增加校验，如用户鉴权校验，id做基础校验，id<=0的直接拦截；
****** 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击
****** 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力
**** 缓存击穿
***** 缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。和缓存雪崩不同的是，
     缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。
****** 设置热点数据永远不过期。
****** 加互斥锁，互斥锁
**** 缓存预热
***** 缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！
***** 方案
****** 直接写个缓存刷新页面，上线时手工操作一下；
****** 数据量不大，可以在项目启动的时候自动进行加载；
****** 定时刷新缓存；
**** 缓存降级
***** 当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。
***** 缓存降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。
***** 在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案：
****** 一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；
****** 警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警；
****** 错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；
****** 严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。
***** 服务降级的目的，是为了防止Redis服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，
     Redis出现问题，不去数据库查询，而是直接返回默认值给用户。
*** lua脚本
**** Lua 脚本功能是 Reids在 2.6 版本的最⼤亮点， 通过内嵌对 Lua 环境的⽀持， Redis 解决了⻓久以来不能⾼效地处理 CAS （check-and-set）命令的缺点， 并且可以通过组合使⽤多个命令，
     轻松实现以前很难实现或者不能⾼效实现的模式。
*** 遇到过的坑
**** sync的时候遇到了bgsave 这个时候cpu飙升 ，因为bgsave之后会做⼀个emptyDB这个时候做bgsave, cow的机制就没了会重新加载整个RDB 然后就swap了。
**** 还有当写的tps较⾼，slave在加载rdb⽂件，slave会被阻塞，⽆法执⾏master同步过来的命令，复制积压缓冲区数据被冲掉。
**** 有个参数client-output-buffer-limit slave 256mb 64mb 60（⼤于256mb断开,或者连续60秒超过64mb断开） 当加载完数据之后，slave向master发送 sync 命令，由于复制积压数据被冲掉了，slave会再次请求同步数据。
*** 哨兵使用(sentinel)
**** 端口：26379
**** 故障转移：对redis主从进行监控，当master挂了以后，进行slave选举
**** quroum设置：节点数/2+1
**** 哨兵数量至少3个，而且要奇数个
**** 哨兵分布式部署
**** 一组哨兵监听一组主从
*** 三主三从模式构建redis集群
*** redis持久化：
**** rdb：快照模式 对数据完整性要求不行 (set m n) 设置当m时间后并且n个数据更改则进行持久化
**** aof：添加模式 设置间隔时间进行持久话 3种写入方式：每秒、每次写入、no（放入buffer中，当大小达到一定后写入）
**** 使用redis-cli --cluster create ip:port ip:port ...
**** 横向扩展，通告一致性hash当方式来选择响应当master节点获取和设置数据。
**** slot是redis上的槽点，三台master的slot编号不同，插入时更加hash选择插槽。
*** redis的java客户端
**** redisson、jedis、lettuce 官方推荐：redisson
**** jedis 和 redisson 有哪些区别？
***** Jedis 和 Redisson 都是Java中对Redis操作的封装。Jedis 只是简单的封装了 Redis 的API库，可以看作是Redis客户端，它的方法和Redis 的命令很类似。Redisson 不仅封装了 redis ，
    还封装了对更多数据结构的支持，以及锁等功能，相比于Jedis 更加大。但Jedis相比于Redisson 更原生一些，更灵活。
**** Redis集群支持最大节点数是多少？
***** Redis 集群有 16384 个哈希槽，每个 key 通过 CRC16 算法计算的结果，对 16384 取模后放到对应的编号在 0-16383 之间的哈希槽，集群的每个节点负责一部分哈希槽
**** redis事务命令
***** discard、exec、multi、unwatch、watch
**** redis一个实例能存多少key，key与value最大是多少？
***** 最多2^32个key；
** TODO 12	MySQL数据库面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104778621
*** 图片
**** 快照读和当前读区别 https://www.aneasystone.com/usr/uploads/2017/10/1389685905.png
*** 文章
**** 事务 https://www.aneasystone.com/archives/2017/10/solving-dead-locks-one.html
*** 总结性句子
****  数据库使用锁是为了支持更好的并发，提供数据的完整性和一致性。InnoDB是一个支持行锁的存储引擎，锁的类型有：共享锁（S）、排他锁（X）、意向共享（IS）、意向排他（IX）。为了提供更好的并发，
    InnoDB提供了非锁定读：不需要等待访问行上的锁释放，读取行的一个快照。该方法是通过InnoDB的一个特性：MVCC来实现的。
*** 三大范式
**** 第一范式：每个列都不可以再拆分。
**** 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。
**** 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。
*** DONE order by
    CLOSED: [2021-02-03 Wed 16:44]
**** 当我们使用order by将查询结果按照某个字段排序时，如果该字段没有建立索引，那么执行计划会将查询出的所有数据使用外部排序（将数据从硬盘分批读取到内存使用内部排序，最后合并排序结果），
    这个操作是很影响性能的，因为需要将查询涉及到的所有数据从磁盘中读到内存（如果单条数据过大或者数据量过多都会降低效率），更无论读到内存之后的排序了。
**** 但是如果我们对该字段建立索引alter table 表名 add index(字段名)，那么由于索引本身是有序的，因此直接按照索引的顺序和映射关系逐条取出数据即可。而且如果分页的，
    那么只用取出索引表某个范围内的索引对应的数据，而不用像上述那取出所有数据进行排序再返回某个范围内的数据。（从磁盘取数据是最影响性能的）
*** DONE 索引覆盖(面试时可以说，多使用索引覆盖，可以减少2次查找)
    CLOSED: [2021-02-03 Wed 16:44]
**** 如果要查询的字段都建立过索引，那么引擎会直接在索引表中查询而不会访问原始数据（否则只要有一个字段没有建立索引就会做全表扫描），这叫索引覆盖。因此我们需要尽可能的在select后只写必要的查询字段，以增加索引覆盖的几率。
*** DONE 聚簇索引和非聚簇索引
    CLOSED: [2021-02-03 Wed 16:44]
**** 聚触索引：一般时主键索引，叶子节点是数据库中的记录
**** 非聚触索引：叶子节点是主键值（获取到值后，进行二次搜索）
**** innodb中，在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的不再是行的物理位置，而是主键值
*** DONE 索引失效解决办法
    CLOSED: [2021-02-03 Wed 16:44]
**** 在mysql中，可以用locate和position函数，如table.field like '%AAA%'可以改为locate('AAA', table.field) > 0或POSITION('AAA' IN table.field)>0。
*** DONE like（%xxx）类型解决
    CLOSED: [2021-02-03 Wed 16:44]
**** 只有oracle有解决方案，使用reverse建立索引，而后查询也是用reverse
*** DONE 为什么要使用联合索引
    CLOSED: [2021-02-03 Wed 16:44]
**** 减少开销。建一个联合索引(col1,col2,col3)，实际相当于建了(col1),(col1,col2),(col1,col2,col3)三个索引。每多一个索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开销！
**** 覆盖索引。对联合索引(col1,col2,col3)，如果有如下的sql: select col1,col2,col3 from test where col1=1 and col2=2。那么MySQL可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机io操作。减少io操作，
    特别的随机io其实是dba主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。
**** 效率高。索引列越多，通过索引筛选出的数据越少。有1000W条数据的表，有如下sql:select from table where col1=1 and col2=2 and col3=3,假设假设每个条件可以筛选出10%的数据，如果只有单值索引，
    那么通过该索引能筛选出1000W10%=100w条数据，然后再回表从100w条数据中找到符合col2=2 and col3= 3的数据，然后再排序，再分页；如果是联合索引，通过索引筛选出1000w10% 10% *10%=1w，效率提升可想而知！
*** DONE 索引有哪几种类型(主键、唯一、普通、全文)
    CLOSED: [2021-02-03 Wed 16:44]
**** 主键索引: 数据列不允许重复，不允许为NULL，一个表只能有一个主键。
**** 唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。
**** 普通索引: 基本的索引类型，没有唯一性的限制，允许为NULL值。
**** 全文索引： 是目前搜索引擎使用的一种关键技术。
*** DONE 删除主键索引
    CLOSED: [2021-02-03 Wed 16:44]
**** alter table 表名 drop primary key（因为主键只有一个）。这里值得注意的是，如果主键自增长，那么不能直接执行此操作（自增长依赖于主键索引）
**** 需要取消自增长再行删除：
***** alter table user_index
***** -- 重新定义字段
***** MODIFY id int,
***** drop PRIMARY KEY
*** DONE 百万级别或以上的数据如何删除
    CLOSED: [2021-02-03 Wed 16:44]
**** 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟）
**** 然后删除其中无用数据（此过程需要不到两分钟）
**** 删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。
**** 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。
*** DONE 前缀索引
    CLOSED: [2021-02-03 Wed 16:44]
**** 语法：index(field(10))，使用字段值的前10个字符建立索引，默认是使用字段的全部内容建立索引。
**** 前提：前缀的标识度高。比如密码就适合建立前缀索引，因为密码几乎各不相同。
**** 实操的难度：在于前缀截取的长度。
**** 我们可以利用select count(*)/count(distinct left(password,prefixLen));，通过从调整prefixLen的值（从1自增）查看不同前缀长度的一个平均匹配度，接近1时就可以了（表示一个密码的前prefixLen个字符几乎能确定唯一一条记录）
*** union all不会去除重复记录+不会排序   union会去重+排序
*** [#B] binlog有有几种录入格式(statement【一些函数语句无法被记录】、row、mixed)
**** statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，
    因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。
**** row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，
    因此这种模式的文件保存的信息太多，日志量太大。
**** mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。
*** [#B] MySQL存储引擎MyISAM与InnoDB区别
**** Innodb引擎：Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。
**** MyIASM引擎(原本Mysql的默认引擎)：不提供事务的支持，也不支持行级锁和外键。
**** MEMORY引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。
*** [#B] MyISAM与InnoDB区别
**** 	MyISAM	Innodb
**** 存储结构	myisam每张表被存放在三个文件：frm-表格定义、MYD(MYData)-数据文件、MYI(MYIndex)-索引文件
    innodb所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB
**** 存储空间	MyISAM可被压缩，存储空间较小	InnoDB的表需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引
**** 可移植性、备份及恢复	myisam由于MyISAM的数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作
    innodb免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了
**** 文件格式	数据和索引是分别存储的，数据.MYD，索引.MYI	数据和索引是集中存储的，.ibd
**** 记录存储顺序	按记录插入顺序保存	按主键大小有序插入
**** 外键	不支持	支持
**** 事务	不支持	支持
**** 锁支持（锁是避免资源争用的一个机制，MySQL锁对用户几乎是透明的）	表级锁定	行级锁定、表级锁定，锁定力度小并发能力高
**** SELECT	MyISAM更优	
**** INSERT、UPDATE、DELETE		InnoDB更优
**** select count(*)	myisam更快，因为myisam内部维护了一个计数器，可以直接调取。	
**** 索引的实现方式	B+树索引，myisam 是堆表	B+树索引，Innodb 是索引组织表
**** 哈希索引	不支持	支持
**** 全文索引	支持	不支持
*** [#B] MyISAM索引与InnoDB索引的区别
**** InnoDB索引是聚簇索引，MyISAM索引是非聚簇索引。
**** InnoDB的主键索引的叶子节点存储着行数据，因此主键索引非常高效。
**** MyISAM索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。
**** InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。
*** [#B] 存储引擎选择
**** 如果没有特别的需求，使用默认的Innodb即可。
**** MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。
**** Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。比如OA自动化办公系统。
*** [#B] 反向索引和正向索引
**** MySQL 8.0之前，不管是否指定索引建的排序方式，都会忽略创建索引时候指定的排序方式（语法上不会报错），最终都会创建为ASC方式的索引，
**** 在执行查询的时候，只存在forwarded（正向）方式对索引进行扫描。
**** 关于正向索引和反向索引，逻辑上很容易理解，这里有两个相关的概念：
***** 正向索引或者反向（倒序）索引，两者都是在构建B树索引时候的相关字段排序方式，是B索引树的逻辑存储方式
***** 正向扫描（forward）和反向扫描（ Backward index scan;）是执行查询的过程中对B树索引的扫描方式，是数据执行计划时候的一种索引扫描方式
***** 关于正向扫描或者反向扫描不是随意的，受sql语句中（正/反向）排序方式以及（正/反向）索引的影响
***** 示例：CREATE TABLE t1 (a INT, b INT, INDEX a_desc_b_asc (a DESC, b ASC));
*** [#B] 索引算法有哪些
**** BTree算法
***** BTree是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在=,>,>=,<,<=和between这些比较操作符上，而且还可以用于like操作符，只要它的查询条件是一个不以通配符开头的常量
**** Hash算法
***** Hash Hash索引只能用于对等比较，例如=,<=>（相当于=）操作符。由于是一次定位数据，不像BTree索引需要从根节点到枝节点，最后才能访问到页节点这样多次IO访问，所以检索效率远高于BTree索引。
*** [#A] 索引设计的原则
**** 适合索引的列是出现在where子句中的列，或者连接子句中指定的列
**** 基数较小的类，索引效果较差，没有必要在此列建立索引(就只有1和0两种)
**** 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间
**** 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。
*** [#A] InnoDB引擎的4大特性
**** 插入缓冲（insert buffer)
**** 二次写(double write)
**** 自适应哈希索引(ahi)
**** 预读(read ahead)
*** [#A] 创建索引的原则(查询用到多、修改少、基数大、扩展代替新增、外键需要索引)
**** 1） 最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，
    d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
**** 2）较频繁作为查询条件的字段才去创建索引
**** 3）更新频繁字段不适合创建索引
**** 4）若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低)
**** 5）尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。
**** 6）定义有外键的数据列一定要建立索引。(连表查询时的速度&修改主表id时的子表锁表问题)
**** 7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。
**** 8）对于定义为text、image和bit的数据类型的列不要建立索引。
*** [#A] 创建索引时需要注意什么(非空、基数大、字段长度短)
**** 非空字段：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值；
**** 取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高；
**** 索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。
*** [#A] 什么是最左前缀原则？什么是最左匹配原则(最频繁、最短、基数最大)
**** 顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。
**** 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，
    如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
**** =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式
*** [#B] 各种树
**** BTREE
***** BTree又叫多路平衡搜索树，一颗m叉的BTree特性如下
***** 树中每个节点最多包含m个孩子。
***** 除根节点与叶子节点外，每个节点至少有[ceil(m/2)]个孩子。
***** 若根节点不是叶子节点，则至少有两个孩子。
***** 所有的叶子节点都在同一层。
***** 每个非叶子节点由n个key与n+1个指针组成，其中[ceil(m/2)-1] <= n <= m-1
**** B+树
***** B+Tree为BTree的变种，B+Tree与BTree的区别为：
***** 1). n叉B+Tree最多含有n个key，而BTree最多含有n-1个key。 
***** 2). B+Tree的叶子节点保存所有的key信息，依key大小顺序排列。 
***** 3). 所有的非叶子节点都可以看作是key的索引部分
***** 4.）B+ 树中，数据对象的插入和删除仅在叶节点上进行。
***** 5.）B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。
**** B+树优点(层级少、稳定、排序功能、全节点遍历快)
***** 1、B+树的层级更少：相较于B树B+每个非叶子节点存储的关键字数更多，树的层级更少所以查询数据更快；
***** 2、B+树查询速度更稳定：B+所有关键字数据地址都存在叶子节点上，所以每次查找的次数都相同所以查询速度要比B树更稳定;
***** 3、B+树天然具备排序功能：B+树所有的叶子节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高。
***** 4、B+树全节点遍历更快：B+树遍历整棵树只需要遍历所有的叶子节点即可，，而不需要像B树一样需要对每一层进行遍历，这有利于数据库做全表扫描。
***** B树相对于B+树的优点是，如果经常访问的数据离根节点很近，而B树的非叶子节点本身存有关键字其数据的地址，所以这种数据检索的时候会要比B+树快。
      
*** [#B] Hash索引和B+树索引有什么区别(不支持范围查询、顺序改变、不支持模糊查询、不支持多列索引最左匹配、无法覆盖索引、性能不稳定)
**** hash索引进行等值查询更快(一般情况下)，但是却无法进行范围查询。
**** 因为在hash索引中经过hash函数建立索引之后，索引的顺序与原顺序无法保持一致，不能支持范围查询。而B+树的的所有节点皆遵循(左节点小于父节点，右节点大于父节点，多叉树也类似)，天然支持范围。
**** hash索引不支持使用索引进行排序，原理同上。
**** hash索引不支持模糊查询以及多列索引的最左前缀匹配。原理也是因为hash函数的不可预测。AAAA和AAAAB的索引没有相关性。
**** hash索引任何时候都避免不了回表查询数据，而B+树在符合某些条件(聚簇索引，覆盖索引等)的时候可以只通过索引完成查询。
**** hash索引虽然在等值查询上较快，但是不稳定。性能不可预测，当某个键值存在大量重复的时候，发生hash碰撞，此时效率可能极差。而B+树的查询效率比较稳定，对于所有的查询都是从根节点到叶子节点，且树的高度较低。
*** [#B] 数据库为什么使用B+树而不是B树(不支持顺序检索、B+树内部节点小【空间利用率高】、B+树查询稳定、B+删除节点效率高)
**** B树只适合随机检索，而B+树同时支持随机检索和顺序检索；
**** B+树空间利用率更高，可减少I/O次数，磁盘读写代价更低。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。
    B+树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素；
**** B+树的查询效率更加稳定。B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，
    任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。
**** B-树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。
**** 增删文件（节点）时，效率更高。因为B+树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。
*** DONE 什么是脏读？幻读？不可重复读？(事务并发的问题)
    CLOSED: [2021-02-03 Wed 16:42]
**** 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。
**** 不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。
**** 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。
*** DONE SQL 标准定义了四个隔离级别
    CLOSED: [2021-02-03 Wed 16:43]
**** 隔离级别	       脏读	不可重复读	幻影读
**** READ-UNCOMMITTED√	√	        √     （写-读）
**** READ-COMMITTED	×	√	        √      (读-写)
**** REPEATABLE-READ	×	×	        √      (读-插入)
**** SERIALIZABLE	×	×	        ×
**** READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。
**** READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。
**** REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。
**** SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。
*** DONE 默认级别隔离级别
    CLOSED: [2021-02-03 Wed 16:43]
**** Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别
*** [#B] 隔离级别与锁的关系
**** 在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突
**** 在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁；
**** 在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。
**** SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。
**** 读未提交（Read Uncommitted）：事务读不阻塞其他事务读和写，事务写阻塞其他事务写但不阻塞读；通过对写操作加 “持续X锁”，对读操作不加锁 实现；
**** 读已提交（Read Committed）：事务读不会阻塞其他事务读和写，事务写会阻塞其他事务读和写；通过对写操作加 “持续X锁”，对读操作加 “临时S锁” 实现；不会出现脏读；
**** 可重复读（Repeatable Read）：事务读会阻塞其他事务事务写但不阻塞读，事务写会阻塞其他事务读和写；通过对写操作加 “持续X锁”，对读操作加 “持续S锁” 实现；
**** 序列化（Serializable）：为了解决幻读问题，行级锁做不到，需使用表级锁。
*** DONE 按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法
    CLOSED: [2021-02-03 Wed 16:54]
**** 在关系型数据库中，可以按照锁的粒度把数据库锁分为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。
**** MyISAM采用表级锁(table-level locking)。
**** InnoDB支持行级锁(row-level locking)和表级锁，默认为行级锁
*** DONE 行级锁，表级锁和页级锁对比
    CLOSED: [2021-02-03 Wed 16:56]
**** 行级锁 行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。
***** 特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。
**** 表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。
***** 特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。
**** 页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。
***** 特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般
*** [#B] 从锁的类别上分MySQL都有哪些锁呢
**** 共享锁: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。
**** 排他锁: 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。
*** DONE MySQL中InnoDB引擎的行锁是怎么实现的
    CLOSED: [2021-02-03 Wed 16:57]
**** InnoDB是基于索引来完成行锁
***** select * from tab_with_index where id = 1 for update;
***** for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么InnoDB将完成表锁，并发将无从谈起
*** [#B] InnoDB存储引擎的锁的算法
**** Record lock：单个行记录上的锁
**** Gap lock：间隙锁，锁定一个范围，不包括记录本身
**** Next-key lock：record+gap 锁定一个范围，包含记录本身
*** DONE 三、四级加锁协议
    CLOSED: [2021-02-03 Wed 16:59]
**** 一级加锁协议
***** 事务在修改数据前必须加X锁，直到事务结束（提交或终止）才可释放；如果仅仅是读数据，不需要加锁。
**** 二级加锁协议(read committed)
***** 满足一级加锁协议，且事务在读取数据之前必须先加S锁，读完后即可释放S锁。
**** 三级加锁协议(repeatable read)
***** 满足一级加锁协议，且事务在读取数据之前必须先加S锁，直到事务结束才释放。
**** 4. 两段锁协议（2-phase locking）(serializable)
***** 加锁阶段：事务在读数据前加S锁，写数据前加X锁，加锁不成功则等待。
***** 解锁阶段：一旦开始释放锁，就不允许再加锁了。

*** [#B] 多版本并发控制(mvcc)
**** 虽然数据库的四种隔离级别通过 LBCC(基于锁的并发控制) 技术都可以实现，但是它最大的问题是它只实现了并发的读读，对于并发的读写还是冲突的，写时不能读，读时不能写，当读写操作都很频繁时，
     数据库的并发性将大大降低，针对这种场景，MVCC 技术应运而生
**** 事务在写一条记录时会将其拷贝一份生成这条记录的一个原始拷贝，写操作同样还是会对原记录加锁，但是读操作会读取未加锁的新记录，这就保证了读写并行。要注意的是，生成的新版本其实就是 undo log，
     它也是实现事务回滚的关键技术。
*** [#B] rr（repeatable read）和rc（read committed）都是快照读
**** 尽管 RR 和 RC 隔离级别都实现了 MVCC 来满足读写并行，但是读的实现方式是不一样的：RC 总是读取记录的最新版本，如果该记录被锁住，则读取该记录最新的一次快照，而 RR 是读取该记录事务开始时的那个版本。
     虽然这两种读取方式不一样，但是它们读取的都是快照数据，并不会被写操作阻塞，所以这种读操作称为 快照读（Snapshot Read），有时候也叫做 非阻塞读（Nonlocking Read），RR 隔离级别下的叫做
     一致性非阻塞读（Consistent Nonlocking Read）。
*** [#B] 当前读写方式(意向锁,当前读)
**** SELECT ... LOCK IN SHARE MODE：加 S 锁（在事务只读的场景下使用）
**** SELECT ... FOR UPDATE：加 X 锁（在事务读取数据后要修改的场景下使用）
**** INSERT / UPDATE / DELETE：加 X 锁
*** 意向锁
**** 排他 / 共享锁指的都是表锁！！！意向锁不会与行级的共享 / 排他锁互斥！！！
**** 意向锁不会与行级的共享 / 排他锁互斥！！！
*** [#B] 当前读的两种隔离级别
**** RC 只加记录锁
**** RR 除了加记录锁，还会加间隙锁，用于解决幻读问题
*** [#B] InnoDB存储引擎的锁的算法有三种
**** Record lock：记录锁
**** Gap lock：间隙锁，锁定一个范围，不包括记录本身
**** Next-key lock：record+gap 锁定一个范围，包含记录本身
**** 相关知识点
***** innodb对于行的查询使用next-key lock
***** Next-locking keying为了解决Phantom Problem幻读问题
***** 当查询的索引含有唯一属性时，将next-key lock降级为record key
***** Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生
***** 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1
*** [#B] 什么是死锁？怎么解决？
**** 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。
**** 常见的解决死锁的方法
**** 1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。
**** 2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；
**** 3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率；
**** 如果业务处理不好可以用分布式事务锁或者使用乐观锁
*** [#B] 数据库的乐观锁和悲观锁是什么？怎么实现的？
**** 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制
**** 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现。
**** 两种锁的使用场景
***** 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。
***** 但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。
*** [#B] 什么是存储过程？
**** 优点
***** 1）存储过程是预编译过的，执行效率高。
***** 2）存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。
***** 3）安全性高，执行存储过程需要有一定权限的用户。
***** 4）存储过程可以重复使用，减少数据库开发人员的工作量。
**** 缺点
***** 1）调试麻烦，但是用 PL/SQL Developer 调试很方便！弥补这个缺点。
***** 2）移植问题，数据库端代码当然是与数据库相关的。但是如果是做工程型项目，基本不存在移植问题。
***** 3）重新编译问题，因为后端代码是运行前编译的，如果带有引用关系的对象发生改变时，受影响的存储过程、包将需要重新编译（不过也可以设置成运行时刻自动编译）。
*** [#B] 六种关联查询
**** 内连接（INNER JOIN）(SELECT * FROM A T1 INNER JOIN A T2 ON T1.id=T2.pid 两边都有的)
**** 外连接（LEFT JOIN/RIGHT JOIN）
**** 联合查询（UNION与UNION ALL）(SELECT * FROM A UNION SELECT * FROM B U ...)(就是把多个结果集集中在一起，UNION前的结果为基准，需要注意的是联合查询的列数要相等，相同的记录行会合并)
**** 全连接（FULL JOIN）(mysql中没有)
**** 交叉连接（CROSS JOIN）(select r.*,s.* from r,s 笛卡尔积)
*** [#B] 字段类型
**** int(20)中20的涵义
***** 是指显示字符的长度。20表示最大显示宽度为20，但仍占4字节存储，存储范围不变；
***** 不影响内部存储，只是影响带 zerofill 定义的 int 时，前面补多少个 0，易于报表展示
**** FLOAT和DOUBLE的区别是什么？
***** FLOAT类型数据可以存储至多8位十进制数，并在内存中占4字节。
***** DOUBLE类型数据可以存储至多18位十进制数，并在内存中占8字节。
*** [#B] 大表数据查询，怎么优化(索引、缓存、分表、主从)
**** 优化shema、sql语句+索引；
**** 第二加缓存，memcached, redis；
**** 主从复制，读写分离；
**** 垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统；
**** 水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表；
*** [#A] 超大分页怎么处理
**** 使用覆盖索引
***** select * from table where id in (select id from table where age > 20 limit 1000000,10)
***** 直接拿出id来，回表只需要10条
*** DONE 慢查询日志
    CLOSED: [2021-02-03 Wed 17:13]
**** 开启慢查询日志
**** 配置项：slow_query_log
**** 可以使用show variables like ‘slov_query_log’查看是否开启，如果状态值为OFF，可以使用set GLOBAL slow_query_log = on来开启，它会在datadir下产生一个xxx-slow.log的文件。
**** 设置临界时间
**** 配置项：long_query_time
**** 查看：show VARIABLES like 'long_query_time'，单位秒
**** 设置：set long_query_time=0.5
*** [#A] 关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？
**** 在业务系统中，除了使用主键进行的查询，其他的我都会在测试库上测试其耗时，慢查询的统计主要由运维在做，会定期将业务中的慢查询反馈给我们。
**** 优化
***** 首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。
***** 分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。
***** 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。
*** [#B] 如果要存储用户的密码散列，应该使用什么字段进行存储？
**** 密码散列，盐，用户身份证号等固定长度的字符串应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。
*** [#B] 优化where句子
**** 1.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。
**** 2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：
***** select id from t where num is null
**** 3.应尽量避免在 where 子句中使用!=或<>操作符，否则引擎将放弃使用索引而进行全表扫描。
**** 4.应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：
***** select id from t where num=10 or num=20
**** 5.in 和 not in 也要慎用，否则会导致全表扫描，如：
***** select id from t where num in(1,2,3) 
**** 6.下面的查询也将导致全表扫描：select id from t where name like ‘%李%’若要提高效率，可以考虑全文检索。
**** 7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，
    因而无法作为索引选择的输入项。如下面语句将进行全表扫描：
***** select id from t where num=@num
**** 8.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：
***** select id from t where num/2=100
**** 9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：
***** select id from t where substring(name,1,3)=’abc’
**** 10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。
*** [#C] 数据库结构优化
**** 将字段很多的表分解成多个表
**** 增加中间表
**** 增加冗余字段
*** [#B] MySQL数据库cpu飙升到500%的话他怎么处理？
**** 当 cpu 飙升到 500%时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。
**** 如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。
**** 一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。
**** 也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等
*** [#A] 大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？
**** 优化措施
***** 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内。；
***** 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读；
***** 缓存： 使用MySQL的缓存，另外对重量级、更新少的数据可以考虑使用应用级别的缓存；
***** 主要有垂直分表和水平分表
****** 水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。
****** 垂直拆分：可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。但是主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。
       此外，垂直分区会让事务变得更加复杂；
****** 《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，
       尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。
**** 分库分表后面临的问题
***** 事务支持 分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。
***** 跨库join
****** 只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,
       根据这些id发起第二次请求得到关联数据。 分库分表方案产品
***** 跨节点的count,order by,group by以及聚合函数问题 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，
      分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。
***** 数据迁移，容量规划，扩容等问题 来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，
      同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。
***** ID问题
****** 一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由.
       一些常见的主键生成策略
******  Twitter的分布式自增ID算法Snowflake 在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，实现也还是很简单的，除去配置信息，
       核心代码就是毫秒级时间41位 机器ID 10位 毫秒内序列12位
*** [#A] MySQL主从
**** 主从复制流程
***** 将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行（重做）；从而使得从数据库的数据与主数据库保持一致。
**** 主从复制的作用
***** 主数据库出现问题，可以切换到从数据库。
***** 可以进行数据库层面的读写分离。
***** 可以在从数据库上进行日常备份。
**** MySQL主从复制解决的问题
***** 数据分布：随意开始或停止复制，并在不同地理位置分布数据备份
***** 负载均衡：降低单个服务器的压力
***** 高可用和故障切换：帮助应用程序避免单点失败
***** 升级测试：可以用更高版本的MySQL作为从库
**** MySQL主从复制工作原理
***** 在主库上把数据更改记录到二进制日志
***** 从库将主库的日志复制到自己的中继日志
***** 从库读取中继日志的事件，将其重放到从库数据中
**** 基本原理流程，3个线程以及之间的关联
***** 主：binlog线程——记录下所有改变了数据库数据的语句，放进master上的binlog中；
***** 从：io线程——在使用start slave 之后，负责从master上拉取 binlog 内容，放进自己的relay log中；
***** 从：sql执行线程——执行relay log中的语句；
*** TODO [#A] 读写分离有哪些解决方案？
**** 使用mysql-proxy代理
***** 优点：直接实现读写分离和负载均衡，不用修改代码，master和slave用一样的帐号，mysql官方不建议实际生产中使用
***** 缺点：降低性能， 不支持事务
**** 使用AbstractRoutingDataSource+aop+annotation在dao层决定数据源。
***** 如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过mybatis plugin拦截sql语句，所有的insert/update/delete都访问master库，所有的select 都访问salve库，这样对于dao层都是透明。
     plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。不过这样依然有一个问题， 也就是不支持事务， 所以我们还需要重写一下DataSourceTransactionManager， 将read-only的事务扔进读库，
     其余的有读有写的扔进写库。
**** 使用AbstractRoutingDataSource+aop+annotation在service层决定数据源，可以支持事务.
***** 缺点：类内部方法通过this.xx()方式相互调用时，aop不会进行拦截，需进行特殊处理。
*** TODO [#A] 备份计划
**** 备份方案
***** 视库的大小来定，一般来说 100G 内的库，可以考虑使用 mysqldump 来做，因为 mysqldump更加轻巧灵活，备份时间选在业务低峰期，可以每天进行都进行全量备份(mysqldump 备份出来的文件比较小，压缩之后更小)。
***** 100G 以上的库，可以考虑用 xtranbackup 来做，备份速度明显要比 mysqldump 要快。一般是选择一周一个全备，其余每天进行增量备份，备份时间为业务低峰期。
**** 备份恢复时间
***** 物理备份恢复快，逻辑备份恢复慢
***** 这里跟机器，尤其是硬盘的速率有关系，以下列举几个仅供参考
****** 20G的2分钟（mysqldump）
****** 80G的30分钟(mysqldump)
****** 111G的30分钟（mysqldump)
****** 288G的3小时（xtra)
****** 3T的4小时（xtra)
**** 备份恢复失败如何处理
***** 首先在恢复之前就应该做足准备工作，避免恢复的时候出错。比如说备份之后的有效性检查、权限检查、空间检查等。如果万一报错，再根据报错的提示来进行相应的调整。
**** mysqldump和xtrabackup实现原理
***** mysqldump
****** mysqldump 属于逻辑备份。加入–single-transaction 选项可以进行一致性备份。后台进程会先设置 session 的事务隔离级别为 RR(SET SESSION TRANSACTION ISOLATION LEVELREPEATABLE READ)，
      之后显式开启一个事务(START TRANSACTION /*!40100 WITH CONSISTENTSNAPSHOT */)，这样就保证了该事务里读到的数据都是事务事务时候的快照。之后再把表的数据读取出来。如果加上–master-data=1 的话，
      在刚开始的时候还会加一个数据库的读锁(FLUSH TABLES WITH READ LOCK),等开启事务后，再记录下数据库此时 binlog 的位置(showmaster status)，马上解锁，再读取表的数据。等所有的数据都已经导完，就可以结束事务
***** Xtrabackup:
****** xtrabackup 属于物理备份，直接拷贝表空间文件，同时不断扫描产生的 redo 日志并保存下来。最后完成 innodb 的备份后，会做一个 flush engine logs 的操作(老版本在有 bug，在5.6 上不做此操作会丢数据)，确保所有的 redo log 都已经落盘(涉及到事务的两阶段提交
      概念，因为 xtrabackup 并不拷贝 binlog，所以必须保证所有的 redo log 都落盘，否则可能会丢最后一组提交事务的数据)。这个时间点就是 innodb 完成备份的时间点，数据文件虽然不是一致性的，但是有这段时间的 redo 就可以让数据文件达到一致性(恢复的时候做的事情)。
      然后还需要 flush tables with read lock，把 myisam 等其他引擎的表给备份出来，备份完后解锁。这样就做到了完美的热备。
**** 数据表损坏的修复方式有哪些
***** 使用 myisamchk 来修复，具体步骤：
****** 1）修复前将mysql服务停止。
****** 2）打开命令行方式，然后进入到mysql的/bin目录。
****** 3）执行myisamchk –recover 数据库所在路径/*.MYI
****** 使用repair table 或者 OPTIMIZE table命令来修复，REPAIR TABLE table_name 修复表 OPTIMIZE TABLE table_name 优化表 REPAIR TABLE 用于修复被破坏的表。 OPTIMIZE TABLE 用于回收闲置的数据库空间，当表上的数据行被删除时，
      所占据的磁盘空间并没有立即被回收，使用了OPTIMIZE TABLE命令后这些空间将被回收，并且对磁盘上的数据行进行重排（注意：是磁盘上，而非数据库）
*** 一个 SQL 执行的很慢
**** 大多数情况下很正常，偶尔很慢，则有如下原因
***** 数据库在刷新脏页，例如 redo log 写满了需要同步到磁盘。
***** 执行的时候，遇到锁，如表锁、行锁。
**** 这条 SQL 语句一直执行的很慢，则有如下原因
***** 没有用上索引：例如该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。
***** 数据库选错了索引
***** sql语句不合理
*** 分库分表
**** 水平分区： 数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。
**** 垂直分区： 
***** 垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。
***** 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；
**** 读/写分离
***** 经典的数据库拆分方案，主库负责写，从库负责读；
**** 实现方式
***** 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。
***** 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。
**** 分库分表后id如何生成
***** 可用redis来生成
***** Twitter的snowflake算法 
** TODO 13	消息中间件MQ与RabbitMQ面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104588612
*** [#B] 为什么使用MQ？MQ的优点(主要是：解耦、异步、削峰)
**** 异步处理 - 相比于传统的串行、并行方式，提高了系统吞吐量。
**** 应用解耦 - 系统间通过消息通信，不用关心其他系统的处理。
**** 流量削锋 - 可以通过消息队列长度控制请求量；可以缓解短时间内的高并发请求。
**** 日志处理 - 解决大量日志传输。
**** 消息通讯 - 消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯。比如实现点对点消息队列，或者聊天室等。
*** [#B] 解耦、异步、削峰
**** 解耦：A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃…A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，
    很多系统都需要 A 系统将这个数据发送过来。如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。
    这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。
    但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦。
**** 异步：A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，
    用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求。如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms。
**** 削峰：减少高峰时期对服务器压力。
*** [#B] 消息队列有什么优缺点？RabbitMQ有什么优缺点？(可用性降低、复杂度提升、一致性问题)
**** 缺点
***** 系统可用性降低
****** 本来系统运行好好的，现在你非要加入个消息队列进去，那消息队列挂了，你的系统不是呵呵了。因此，系统可用性会降低；
***** 系统复杂度提高
****** 加入了消息队列，要多考虑很多方面的问题，比如：一致性问题、如何保证消息不被重复消费、如何保证消息可靠性传输等。因此，需要考虑的东西更多，复杂性增大。
***** 一致性问题
****** A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。
*** Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？
**** 	        ActiveMQ	    RabbitMQ	        RocketMQ	Kafka	  ZeroMQ
**** 单机吞吐量	比RabbitMQ低	2.6w/s（消息做持久化）	11.6w/s	    17.3w/s	  29w/s
**** 开发语言	    Java	         Erlang	            Java	    Scala/Java	  C
**** 主要维护者	Apache	          Mozilla/Spring	Alibaba	    Apache	iMatix，创始人已去世
**** 成熟度	     成熟	           成熟	         开源版本不够成熟	比较成熟	只有C、PHP等版本成熟
**** 订阅形式	点对点(p2p)、广播（发布-订阅）
    提供了4种：direct, topic ,Headers和fanout。fanout就是广播模式
    基于topic/messageTag以及按照消息类型、属性进行正则匹配的发布订阅模式
    基于topic以及按照topic进行正则匹配的发布订阅模式
    点对点(p2p)
**** 持久化	    支持少量堆积	   支持少量堆积	      支持大量堆积	支持大量堆积	不支持
**** 顺序消息	    不支持	       不支持	             支持	      支持	    不支持
**** 性能稳定性	好	             好	                 一般	      较差	     很好
**** 集群方式	支持简单集群模式，比如’主-备’，对高级集群模式支持不好。
    支持简单集群，'复制’模式，对高级集群模式支持不好。
    常用 多对’Master-Slave’ 模式，开源版本需手动切换Slave变成Master
    天然的‘Leader-Slave’无状态集群，每台服务器既是Master也是Slave
    不支持
**** 管理界面	    一般	        较好	             一般	       无	     无
*** [#B] 消息的顺序问题(1.生产者-mq-消费者一对一对一，2.从业务层面保证消息顺序)
**** 消息有序指的是可以按照消息的发送顺序来消费。
**** 方案1
***** 保证生产者 - MQServer - 消费者是一对一对一的关系
***** 并行度就会成为消息系统的瓶颈（吞吐量不够）
***** 更多的异常处理，比如：只要消费端出现问题，就会导致整个处理流程阻塞，我们不得不花费更多的精力来解决阻塞的问题。 
**** 方案2
***** 通过合理的设计或者将问题分解来规避。
***** 不关注乱序的应用实际大量存在
***** 队列无序并不意味着消息无序 所以从业务层面来保证消息的顺序而不仅仅是依赖于消息系统，是一种更合理的方式。
*** [#B] 消息的重复问题 
**** 造成消息重复的根本原因是：网络不可达。
**** 所以解决这个问题的办法就是绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？
**** 消费端处理消息的业务逻辑保持幂等性。只要保持幂等性，不管来多少条重复消息，最后处理的结果都一样。保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现。利用一张日志表来记录已经处理成功的消息的 ID，
    如果新到的消息 ID 已经在日志表中，那么就不再处理这条消息。
*** [#B] rabbitmq 的使用场景
**** （1）服务间异步通信
**** （2）顺序消费
**** （3）定时任务
**** （4）请求削峰
*** RabbitMQ基本概念(borker、exchange、queue、binding、routing key、vhost、producer、consumer、channel【每个客户端连接有多个channel】)
**** Broker： 简单来说就是消息队列服务器实体
**** Exchange： 消息交换机，它指定消息按什么规则，路由到哪个队列
**** Queue： 消息队列载体，每个消息都会被投入到一个或多个队列
**** Binding： 绑定，它的作用就是把exchange和queue按照路由规则绑定起来
**** Routing Key： 路由关键字，exchange根据这个关键字进行消息投递
**** VHost： vhost 可以理解为虚拟 broker ，即 mini-RabbitMQ server。其内部均含有独立的 queue、exchange 和 binding 等，但最最重要的是，其拥有独立的权限系统，可以做到 vhost 范围的用户控制。当然，从 RabbitMQ 的全局角度，
     vhost 可以作为不同权限隔离的手段（一个典型的例子就是不同的应用可以跑在不同的 vhost 中）。
**** Producer： 消息生产者，就是投递消息的程序
**** Consumer： 消息消费者，就是接受消息的程序
**** Channel： 消息通道，在客户端的每个连接里，可建立多个channel，每个channel代表一个会话任务
*** RabbitMQ的工作模式(simple【没有用exchanger】、work【多个消费者轮询】、pub/sub【fanout】、routing【direct】、topic【topic】)
**** 一.simple模式（即最简单的收发模式）
***** 1.消息产生消息，将消息放入队列
***** 2.消息的消费者(consumer) 监听 消息队列,如果队列中有消息,就消费掉,消息被拿走后,自动从队列中删除(隐患 消息可能没有被消费者正确处理,已经从队列中消失了,造成消息的丢失，这里可以设置成手动的ack,但如果设置成手动ack，
     处理完后要及时发送ack消息给队列，否则会造成内存溢出)。
**** 二.work工作模式(资源的竞争)
***** 1.消息产生者将消息放入队列消费者可以有多个,消费者1,消费者2同时监听同一个队列,消息被消费。C1 C2共同争抢当前的消息队列内容,谁先拿到谁负责消费消息(隐患：高并发情况下,默认会产生某一个消息被多个消费者共同使用,
     可以设置一个开关(syncronize) 保证一条消息只能被一个消费者使用)。
**** 三.publish/subscribe发布订阅(共享资源)
***** 1、每个消费者监听自己的队列；
***** 2、生产者将消息发给broker，由交换机将消息转发到绑定此交换机的每个队列，每个绑定交换机的队列都将接收到消息。
**** 四.routing路由模式
***** 1.消息生产者将消息发送给交换机按照路由判断,路由是字符串(info) 当前产生的消息携带路由字符(对象的方法),交换机根据路由的key,只能匹配上路由key对应的消息队列,对应的消费者才能消费消息;
***** 2.根据业务功能定义路由字符串
***** 3.从系统的代码逻辑中获取对应的功能字符串,将消息任务扔到对应的队列中。
***** 4.业务场景:error 通知;EXCEPTION;错误通知的功能;传统意义的错误通知;客户通知;利用key路由,可以将程序中的错误封装成消息传入到消息队列中,开发者可以自定义消费者,实时接收错误;
**** 五.topic 主题模式(路由模式的一种)
***** 1.星号井号代表通配符
***** 2.星号代表多个单词,井号代表一个单词
***** 3.路由功能添加模糊匹配
***** 4.消息产生者产生消息,把消息交给交换机
***** 5.交换机根据key的规则模糊匹配到对应的队列,由队列的监听消费者接收消息消费
***** （在我的理解看来就是routing查询的一种模糊匹配，就类似sql的模糊查询方式）
*** 如何保证RabbitMQ消息的顺序性？
**** 拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。
*** 消息如何分发？
**** 若该队列至少有一个消费者订阅，消息将以循环（round-robin）的方式发送给消费者。每条消息只会分发给一个订阅的消费者（前提是消费者能够正常处理消息并进行确认）。通过路由可实现多消费的功能
*** 消息怎么路由？(fanout、direct、topic、header)
**** 消息提供方->路由->一至多个队列消息发布到交换器时，消息将拥有一个路由键（routing key），在消息创建时设定。通过队列路由键，可以把队列绑定到交换器上。消息到达交换器后，
    RabbitMQ 会将消息的路由键与队列的路由键进行匹配（针对不同的交换器有不同的路由规则）；
**** 常用的交换器主要分为一下三种：
***** fanout：如果交换器收到消息，将会广播到所有绑定的队列上
***** direct：如果路由键完全匹配，消息就被投递到相应的队列
***** topic：可以使来自不同源头的消息能够到达同一个队列。 使用 topic 交换器时，可以使用通配符
***** headers
*** 消息基于什么传输？(tcp内的信道channel)
**** 由于 TCP 连接的创建和销毁开销较大，且并发数受系统资源限制，会造成性能瓶颈。RabbitMQ 使用信道的方式来传输数据。信道是建立在真实的 TCP 连接内的虚拟连接，且每条 TCP 连接上的信道数量没有限制。
*** 如何保证消息不被重复消费？或者说，如何保证消息消费时的幂等性？
**** 先说为什么会重复消费：正常情况下，消费者在消费消息的时候，消费完毕后，会发送一个确认消息给消息队列，消息队列就知道该消息被消费了，就会将该消息从消息队列中删除；
    但是因为网络传输等等故障，确认信息没有传送到消息队列，导致消息队列不知道自己已经消费过该消息了，再次将消息分发给其他的消费者。
    针对以上问题，一个解决思路是：保证消息的唯一性，就算是多次传输，不要让消息的多次消费带来影响；保证消息等幂性；
    比如：在写入消息队列的数据做唯一标示，消费消息时，根据唯一标识判断是否消费过；
    假设你有个系统，消费一条消息就往数据库里插入一条数据，要是你一个消息重复两次，你不就插入了两条，这数据不就错了？但是你要是消费到第二次的时候，自己判断一下是否已经消费过了，若是就直接扔了，这样不就保留了一条数据，从而保证了数据的正确性。
*** 如何确保消息正确地发送至 RabbitMQ？ 如何确保消息接收方消费了消息？
**** 发送方确认模式
***** 将信道设置成 confirm 模式（发送方确认模式），则所有在信道上发布的消息都会被指派一个唯一的 ID。
     一旦消息被投递到目的队列后，或者消息被写入磁盘后（可持久化的消息），信道会发送一个确认给生产者（包含消息唯一 ID）。
     如果 RabbitMQ 发生内部错误从而导致消息丢失，会发送一条 nack（notacknowledged，未确认）消息。
     发送方确认模式是异步的，生产者应用程序在等待确认的同时，可以继续发送消息。当确认消息到达生产者应用程序，生产者应用程序的回调方法就会被触发来处理确认消息。
**** 接收方确认机制
***** 消费者接收每一条消息后都必须进行确认（消息接收和消息确认是两个不同操作）。只有消费者确认了消息，RabbitMQ 才能安全地把消息从队列中删除。 这里并没有用到超时机制，
     RabbitMQ 仅通过 Consumer 的连接中断来确认是否需要重新发送消息。也就是说，只要连接不中断，RabbitMQ 给了 Consumer 足够长的时间来处理消息。保证数据的最终一致性；
***** 下面罗列几种特殊情况
****** 如果消费者接收到消息，在确认之前断开了连接或取消订阅，RabbitMQ 会认为消息没有被分发，然后重新分发给下一个订阅的消费者。（可能存在消息重复消费的隐患，需要去重）
****** 如果消费者接收到消息却没有确认消息，连接也未断开，则 RabbitMQ 认为该消费者繁忙，将不会给该消费者分发更多的消息。
*** 如何保证RabbitMQ消息的可靠传输？(生产者使用transaction或confirm模式，消息队列持久化，消费者手动确认)
**** 消息不可靠的情况可能是消息丢失，劫持等原因；
**** 丢失又分为：
***** 生产者丢失消息、消息列表丢失消息、消费者丢失消息；
***** 生产者丢失消息：
****** 从生产者弄丢数据这个角度来看，RabbitMQ提供transaction和confirm模式来确保生产者不丢消息；
      transaction机制就是说：发送消息前，开启事务（channel.txSelect()）,然后发送消息，如果发送过程中出现什么异常，事务就会回滚（channel.txRollback()）,如果发送成功则提交事务（channel.txCommit()）。
      然而，这种方式有个缺点：吞吐量下降；
      confirm模式用的居多：一旦channel进入confirm模式，所有在该信道上发布的消息都将会被指派一个唯一的ID（从1开始），一旦消息被投递到所有匹配的队列之后；
      rabbitMQ就会发送一个ACK给生产者（包含消息的唯一ID），这就使得生产者知道消息已经正确到达目的队列了；如果rabbitMQ没能处理该消息，则会发送一个Nack消息给你，你可以进行重试操作。
***** 消息队列丢数据：消息持久化。
****** 处理消息队列丢数据的情况，一般是开启持久化磁盘的配置。这个持久化配置可以和confirm机制配合使用，你可以在消息持久化磁盘后，再给生产者发送一个Ack信号。
      这样，如果消息持久化磁盘之前，rabbitMQ阵亡了，那么生产者收不到Ack信号，生产者会自动重发。那么如何持久化呢？这里顺便说一下吧，其实也很容易，就下面两步将queue的持久化标识durable设置为true,
      则代表是一个持久的队列，发送消息的时候将deliveryMode=2这样设置以后，即使rabbitMQ挂了，重启后也能恢复数据
***** 消费者丢失消息：
****** 消费者丢数据一般是因为采用了自动确认消息模式，改为手动确认消息即可！消费者在收到消息之后，处理消息之前，会自动回复RabbitMQ已收到消息；如果这时处理消息失败，就会丢失该消息；
      解决方案：处理消息成功后，手动回复确认消息。
*** 为什么不应该对所有的 message 都使用持久化机制？(性能下降、集群坑爹问题)
**** 首先，必然导致性能的下降，因为写磁盘比写 RAM 慢的多，message 的吞吐量可能有 10 倍的差距。
**** 其次，message 的持久化机制用在 RabbitMQ 的内置 cluster 方案时会出现“坑爹”问题。矛盾点在于，若 message 设置了 persistent 属性，
    但 queue 未设置 durable 属性，那么当该 queue 的 owner node 出现异常后，在未重建该 queue 前，发往该 queue 的 message 将被 blackholed ；
    若 message 设置了 persistent 属性，同时 queue 也设置了 durable 属性，那么当 queue 的 owner node 异常且无法重启的情况下，则该 queue 无法在其他 node 上重建，
    只能等待其 owner node 重启后，才能恢复该 queue 的使用，而在这段时间内发送给该 queue 的 message 将被 blackholed 。
**** 所以，是否要对 message 进行持久化，需要综合考虑性能需要，以及可能遇到的问题。若想达到 100,000 条/秒以上的消息吞吐量（单 RabbitMQ 服务器），
    则要么使用其他的方式来确保 message 的可靠 delivery ，要么使用非常快速的存储系统以支持全持久化（例如使用 SSD）。另外一种处理原则是：仅对关键消息作持久化处理（根据业务重要程度），
    且应该保证关键消息的量不会导致性能瓶颈。
*** 如何保证高可用的？RabbitMQ 的集群(单例、普通集群、镜像集群模式)
**** RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。RabbitMQ 有三种模式：单机模式、
    普通集群模式、镜像集群模式。
**** 单机模式
***** 就是 Demo 级别的，一般就是你本地启动了玩玩儿的?，没人生产用单机模式
**** 普通集群模式
***** 意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，
     通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。这方案主要是提高吞吐量的，
     就是说让集群中多个节点来服务某个 queue 的读写操作。
**** 镜像集群模式
***** 这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，
     每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。RabbitMQ 有很好的管理控制台,
     就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。
     这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，
     消息需要同步到所有机器上，导致网络带宽压力和消耗很重！RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。
*** 如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？
**** 消息积压处理办法：临时紧急扩容
**** 步骤
***** 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 cnosumer 都停掉。
***** 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。
***** 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。
***** 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。
***** 等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。
**** MQ中消息失效：
***** 假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。
     那这就是第二个坑了。这就不是说数据会大量积压在 mq 里，而是大量的数据会直接搞丢。我们可以采取一个方案，就是批量重导，这个我们之前线上也有类似的场景干过。就是大量积压的时候，
     我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，
     然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，
     手动发到 mq 里去再补一次。
**** mq消息队列块满了：
***** 如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，
     都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。
** DONE 15	Linux面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104588679
   CLOSED: [2021-04-14 Wed 23:01]
*** Linux 的体系结构(用户空间、内核空间)
**** 用户空间(User Space) ：用户空间又包括用户的应用程序(User Applications)、C 库(C Library) 。
**** 内核空间(Kernel Space) ：内核空间又包括系统调用接口(System Call Interface)、内核(Kernel)、平台架构相关的代码(Architecture-Dependent Kernel Code) 。
*** 为什么 Linux 体系结构要分为用户空间和内核空间的原因？(不同模式下可以执行指令和寄存器不同)
**** 1、现代 CPU 实现了不同的工作模式，不同模式下 CPU 可以执行的指令和访问的寄存器不同。
**** 2、Linux 从 CPU 的角度出发，为了保护内核的安全，把系统分成了两部分。
*** Linux 使用的进程间通信方式？(管道、信号、消息队列、共享内存、信号量、套接字)
**** 1、管道(pipe)、流管道(s_pipe)、有名管道(FIFO)。
***** 管道可用于具有亲缘关系进程间的通信,允许一个进程和另一个与它有共同祖先的进程之间进行通信。
***** 命名管道克服了管道没有名字的限制,因此,除具有管道所具有的功能外,它还允许无亲缘关系进程间的通信。命名管道在文件系统中有对应的文件名。命名管道通过命令 mkfifo 或系统调用 mkfifo 来创建。
**** 2、信号(signal) 。
***** 用于通知接受进程有某种事件发生,除了用于进程间通信外,进程还可以发送信号给进程本身;linux 除了支持 Unix 早期信号语义函数 sigal外,还支持语义符合 Posix.1 标准的信号函数 sigaction
**** 3、消息队列。
***** 消息队列是消息的链接表,包括 Posix 消息队列 system V 消息队列。有足够权限的进程可以向队列中添加消息,被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少,管道只能承载无格式字节流以及缓冲区大小受限等缺
**** 4、共享内存。
***** 使得多个进程可以访问同一块内存空间,是最快的可用 IPC 形式。是针对其他通信机制运行效率较低而设计的。往往与其它通信机制,如信号量结合使用,来达到进程间的同步及互斥。
**** 5、信号量。
***** 主要作为进程间以及同一进程不同线程之间的同步手段
**** 6、套接字(socket) 。
***** 更为一般的进程间通信机制,可用于不同机器之间的进程间通信。起初是由 Unix 系统的 BSD 分支开发出来的,但现在一般可以移植到其它类 Unix 系统上:Linux 和System V 的变种都支持套接字。
*** Linux 有哪些系统日志文件？(var/log/message文件、elk日志也会被收集进去)
**** 比较重要的是 /var/log/messages 日志文件。
**** 该日志文件是许多进程日志文件的汇总，从该文件可以看出任何入侵企图或成功的入侵。
**** 另外，如果胖友的系统里有 ELK 日志集中收集，它也会被收集进去。
*** Linux 的目录结构是怎样的？(/opt可以额外安装可选应用程序包如tomcat、/mnt系统管理员安装临时文件安装点、/boot系统引导文件、/lib系统运行相关库、/tmp临时文件、/var运行时需要改变数据的文件、大文见溢出区、日志文件)
**** /bin： 存放二进制可执行文件(ls,cat,mkdir等)，常用命令一般都在这里；
**** /etc： 存放系统管理和配置文件；
**** /home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示；
**** /usr： 用于存放系统应用程序；
**** /opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里；
**** /proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息；
**** /root： 超级用户（系统管理员）的主目录（特权阶级o）；
**** /sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等；
**** /dev： 用于存放设备文件；
**** /mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统；
**** /boot： 存放用于系统引导时使用的各种文件；
**** /lib： 存放着和系统运行相关的库文件 ；
**** /tmp： 用于存放各种临时文件，是公用的临时文件存储点；
**** /var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等；
**** /lost+found： 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里。
*** 什么是 inode ？(每一个文件都有对应的inode，里面包含了与该文件有关的一些信息)
*** Linux 虚拟文件系统的关键数据结构有哪些(struct super_block,struct inode,struct file,struct dentry)
*** 链接(硬链接的建立相当于copy一份副本)
**** 硬链接不可以跨分区，软件链可以跨分区。
**** 硬链接的建立相当于copy一份副本，除非所有硬链接都删除了，源文件不存在了。某种程度上讲，你说的也对，原始文件目录也可以看做是硬链接
*** Shell脚本中 $? 标记的用途是什么？
**** echo $? 查看前一命令是否成功
**** $0    命令行中的脚本名字
**** $1    第一个命令行参数
**** $2    第二个命令行参数
**** $9    第九个命令行参数
**** $##    命令行参数的数量
**** $*    所有命令行参数，以空格隔开
*** 如何调试 Shell脚本？(sh -x helo.sh或sh -nv helo.sh)
**** 使用 -x' 数（sh -x myscript.sh）可以调试 Shell脚本。
**** 另一个种方法是使用 -nv 参数(sh -nv myscript.sh)。
*** 如何执行算术运算？(1、expr 5 + 2，  2、test=$[5 + 2])
**** 1、使用 expr 命令：## expr 5 + 2 。
**** 2、用一个美元符号和方括号（$[ 表达式 ]）：test=$[16 + 4] ; test=$[16 + 4] 。
*** 针对网站访问慢，怎么去排查？(浏览器访问网站->浏览器调试->服务器资源查看->mysql慢查询)
**** 1、首先要确定是用户端还是服务端的问题。当接到用户反馈访问慢，那边自己立即访问网站看看，如果自己这边访问快，基本断定是用户端问题，就需要耐心跟客户解释，协助客户解决问题。
**** 不要上来就看服务端的问题。一定要从源头开始，逐步逐步往下。
**** 2、如果访问也慢，那么可以利用浏览器的调试功能，看看加载那一项数据消耗时间过多，是图片加载慢，还是某些数据加载慢。
**** 3、针对服务器负载情况。查看服务器硬件(网络、CPU、内存)的消耗情况。如果是购买的云主机，比如阿里云，可以登录阿里云平台提供各方面的监控，比如 CPU、内存、带宽的使用情况。
**** 4、如果发现硬件资源消耗都不高，那么就需要通过查日志，比如看看 MySQL慢查询的日志，看看是不是某条 SQL 语句查询慢，导致网站访问慢。
*** 怎么去解决？(带宽、sql优化、redis缓存、主从、分库分表、cdn、分布式集群)
**** 1、如果是出口带宽问题，那么久申请加大出口带宽。
**** 2、如果慢查询比较多，那么就要开发人员或 DBA 协助进行 SQL 语句的优化。
**** 3、如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等等。然后也可以搭建MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。
**** 4、申请购买 CDN 服务，加载用户的访问。
**** 5、如果访问还比较慢，那就需要从整体架构上进行优化咯。做到专角色专用，多台服务器提供同一个服务。
*** Linux 性能调优都有哪几种方法？
**** 1、Disabling daemons (关闭 daemons)。
**** 2、Shutting down the GUI (关闭 GUI)。
**** 3、Changing kernel parameters (改变内核参数)。
**** 4、Kernel parameters (内核参数)。
**** 5、Tuning the processor subsystem (处理器子系统调优)。
**** 6、Tuning the memory subsystem (内存子系统调优)。
**** 7、Tuning the file system (文件系统子系统调优)。
**** 8、Tuning the network subsystem（网络子系统调优)。
*** chmod权限范围
**** u ：目录或者文件的当前的用户
**** g ：目录或者文件的当前的群组
**** o ：除了目录或者文件的当前用户或群组之外的用户或者群组
**** a ：所有的用户及群组
*** linux锁(自旋锁和信号量、读写锁)
*** 对文件或设备的操作函数保存在那个数据结构中(struct file_operations)
***  Linux 中的文件包括哪些(执行文件,普通文件,目录文件,链接文件和设备文件,管道文件)
*** 创建进程的系统调用有那些(clone(),fork(),vfork();系统调用服务例程:sys_clone,sys_fork,sys_vfork;)
*** 调用 schedule()进行进程切换的方式有几种
**** 1.系统调用 do_fork();
**** 2.定时中断 do_timer();
**** 3.唤醒进程 wake_up_process
**** 4.改变进程的调度策略 setscheduler();
**** 5.系统调用礼让 sys_sched_yield();
***  Linux 调度程序是根据进程的动态优先级还是静态优先级来调度进程的?(动态优先级是为了防止饥饿，根据静态优先级计算出来的)
*** 如何加载、卸载一个模块?(insmod 加载,rmmod 卸载)
***  Linux 中的浮点运算由应用程序实现
**** 应用程序实现,Linux 中的浮点运算是利用数学库函数实现的,库函数能够被应用程序链接后调用,不能被内核链接调用。
*** 模块程序能否使用可链接的库函数(模块程序运行在内核空间,不能链接库函数)
*** TLB 中缓存的是什么内容
**** TLB,页表缓存,当线性地址被第一次转换成物理地址的时候,将线性地址和物理地址的对应放到 TLB 中,用于下次访问这个线性地址时,加快转换速度。
***  Linux 中有哪几种设备(字符设备和块设备)
**** ,mknod 系统调用用来创建设备文件
*** 设备驱动程序包括哪些功能函数
**** open(),read(),write(),llseek(),realse();
***  Linux 通过什么方式实现系统调用(软件中断实现)
** TODO 17	ZooKeeper面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104397719
*** 要点【必看】
扩容是什么意思？增加容量还是增加处理能力？ZK本来就不是当数据库的，每次最大返回数据大小默认的限制是在1M内，所以不在于提高容量。ZK主要用于协调的，增加从节点一可以提高读性能，二可以提高集群的容灾能力和可用性。
读取数据时可能是读取到旧数据的。只保证最终一致性。保证全局有序，同一个客户端发送写，写消息a在写消息b前发送，那么一定是先读到a更新。zk的所有写操作都由主节点进行管理，任何节点接到写请求都会把这个请求发送给主节点，
主节点发送事务proposal给所有从节点，zk的原子更新类似于2PC, 但是每次都只需要半数节点ACK就算同意更新, 主节点就发送commit给从节点，在恢复模式下, 只要有一半以上机子还存活着，就能选主出有最新zxid（事务）的机子.
(比如有5台机子, 编号ZXID为12的事务有3台机子(包括主节点)同意更新, 那么此时包括主节点在内的2台机子挂了, 还剩3台, 3+3>5, 所以必定有一台机子有最新的更新, 所以就能恢复最新的更新)推荐从Paxos到Zookeeper 这本书
*** 各种监听（getdata、getchildren、exists）都是消息通知的方式，一次性监听，需要重新监听
*** 队列：1.同步队列（创建临时目录节点，节点监听，若达到相应数量则进行）2.分布式FIFO队列（在特定目录下创建persist-sequential节点，通知watcher，取最小的进行执行）
*** 简介(zookeeper是分布式协调服务，进行集群监视和管理)(请求任意节点都可以【类似redis cluster】、有序性通过更新的时间戳【zxid】实现],读请求的时候会返回zxid)
**** ZooKeeper 是一个开源的分布式协调服务。它是一个为分布式应用提供一致性服务的软件，分布式应用程序可以基于 Zookeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、
     集群管理、Master 选举、分布式锁和分布式队列等功能。
**** Zookeeper 保证了如下分布式一致性特性：
***** （1）顺序一致性
***** （2）原子性
***** （3）单一视图
***** （4）可靠性
***** （5）实时性（最终一致性）
**** 客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的 zookeeper 机器来处理。对于写请求，这些请求会同时发给其他
     zookeeper 机器并且达成一致后，请求才会返回成功。因此，随着 zookeeper 的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。
**** 有序性是 zookeeper 中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为 zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，
     也就是读请求的返回结果中会带有这个zookeeper 最新的 zxid。
*** 分布式锁实现原理(节点目录下创建临时有序节点，获取所有子节点，若最小则已获得锁，若不是则监听相邻节点删除事件，等到相邻节点删除时，再次获取所有节点，判断是否是最小的，最后删除节点)
**** 1.客户端对某个方法加锁时，在 zk 上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点 node1;
**** 2.客户端获取该路径下所有已经创建的子节点，如果发现自己创建的 node1 的序号是最小的，就认为这个客户端获得了锁。
**** 3.如果发现 node1 不是最小的，则监听比自己创建节点序号小的最大的节点，进入等待。
**** 4.获取锁后，处理完逻辑，删除自己创建的 node1 即可。
**** 区别:zk 性能差一些，开销大，实现简单。
*** zookeeper 是如何保证事务的顺序一致性的？(采用了递增的事务 Id 来标识，所有的 proposal（提议）都在被提出的时候加上了 zxid,首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功)
*** ZooKeeper 提供了什么？(文件系统、通知机制)
**** 文件系统
**** 通知机制
*** Zookeeper 文件系统(每个节点都可以存放数据，最大数据量1M)
**** Zookeeper 提供一个多层级的节点命名空间（节点称为 znode）。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。
**** Zookeeper 为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得 Zookeeper 不能用于存放大量的数据，每个节点的存放数据上限为1M。
*** Zookeeper 怎么保证主从节点的状态同步？(原子广播机制、Zab协议、[恢复模式｜广播模式])
**** Zookeeper 的核心是原子广播机制，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式和广播模式。
**** 恢复模式
***** 当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。
**** 广播模式
***** 一旦 leader 已经和多数的 follower 进行了状态同步后，它就可以开始广播消息了，即进入广播状态。这时候当一个 server 加入 ZooKeeper 服务中，它会在恢复模式下启动，
      发现 leader，并和 leader 进行状态同步。待到同步结束，它也参与消息广播。ZooKeeper 服务一直维持在 Broadcast 状态，直到 leader 崩溃了或者 leader 失去了大部分的 followers 支持。
*** 四种类型的数据节点 Znode(持久节点、临时节点【会话相关】、持久顺序节点【有一个自增数字】、临时顺序节点)EPHEMERAL_SEQUENTIAL
**** （1）PERSISTENT-持久节点
***** 除非手动删除，否则节点一直存在于 Zookeeper 上
**** （2）EPHEMERAL-临时节点
***** 临时节点的生命周期与客户端会话绑定，一旦客户端会话失效（客户端与zookeeper 连接断开不一定会话失效），那么这个客户端创建的所有临时节点都会被移除。
**** （3）PERSISTENT_SEQUENTIAL-持久顺序节点
***** 基本特性同持久节点，只是增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。
**** （4）EPHEMERAL_SEQUENTIAL-临时顺序节点
***** 基本特性同临时节点，增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。
*** Zookeeper Watcher 机制 – 数据变更通知(注册节点监听，一次性、轻量)
**** Zookeeper 允许客户端向服务端的某个 Znode 注册一个 Watcher 监听，当服务端的一些指定事件触发了这个 Watcher，
     服务端会向指定客户端发送一个事件通知来实现分布式的通知功能，然后客户端根据 Watcher 通知状态和事件类型做出业务上的改变。
**** 工作机制：
***** （1）客户端注册 watcher
***** （2）服务端处理 watcher
***** （3）客户端回调 watcher
**** Watcher 特性总结：
***** （1）一次性
****** 无论是服务端还是客户端，一旦一个 Watcher 被 触 发 ，Zookeeper 都会将其从相应的存储中移除。这样的设计有效的减轻了服务端的压力，不然对于更新非常频繁的节点，服务端会不断的向客户端发送事件通知，
       无论对于网络还是服务端的压力都非常大。
***** （2）客户端串行执行
****** 客户端 Watcher 回调的过程是一个串行同步的过程。
***** （3）轻量
****** 3.1、Watcher 通知非常简单，只会告诉客户端发生了事件，而不会说明事件的具体内容。
****** 3.2、客户端向服务端注册 Watcher 的时候，并不会把客户端真实的 Watcher 对象实体传递到服务端，仅仅是在客户端请求中使用 boolean 类型属性进行了标记。
***** （4）watcher event 异步发送 watcher 的通知事件从 server 发送到 client 是异步的，这就存在一个问题，不同的客户端和服务器之间通过 socket 进行通信，
      由于网络延迟或其他因素导致客户端在不同的时刻监听到事件，由于 Zookeeper 本身提供了 ordering guarantee，即客户端监听事件后，才会感知它所监视 znode发生了变化。
      所以我们使用 Zookeeper 不能期望能够监控到节点每次的变化。Zookeeper 只能保证最终的一致性，而无法保证强一致性。
***** （5）注册 watcher getData、exists、getChildren
***** （6）触发 watcher create、delete、setData
***** （7）当一个客户端连接到一个新的服务器上时，watch 将会被以任意会话事件触发。当与一个服务器失去连接的时候，是无法接收到 watch 的。而当 client 重新连接时，如果需要的话，
      所有先前注册过的 watch，都会被重新注册。通常这是完全透明的。只有在一个特殊情况下，watch 可能会丢失：对于一个未创建的 znode的 exist watch，如果在客户端断开连接期间被创建了，
      并且随后在客户端连接上之前又删除了，这种情况下，这个 watch 事件可能会被丢失。
*** 客户端注册 Watcher 实现
**** （1）调用 getData()/getChildren()/exist()三个 API，传入 Watcher 对象
**** （2）标记请求 request，封装 Watcher 到 WatchRegistration
**** （3）封装成 Packet 对象，向服务端发送 request
**** （4）收到服务端响应后，将 Watcher 注册到 ZKWatcherManager 中进行管理
**** （5）请求返回，完成注册。
*** 服务端处理 Watcher 实现
**** （1）服务端接收 Watcher 并存储
***** 接收到客户端请求，处理请求判断是否需要注册 Watcher，需要的话将数据节点的节点路径和 ServerCnxn（ServerCnxn 代表一个客户端和服务端的连接，实现了 Watcher 的 process 接口，
      此时可以看成一个 Watcher 对象）存储在WatcherManager 的 WatchTable 和 watch2Paths 中去。
**** （2）Watcher 触发
***** 以服务端接收到 setData() 事务请求触发 NodeDataChanged 事件为例：
**** 2.1 封装 WatchedEvent
***** 将通知状态（SyncConnected）、事件类型（NodeDataChanged）以及节点路径封装成一个 WatchedEvent 对象
**** 2.2 查询 Watcher
***** 从 WatchTable 中根据节点路径查找 Watcher
**** 2.3 没找到；说明没有客户端在该数据节点上注册过 Watcher
**** 2.4 找到；提取并从 WatchTable 和 Watch2Paths 中删除对应 Watcher（从这里可以看出 Watcher 在服务端是一次性的，触发一次就失效了）
***** （3）调用 process 方法来触发 Watcher
***** 这里 process 主要就是通过 ServerCnxn 对应的 TCP 连接发送 Watcher 事件通知。
*** 客户端回调 Watcher
**** 客户端 SendThread 线程接收事件通知，交由 EventThread 线程回调 Watcher。
**** 客户端的 Watcher 机制同样是一次性的，一旦被触发后，该 Watcher 就失效了。
*** ACL 权限控制机制(类似linux文件系统权限控制，权限模式【ip｜Digest｜world｜super】, 权限类型【增删查写&admin进行权限管理】)
**** UGO（User/Group/Others）
**** 目前在 Linux/Unix 文件系统中使用，也是使用最广泛的权限控制方式。是一种粗粒度的文件系统权限控制模式。
**** ACL（Access Control List）访问控制列表
**** 包括三个方面：
***** 权限模式（Scheme）
***** （1）IP：从 IP 地址粒度进行权限控制
***** （2）Digest：最常用，用类似于 username:password 的权限标识来进行权限配置，便于区分不同应用来进行权限控制
***** （3）World：最开放的权限控制方式，是一种特殊的 digest 模式，只有一个权限标识“world:anyone”
***** （4）Super：超级用户
**** 授权对象
***** 授权对象指的是权限赋予的用户或一个指定实体，例如 IP 地址或是机器等。
****  权限 Permission
***** （1）CREATE：数据节点创建权限，允许授权对象在该 Znode 下创建子节点
***** （2）DELETE：子节点删除权限，允许授权对象删除该数据节点的子节点
***** （3）READ：数据节点的读取权限，允许授权对象访问该数据节点并读取其数据内容或子节点列表等
***** （4）WRITE：数据节点更新权限，允许授权对象对该数据节点进行更新操作
***** （5）ADMIN：数据节点管理权限，允许授权对象对该数据节点进行 ACL 相关设置操作
*** Chroot 特性(命名空间、类似rabbit的vhost)
**** 3.2.0 版本后，添加了 Chroot 特性，该特性允许每个客户端为自己设置一个命名空间。如果一个客户端设置了 Chroot，那么该客户端对服务器的任何操作，都将会被限制在其自己的命名空间下。
**** 通过设置 Chroot，能够将一个客户端应用于 Zookeeper 服务端的一颗子树相对应，在那些多个应用公用一个 Zookeeper 集群的场景下，对实现不同应用间的相互隔离非常有帮助。
*** 会话管理(将类似的会话放同一区块，便于不同区块隔离，会话超时算法=currentTime + sessionTimeout)
**** 分桶策略：将类似的会话放在同一区块中进行管理，以便于 Zookeeper 对会话进行不同区块的隔离处理以及同一区块的统一处理。
**** 分配原则：每个会话的“下次超时时间点”（ExpirationTime = currentTime + sessionTimeout）
**** 计算公式：
***** ExpirationTime_ = currentTime + sessionTimeout
***** ExpirationTime = (ExpirationTime_ / ExpirationInrerval + 1) *
***** ExpirationInterval , ExpirationInterval 是指 Zookeeper 会话超时检查时间间隔，默认 tickTime
*** 服务器角色(leader【事务请求的唯一调度和处理者】、follower【处理非事务请求，转发事务请求给leader，leader选举】、observer【类似follower，但不参与投票选举】)
**** Leader
***** （1）事务请求的唯一调度和处理者，保证集群事务处理的顺序性
***** （2）集群内部各服务的调度者
****  Follower
***** （1）处理客户端的非事务请求，转发事务请求给 Leader 服务器
***** （2）参与事务请求 Proposal 的投票
***** （3）参与 Leader 选举投票
****  Observer
***** （1）3.0 版本以后引入的一个服务器角色，在不影响集群事务处理能力的基础上提升集群的非事务处理能力
***** （2）处理客户端的非事务请求，转发事务请求给 Leader 服务器
***** （3）不参与任何形式的投票
*** Zookeeper 下 Server 工作状态(LOOKING【寻找leader】、FOLLOWING、LEADING、OBSERVING)【根上面的服务器类型匹配】
**** 服务器具有四种状态，分别是 LOOKING、FOLLOWING、LEADING、OBSERVING。
***** （1）LOOKING：寻 找 Leader 状态。当服务器处于该状态时，它会认为当前集群中没有 Leader，因此需要进入 Leader 选举状态。
***** （2）FOLLOWING：跟随者状态。表明当前服务器角色是 Follower。
***** （3）LEADING：领导者状态。表明当前服务器角色是 Leader。
***** （4）OBSERVING：观察者状态。表明当前服务器角色是 Observer。
*** 数据同步(leader选举=> learner向leader进行注册 => 同步数据【消息传递方式{diff【差异化同步】，trunc+diff，trunc【回滚同步】，snap【全量同步】}】)
**** 整个集群完成 Leader 选举之后，Learner（Follower 和 Observer 的统称）会向Leader 服务器进行注册。当 Learner 服务器向 Leader 服务器完成注册后，进入数据同步环节。
**** 数据同步流程：（均以消息传递的方式进行）
**** Learner 向 Learder 注册
**** 数据同步(learner向leader发送zxid，leader确认同步点，进行消息传输)
**** 同步确认
**** Zookeeper 的数据同步通常分为四类：
***** （1）直接差异化同步（DIFF 同步）
***** （2）先回滚再差异化同步（TRUNC+DIFF 同步）
***** （3）仅回滚同步（TRUNC 同步）
***** （4）全量同步（SNAP 同步）
*** 1.(leader的committedLog中的zxid包含learner的lastzxid、或learner包含leader中没有的zxid)【进行回滚+diff】2.learner的lastzxid小于mincommittedlog【全量同步】
**** peerLastZxid：
***** · 从 learner 服务器注册时发送的 ACKEPOCH 消息中提取 lastZxid（该Learner 服务器最后处理的 ZXID）
****  minCommittedLog：
***** · Leader 服务器 Proposal 缓存队列 committedLog 中最小 ZXIDmaxCommittedLog：
***** · Leader 服务器 Proposal 缓存队列 committedLog 中最大 ZXID直接差异化同步（DIFF 同步）
***** · 场景：peerLastZxid 介于 minCommittedLog 和 maxCommittedLog之间先回滚再差异化同步（TRUNC+DIFF 同步）
***** · 场景：当新的 Leader 服务器发现某个 Learner 服务器包含了一条自己没有的事务记录，那么就需要让该 Learner 服务器进行事务回滚–回滚到 Leader服务器上存在的，
      同时也是最接近于 peerLastZxid 的 ZXID仅回滚同步（TRUNC 同步）
***** · 场景：peerLastZxid 大于 maxCommittedLog
****  全量同步（SNAP 同步）
***** · 场景一：peerLastZxid 小于 minCommittedLog
***** · 场景二：Leader 服务器上没有 Proposal 缓存队列且 peerLastZxid 不等于 lastProcessZxid
*** leader存在意义（减少重复计算，提高性能）
*** zookeeper 是如何保证事务的顺序一致性的？(zxid使用64位来记录，高32位代表leader的纪元，低32位用来递增计数，产生proposal使用两阶段过程【向其他server发出事务执行请求，超半数成功才执行】)
**** zookeeper 采用了全局递增的事务 Id 来标识，所有的 proposal（提议）都在被提出的时候加上了 zxid，zxid 实际上是一个 64 位的数字，高 32 位是 epoch（ 时期; 纪元; 世; 新时代）
     用来标识 leader 周期，如果有新的 leader 产生出来，epoch会自增，低 32 位用来递增计数。当新产生 proposal 的时候，会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，
     如果超过半数的机器都能执行并且能够成功，那么就会开始执行。
*** 分布式集群中为什么会有 Master主节点？(有些业务逻辑只需要集群中的某一台机器进行执行，提高性能)
**** 在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行 leader 选举。
***  zk 节点宕机如何处理？(zk上的数据有多个副本、推荐配置不少于3台，只要超过半数机器可用就可以)
**** Zookeeper 本身也是集群，推荐配置不少于 3 个服务器。Zookeeper 自身也要保证当一个节点宕机时，其他节点会继续提供服务。
**** 如果是一个 Follower 宕机，还有 2 台服务器提供访问，因为 Zookeeper 上的数据是有多个副本的，数据并不会丢失；
**** 如果是一个 Leader 宕机，Zookeeper 会选举出新的 Leader。
**** ZK 集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在 ZK节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。
**** 所以
**** 3 个节点的 cluster 可以挂掉 1 个节点(leader 可以得到 2 票>1.5)
**** 2 个节点的 cluster 就不能挂掉任何 1 个节点了(leader 可以得到 1 票<=1)
*** zookeeper 负载均衡和 nginx 负载均衡区别(nginx快、zk可用性更好)
**** zk 的负载均衡是可以调控，nginx 只是能调权重，其他需要可控的都需要自己写插件；但是 nginx 的吞吐量比 zk 大很多，应该说按业务选择用哪种方式。
*** 集群支持动态添加机器吗？(支持不大好，需要全部重启，修改配置之后启动、逐个重启：在过半存活即可用的原则下)
**** 其实就是水平扩容了，Zookeeper 在这方面不太好。两种方式：
**** 全部重启：关闭所有 Zookeeper 服务，修改配置之后启动。不影响之前客户端的会话。
**** 逐个重启：在过半存活即可用的原则下，一台机器重启不影响整个集群对外提供服务。这是比较常用的方式。
**** 3.5 版本开始支持动态扩容。
*** Zookeeper 对节点的 watch 监听通知是永久的吗？为什么不是永久的?
**** 不是。官方声明：一个 Watch 事件是一个一次性的触发器，当被设置了 Watch的数据发生了改变的时候，则服务器将这个改变发送给设置了 Watch 的客户端，以便通知它们。
**** 为什么不是永久的，举个例子，如果服务端变动频繁，而监听的客户端很多情况下，每次变动都要通知到所有的客户端，给网络和服务器造成很大压力。
**** 一般是客户端执行 getData(“/节点 A”,true)，如果节点 A 发生了变更或删除，客户端会得到它的 watch 事件，但是在之后节点 A 又发生了变更，而客户端又没有设置 watch 事件，就不再给客户端发送。
**** 在实际应用中，很多情况下，我们的客户端不需要知道服务端的每一次变动，我只要最新的数据即可。
*** Zookeeper 的 java 客户端都有哪些？(zk自带zkclient、apache开源curator)
**** java 客户端：zk 自带的 zkclient 及 Apache 开源的 Curator。
*** chubby 是 google 的，完全实现 paxos 算法，不开源。zookeeper 是 chubby的开源实现，使用 zab 协议，paxos 算法的变种。
*** 常用命令：ls get set create delete 等。
*** ZAB特性（原子广播机制，实现方式有：恢复模式、广播模式）
*** ZAB 和 Paxos 算法的联系与区别？
**** 相同点：
***** （1）两者都存在一个类似于 Leader 进程的角色，由其负责协调多个 Follower 进程的运行
***** （2）Leader 进程都会等待超过半数的 Follower 做出正确的反馈后，才会将一个提案进行提交
***** （3）ZAB 协议中，每个 Proposal 中都包含一个 epoch 值来代表当前的 Leader周期，Paxos 中名字为 Ballot
**** 不同点：
***** ZAB 用来构建高可用的分布式数据主备系统（Zookeeper），Paxos 是用来构建分布式一致性状态机系统。
*** Zookeeper 的典型应用场景
**** Zookeeper 是一个典型的发布/订阅模式的分布式数据管理与协调框架，开发人员可以使用它来进行分布式数据的发布和订阅。
**** 通过对 Zookeeper 中丰富的数据节点进行交叉使用，配合 Watcher 事件通知机制，可以非常方便的构建一系列分布式应用中都会涉及的核心功能，如：
***** （1）数据发布/订阅
****** 介绍
******* 数据发布/订阅系统，即所谓的配置中心，顾名思义就是发布者发布数据供订阅者进行数据订阅。
****** 目的
******* 动态获取数据（配置信息）
******* 实现数据（配置信息）的集中式管理和数据的动态更新
****** 设计模式
******* Push 模式
******* Pull 模式
****** 数据（配置信息）特性
******* （1）数据量通常比较小
******* （2）数据内容在运行时会发生动态更新
******* （3）集群中各机器共享，配置一致
*******  如：机器列表信息、运行时开关配置、数据库配置信息等
******  基于 Zookeeper 的实现方式
******* · 数据存储：将数据（配置信息）存储到 Zookeeper 上的一个数据节点
******* · 数据获取：应用在启动初始化节点从 Zookeeper 数据节点读取数据，并在该节点上注册一个数据变更 Watcher
******* · 数据变更：当变更数据时，更新 Zookeeper 对应节点数据，Zookeeper会将数据变更通知发到各客户端，客户端接到通知后重新读取变更后的数据即可。
***** （2）负载均衡
****** zk 的命名服务
****** 命名服务是指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个全局的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务的地址，或者一个远程的对象等等。
***** （3）命名服务
****** 命名服务是指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个全局的路径，即是唯一的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务的地址，或者一个远程的对象等等。
***** （4）分布式协调/通知
****** 对于系统调度来说：操作人员发送通知实际是通过控制台改变某个节点的状态，然后 zk 将这些变化发送给注册了这个节点的 watcher 的所有客户端。
****** 对于执行情况汇报：每个工作进程都在某个目录下创建一个临时节点。并携带工作的进度数据，这样汇总的进程可以监控目录子节点的变化获得工作进度的实时的全局情况。
***** （5）集群管理\Master 选举
****** 所谓集群管理无在乎两点：是否有机器退出和加入、选举 master。
****** 对于第一点，所有机器约定在父目录下创建临时目录节点，然后监听父目录节点
****** 的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper 的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。
****** 新机器加入也是类似，所有机器收到通知：新兄弟目录加入，highcount 又有了，对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为 master 就好。
***** （7）分布式锁
****** 有了 zookeeper 的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。
****** 对于第一类，我们将 zookeeper 上的一个 znode 看作是一把锁，通过 createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。
       用完删除掉自己创建的 distribute_lock 节点就释放出锁。
****** 对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选 master 一样，编号最小的获得锁，用完删除，依次方便。
***** （8）分布式队列
****** 两种类型的队列：
******* （1）同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。
******* （2）队列按照 FIFO 方式进行入队和出队操作。
****** 第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。
****** 第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。在特定的目录下创建 PERSISTENT_SEQUENTIAL 节点，创建成功时Watcher 通知等待的队列，
       队列删除序列号最小的节点用以消费。此场景下Zookeeper 的 znode 用于消息存储，znode 存储的数据就是消息队列中的消息内容，SEQUENTIAL 序列号就是消息的编号，按序取出即可。
       由于创建的节点是持久化的，所以不必担心队列消息的丢失问题。
***** zk 的配置管理（文件系统、通知机制）
****** 程序分布式的部署在不同的机器上，将程序的配置信息放在 zk 的 znode 下，当有配置发生改变时，也就是 znode 发生变化时，可以通过改变 zk 中某个目录节点的内容，利用 watcher 通知给各个客户端，从而更改配置。
*** Zookeeper 都有哪些功能？(集群管理、主节点选举、分布锁、命名空间)
**** 集群管理：监控节点存活状态、运行请求等；
**** 主节点选举：主节点挂掉了之后可以从备用的节点开始新一轮选主，主节点选举说的就是这个选举的过程，使用 Zookeeper 可以协助完成这个过程；
**** 分布式锁：Zookeeper 提供两种锁：独占锁、共享锁。独占锁即一次只能有一个线程使用资源，共享锁是读锁共享，读写互斥，即可以有多线线程同时读同一个资源，如果要使用写锁也只能有一个线程使用。
     Zookeeper 可以对分布式锁进行控制。
**** 命名服务：在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。
*** 说一下 Zookeeper 的通知机制？(znode上watcher使用)
**** client 端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些 client 会收到 zk 的通知，然后 client 可以根据 znode 变化来做出业务上的改变等。
*** ZooKeeper 能够保证数据一致性主要依赖于 ZAB 协议的消息广播，崩溃恢复和数据同步三个过程。
*** Zookeeper 和 Dubbo 的关系？
**** Zookeeper的作用：
***** zookeeper用来注册服务和进行负载均衡，哪一个服务由哪一个机器来提供必需让调用者知道，简单来说就是ip地址和服务名称的对应关系。当然也可以通过硬编码的方式把这种对应关系在调用方业务代码中实现，
      但是如果提供服务的机器挂掉调用者无法知晓，如果不更改代码会继续请求挂掉的机器提供服务。zookeeper通过心跳机制可以检测挂掉的机器并将挂掉机器的ip和服务对应关系从列表中删除。至于支持高并发，
      简单来说就是横向扩展，在不更改代码的情况通过添加机器来提高运算能力。通过添加新的机器向zookeeper注册服务，服务的提供者多了能服务的客户就多了。
**** dubbo：
***** 是管理中间层的工具，在业务层到数据仓库间有非常多服务的接入和服务提供者需要调度，dubbo提供一个框架解决这个问题。
***** 注意这里的dubbo只是一个框架，至于你架子上放什么是完全取决于你的，就像一个汽车骨架，你需要配你的轮子引擎。这个框架中要完成调度必须要有一个分布式的注册中心，储存所有服务的元数据，你可以用zk，
      也可以用别的，只是大家都用zk。
**** zookeeper和dubbo的关系：
***** Dubbo 的将注册中心进行抽象，它可以外接不同的存储媒介给注册中心提供服务，有 ZooKeeper，Memcached，Redis 等。
***** 引入了 ZooKeeper 作为存储媒介，也就把 ZooKeeper 的特性引进来。首先是负载均衡，单注册中心的承载能力是有限的，在流量达到一定程度的时 候就需要分流，负载均衡就是为了分流而存在的，
      一个 ZooKeeper 群配合相应的 Web 应用就可以很容易达到负载均衡；资源同步，单单有负载均衡还不 够，节点之间的数据和资源需要同步，ZooKeeper 集群就天然具备有这样的功能；命名服务，
      将树状结构用于维护全局的服务地址列表，服务提供者在启动 的时候，向 ZooKeeper 上的指定节点 /dubbo/${serviceName}/providers 目录下写入自己的 URL 地址，这个操作就完成了服务的发布。
      其他特性还有 Mast 选举，分布式锁等。
*** zab协议：leader选择（zxid最大的=>epoch版本最高=>serverid最大）
*** leader选举过程（广播先选自己=>每个节点获取到后进行对比=>再次广播=>有节点获票超过半数）
*** zab四种状态(looking、election、discovery、synchronization、broadcast)
** TODO 18	Netty面试题（2020最新版）	https://thinkwon.blog.csdn.net/article/details/104391081
*** 总体流程：(1个boss NioEventLoopGroup包含多个NioEventLoop轮询accept事件，建立连接，生成channel，将channel注册到worker NioEventLoop的selector上、1个Worker NioEventLoopGroup进行读写事件轮询，读写channel)
**** server端包含1个Boss NioEventLoopGroup和1个Worker NioEventLoopGroup，NioEventLoopGroup相当于1个事件循环组，这个组里包含多个事件循环NioEventLoop，
     每个NioEventLoop包含1个selector和1个事件循环线程。
**** 每个Boss NioEventLoop循环执行的任务包含3步：
***** 1 轮询accept事件
***** 2 处理accept I/O事件，与Client建立连接，生成NioSocketChannel，并将NioSocketChannel注册到某个Worker NioEventLoop的Selector上
***** 3 处理任务队列中的任务，runAllTasks。任务队列中的任务包括用户调用eventloop.execute或schedule执行的任务，或者其它线程提交到该eventloop的任务。
**** 每个Worker NioEventLoop循环执行的任务包含3步：
***** 1 轮询read、write事件；
***** 2 处I/O事件，即read、write事件，在NioSocketChannel可读、可写事件发生时进行处理
***** 3 处理任务队列中的任务，runAllTasks。
***** 其中任务队列中的task有3种典型使用场景
****** 1 用户程序自定义的普通任务
****** 2 非当前reactor线程调用channel的各种方法 例如在推送系统的业务线程里面，根据用户的标识，找到对应的channel引用，然后调用write类方法向该用户推送消息，就会进入到这种场景。
       最终的write会提交到任务队列中后被异步消费。
****** 3 用户自定义定时任务
*** Netty 是什么？(异步事件驱动的网络应用框架)
**** Netty是 一个异步事件驱动的网络应用程序框架，用于快速开发可维护的高性能协议服务器和客户端。Netty是基于nio的，它封装了jdk的nio，让我们使用起来更加方法灵活。
*** Netty 的特点是什么？(高并发、传输快、封装好)
**** 高并发：Netty 是一款基于 NIO（Nonblocking IO，非阻塞IO）开发的网络通信框架，对比于 BIO（Blocking I/O，阻塞IO），他的并发性能得到了很大提高。
**** 传输快：Netty 的传输依赖于零拷贝特性，尽量减少不必要的内存拷贝，实现了更高效率的传输。
**** 封装好：Netty 封装了 NIO 操作的很多细节，提供了易于使用调用接口。
*** Netty 的优势有哪些？(使用简单、功能强大、定制能力强、性能高、稳定、社区活跃)
**** 使用简单：封装了 NIO 的很多细节，使用更简单。
**** 功能强大：预置了多种编解码功能，支持多种主流协议。
**** 定制能力强：可以通过 ChannelHandler 对通信框架进行灵活地扩展。
**** 性能高：通过与其他业界主流的 NIO 框架对比，Netty 的综合性能最优。
**** 稳定：Netty 修复了已经发现的所有 NIO 的 bug，让开发人员可以专注于业务本身。
**** 社区活跃：Netty 是活跃的开源项目，版本迭代周期短，bug 修复速度快。
*** Netty 的应用场景有哪些？(dubbo、rocketmq、zookeeper、各种代理服务器mysql proxy、redis proxy)
**** 典型的应用有：阿里分布式服务框架 Dubbo，默认使用 Netty 作为基础通信组件，还有 RocketMQ 也是使用 Netty 作为通讯的基础。
*** Netty 高性能表现在哪些方面？(同步非阻塞、内存0拷贝、内存池设计【直接内存，可重用，引用计数器】、串性处理读写、高性能序列化协议【protobuf】)
**** IO 线程模型：同步非阻塞，用最少的资源做更多的事。
**** 内存零拷贝：尽量减少不必要的内存拷贝，实现了更高效率的传输。
**** 内存池设计：申请的内存可以重用，主要指直接内存。内部实现是用一颗二叉查找树管理内存分配情况。
**** 串形化处理读写：避免使用锁带来的性能开销。
**** 高性能序列化协议：支持 protobuf 等高性能序列化协议。
*** BIO、NIO和AIO的区别？(nio基于reactor线程模型、事件驱动模型、单线程处理多任务、非阻塞io、0拷贝、基于block传输)
**** BIO：一个连接一个线程，客户端有连接请求时服务器端就需要启动一个线程进行处理。线程开销大。
**** 伪异步IO：将请求连接放入线程池，一对多，但线程还是很宝贵的资源。
**** NIO：一个请求一个线程，客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。
**** AIO：一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理，linux底层还是使用的epoll
**** BIO是面向流的，NIO是面向缓冲区的；BIO的各种流是阻塞的。而NIO是非阻塞的；BIO的Stream是单向的，而NIO的channel是双向的。
**** NIO的特点：事件驱动模型、单线程处理多任务、非阻塞I/O，I/O读写不再阻塞，而是返回0、基于block的传输比基于流的传输更高效、更高级的IO函数zero-copy、
     IO多路复用大大提高了Java网络应用的可伸缩性和实用性。基于Reactor线程模型。
**** 在Reactor模式中，事件分发器等待某个事件或者可应用多个操作的状态发生，事件分发器就把这个事件传给事先注册的事件处理函数或者回调函数，由后者来做实际的读写操作。
     如在Reactor中实现读：注册读就绪事件和相应的事件处理器、事件分发器等待事件、事件到来，激活分发器，分发器调用事件对应的处理器、事件处理器完成实际的读操作，处理读到的数据，注册新的事件，然后返还控制权。
*** [#A] NIO的组成？(buffer与channel交互、flip方法【切换读写模式】、clear方法【清除缓冲区】、rewind方法【重新过一遍缓冲区】、DirectByteBuffer使用【使用内存池管理，分配给易受基础系统的本机io影响的大型持久缓冲区】、channel读写都导致数据复制两次)
**** Buffer：与Channel进行交互，数据是从Channel读入缓冲区，从缓冲区写入Channel中的
**** flip方法 ： 反转此缓冲区，将position给limit，然后将position置为0，其实就是切换读写模式
**** clear方法 ：清除此缓冲区，将position置为0，把capacity的值给limit。
**** rewind方法 ： 重绕此缓冲区，将position置为0
**** DirectByteBuffer可减少一次系统空间到用户空间的拷贝。但Buffer创建和销毁的成本更高，不可控，通常会用内存池来提高性能。直接缓冲区主要分配给那些易受基础系统的本机I/O 操作影响的大型、持久的缓冲区。
     如果数据量比较小的中小应用情况下，可以考虑使用heapBuffer，由JVM进行管理。
**** Channel：表示 IO 源与目标打开的连接，是双向的，但不能直接访问数据，只能与Buffer 进行交互。通过源码可知，FileChannel的read方法和write方法都导致数据复制了两次！
**** Selector可使一个单独的线程管理多个Channel，open方法可创建Selector，register方法向多路复用器器注册通道，可以监听的事件类型：读、写、连接、accept。
     注册事件后会产生一个SelectionKey：它表示SelectableChannel 和Selector 之间的注册关系，wakeup方法：使尚未返回的第一个选择操作立即返回，唤醒的
     原因是：注册了新的channel或者事件；channel关闭，取消注册；优先级更高的事件触发（如定时器事件），希望及时处理。
**** Selector在Linux的实现类是EPollSelectorImpl，委托给EPollArrayWrapper实现，其中三个native方法是对epoll的封装，而EPollSelectorImpl. implRegister方法，
     通过调用epoll_ctl向epoll实例中注册事件，还将注册的文件描述符(fd)【channel】与SelectionKey的对应关系添加到fdToKey中，这个map维护了文件描述符与SelectionKey的映射。
**** fdToKey有时会变得非常大，因为注册到Selector上的Channel非常多（百万连接）；过期或失效的Channel没有及时关闭。fdToKey总是串行读取的，而读取是在select方法中进行的，该方法是非线程安全的。
**** Pipe：两个线程之间的单向数据连接，数据会被写到sink通道，从source通道读取
**** NIO的服务端建立过程：Selector.open()：打开一个Selector；ServerSocketChannel.open()：创建服务端的Channel；bind()：绑定到某个端口上。并配置非阻塞模式；
     register()：注册Channel和关注的事件到Selector上；select()轮询拿到已经就绪的事件
*** Netty的线程模型？(主从多线程模型=acceptor线程绑定断开，接收到客户端请求，将channel重主线程池的多路复用器上移除到sub线程池上)
**** Netty通过Reactor模型基于多路复用器接收并处理用户请求，内部实现了两个线程池，boss线程池和work线程池，其中boss线程池的线程负责处理请求的accept事件，
     当接收到accept事件的请求时，把对应的socket封装到一个NioSocketChannel中，并交给work线程池，其中work线程池负责请求的read和write事件，由对应的Handler处理。
**** 单线程模型：所有I/O操作都由一个线程完成，即多路复用、事件分发和处理都是在一个Reactor线程上完成的。既要接收客户端的连接请求,向服务端发起连接，又要发送/读取请求或应答/响应消息。
     一个NIO 线程同时处理成百上千的链路，性能上无法支撑，速度慢，若线程进入死循环，整个程序不可用，对于高负载、大并发的应用场景不合适。
**** 多线程模型：有一个NIO 线程（Acceptor） 只负责监听服务端，接收客户端的TCP 连接请求；NIO 线程池负责网络IO 的操作，即消息的读取、解码、编码和发送；
     1 个NIO 线程可以同时处理N 条链路，但是1 个链路只对应1 个NIO 线程，这是为了防止发生并发操作问题。但在并发百万客户端连接或需要安全认证时，一个Acceptor 线程可能会存在性能不足问题。
**** 主从多线程模型：Acceptor 线程用于绑定监听端口，接收客户端连接，将SocketChannel 从主线程池的Reactor 线程的多路复用器上移除，重新注册到Sub 线程池的线程上，用于处理I/O 的读写等操作，
     从而保证mainReactor只负责接入认证、握手等操作；
*** TCP 粘包/拆包的原因及解决方法？(写入字节大小大于套接字发送缓冲区大小，进行拆包、若相反则进行粘包，解决方法：消息定长【FixedLengthFrameDecoder类】、增加行分隔符LineBasedFrameDecoder)
**** TCP是以流的方式来处理数据，一个完整的包可能会被TCP拆分成多个包进行发送，也可能把小的封装成一个大的数据包发送。
**** TCP粘包/分包的原因：
**** 应用程序写入的字节大小大于套接字发送缓冲区的大小，会发生拆包现象，而应用程序写入数据小于套接字缓冲区大小，网卡将应用多次写入的数据发送到网络上，这将会发生粘包现象；
**** 进行MSS大小的TCP分段，当TCP报文长度-TCP头部长度>MSS的时候将发生拆包
**** 以太网帧的payload（净荷）大于MTU（1500字节）进行ip分片。
**** 解决方法
**** 消息定长：FixedLengthFrameDecoder类
**** 包尾增加特殊字符分割：
***** 行分隔符类：LineBasedFrameDecoder
***** 或自定义分隔符类 ：DelimiterBasedFrameDecoder
****  将消息分为消息头和消息体：LengthFieldBasedFrameDecoder类。分为有头部的拆包与粘包、长度字段在前且有头部的拆包与粘包、多扩展头部的拆包与粘包。
*** 什么是 Netty 的零拷贝？(使用堆外内存，相对于传统堆内存的socket读写，可以减少一次缓冲区的内存拷贝，通过transferTo方法将buffer数据发送到channel中，避免传统循环write方式导致内存拷贝问题)
**** Netty 的零拷贝主要包含三个方面：
***** Netty 的接收和发送 ByteBuffer 采用 DIRECT BUFFERS，使用堆外直接内存进行 Socket 读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存（HEAP BUFFERS）进行 Socket 读写，
      JVM 会将堆内存 Buffer 拷贝一份到直接内存中，然后才写入 Socket 中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。
***** Netty 提供了组合 Buffer 对象，可以聚合多个 ByteBuffer 对象，用户可以像操作一个 Buffer 那样方便的对组合 Buffer 进行操作，避免了传统通过内存拷贝的方式将几个小 Buffer 合并成一个大的 Buffer。
***** Netty 的文件传输采用了 transferTo 方法，它可以直接将文件缓冲区的数据发送到目标 Channel，避免了传统通过循环 write 方式导致的内存拷贝问题。
*** [#A] Netty 中有哪种重要组件？(channel、eventloop、channelfuture、channelhandler、channelpipeline)
**** Channel：Netty 网络操作抽象类，它除了包括基本的 I/O 操作，如 bind、connect、read、write 等。
**** EventLoop：主要是配合 Channel 处理 I/O 操作，用来处理连接的生命周期中所发生的事情。
**** ChannelFuture：Netty 框架中所有的 I/O 操作都为异步的，因此我们需要 ChannelFuture 的 addListener()注册一个 ChannelFutureListener 监听事件，当操作执行成功或者失败时，监听就会自动触发返回结果。
**** ChannelHandler：充当了所有处理入站和出站数据的逻辑容器。ChannelHandler 主要用来处理各种事件，这里的事件很广泛，比如可以是连接、数据接收、异常、数据转换等。
**** ChannelPipeline：为 ChannelHandler 链提供了容器，当 channel 创建时，就会被自动分配到它专属的 ChannelPipeline，这个关联是永久性的。
*** Netty 发送消息有几种方式？(直接写入channel中、写入和channelhandler绑定到channelhandlercontext中)
**** Netty 有两种发送消息的方式：
***** 直接写入 Channel 中，消息从 ChannelPipeline 当中尾部开始移动；
***** 写入和 ChannelHandler 绑定的 ChannelHandlerContext 中，消息从 ChannelPipeline 中的下一个 ChannelHandler 中移动。
*** 默认情况 Netty 起多少线程？何时启动？(cpu*2个线程、bind之后启动)
**** Netty 默认是 CPU 处理器数的两倍，bind 完之后启动。
*** 了解哪几种序列化协议？
**** 序列化（编码）是将对象序列化为二进制形式（字节数组），主要用于网络传输、数据持久化等；而反序列化（解码）则是将从网络、磁盘等读取的字节数组还原成原始对象，主要用于网络传输对象的解码，以便完成远程调用。
**** 影响序列化性能的关键因素：序列化后的码流大小（网络带宽的占用）、序列化的性能（CPU资源占用）；是否支持跨语言（异构系统的对接和开发语言切换）。
**** Java默认提供的序列化(serilizable)：无法跨语言、序列化后的码流太大、序列化的性能差
**** XML，优点：人机可读性好，可指定元素或特性的名称。缺点：序列化数据只包含数据本身以及类的结构，不包括类型标识和程序集信息；只能序列化公共属性和字段；不能序列化方法；文件庞大，文件格式复杂，传输占带宽。
     适用场景：当做配置文件存储数据，实时数据转换。
**** JSON，是一种轻量级的数据交换格式，优点：兼容性高、数据格式比较简单，易于读写、序列化后数据较小，可扩展性好，兼容性好、与XML相比，其协议比较简单，解析速度比较快。缺点：数据的描述性比XML差、
     不适合性能要求为ms级别的情况、额外空间开销比较大。适用场景（可替代ＸＭＬ）：跨防火墙访问、可调式性要求高、基于Web browser的Ajax请求、传输数据量相对小，实时性要求相对低（例如秒级别）的服务。
**** Fastjson，采用一种“假定有序快速匹配”的算法。优点：接口简单易用、目前java语言中最快的json库。缺点：过于注重快，而偏离了“标准”及功能性、代码质量不高，文档不全。适用场景：协议交互、Web输出、
     Android客户端
**** Thrift，不仅是序列化协议，还是一个RPC框架。优点：序列化后的体积小, 速度快、支持多种语言和丰富的数据类型、对于数据字段的增删具有较强的兼容性、支持二进制压缩编码。缺点：使用者较少、跨防火墙访问时，
     不安全、不具有可读性，调试代码时相对困难、不能与其他传输层协议共同使用（例如HTTP）、无法支持向持久层直接读写数据，即不适合做数据持久化序列化协议。适用场景：分布式系统的RPC解决方案
**** Avro，Hadoop的一个子项目，解决了JSON的冗长和没有IDL的问题。优点：支持丰富的数据类型、简单的动态语言结合功能、具有自我描述属性、提高了数据解析速度、快速可压缩的二进制数据形式、
     可以实现远程过程调用RPC、支持跨编程语言实现。缺点：对于习惯于静态类型语言的用户不直观。适用场景：在Hadoop中做Hive、Pig和MapReduce的持久化数据格式。
**** Protobuf，将数据结构以.proto文件进行描述，通过代码生成工具可以生成对应数据结构的POJO对象和Protobuf相关的方法和属性。优点：序列化后码流小，性能高、结构化数据存储格式（XML JSON等）、
     通过标识字段的顺序，可以实现协议的前向兼容、结构化的文档更容易管理和维护。缺点：需要依赖于工具生成代码、支持的语言相对较少，官方只支持Java 、C++ 、python。适用场景：对性能要求高的RPC调用、
     具有良好的跨防火墙的访问属性、适合应用层对象的持久化
**** 其它
***** protostuff 基于protobuf协议，但不需要配置proto文件，直接导包即可
***** Jboss marshaling 可以直接序列化java类， 无须实java.io.Serializable接口
***** Message pack 一个高效的二进制序列化格式
***** Hessian 采用二进制协议的轻量级remoting onhttp工具
***** kryo 基于protobuf协议，只支持java语言,需要注册（Registration），然后序列化（Output），反序列化（Input）
*** 如何选择序列化协议？
**** 具体场景
**** 对于公司间的系统调用，如果性能要求在100ms以上的服务，基于XML的SOAP协议是一个值得考虑的方案。
**** 基于Web browser的Ajax，以及Mobile app与服务端之间的通讯，JSON协议是首选。对于性能要求不太高，或者以动态类型语言为主，或者传输数据载荷很小的的运用场景，JSON也是非常不错的选择。
**** 对于调试环境比较恶劣的场景，采用JSON或XML能够极大的提高调试效率，降低系统开发成本。
**** 当对性能和简洁性有极高要求的场景，Protobuf，Thrift，Avro之间具有一定的竞争关系。
**** 对于T级别的数据的持久化应用场景，Protobuf和Avro是首要选择。如果持久化后的数据存储在hadoop子项目里，Avro会是更好的选择。
**** 对于持久层非Hadoop项目，以静态类型语言为主的应用场景，Protobuf会更符合静态类型语言工程师的开发习惯。由于Avro的设计理念偏向于动态类型语言，对于动态语言为主的应用场景，Avro是更好的选择。
**** 如果需要提供一个完整的RPC解决方案，Thrift是一个好的选择。
**** 如果序列化之后需要支持不同的传输层协议，或者需要跨防火墙访问的高性能场景，Protobuf可以优先考虑。
***** protobuf的数据类型有多种：bool、double、float、int32、int64、string、bytes、enum、message。protobuf的限定符：required: 必须赋值，不能为空、optional:字段可以赋值，
      也可以不赋值、repeated: 该字段可以重复任意次数（包括0次）、枚举；只能用指定的常量集中的一个值作为其值；
***** protobuf的基本规则：每个消息中必须至少留有一个required类型的字段、包含0个或多个optional类型的字段；repeated表示的字段可以包含0个或多个数据；[1,15]之内的标识号在编码的时候会占用一个字节（常用），
      [16,2047]之内的标识号则占用2个字节，标识号一定不能重复、使用消息类型，也可以将消息嵌套任意多层，可用嵌套消息类型来代替组。
*****  protobuf的消息升级原则：不要更改任何已有的字段的数值标识；不能移除已经存在的required字段，optional和repeated类型的字段可以被移除，但要保留标号不能被重用。新添加的字段必须是optional或repeated。
      因为旧版本程序无法读取或写入新增的required限定符的字段。
*****  编译器为每一个消息类型生成了一个.java文件，以及一个特殊的Builder类（该类是用来创建消息类接口的）。如：UserProto.User.Builder builder = UserProto.User.newBuilder();builder.build()；
*****  Netty中的使用：ProtobufVarint32FrameDecoder 是用于处理半包消息的解码类；ProtobufDecoder(UserProto.User.getDefaultInstance())这是创建的UserProto.java文件中的解码类；
      ProtobufVarint32LengthFieldPrepender 对protobuf协议的消息头上加上一个长度为32的整形字段，用于标志这个消息的长度的类；ProtobufEncoder 是编码类
*****  将StringBuilder转换为ByteBuf类型：copiedBuffer()方法
*** [#B] Netty 支持哪些心跳类型设置？(读超时readerIdleTime、写超时writerIdleTime、所有类型allIdleTime)
**** readerIdleTime：为读超时时间（即测试端一定时间内未接受到被测试端消息）。
**** writerIdleTime：为写超时时间（即测试端一定时间内向被测试端发送消息）。
**** allIdleTime：所有类型的超时时间。
*** Netty 和 Tomcat 的区别？
**** 作用不同：Tomcat 是 Servlet 容器，可以视为 Web 服务器，而 Netty 是异步事件驱动的网络应用程序框架和工具用于简化网络编程，例如TCP和UDP套接字服务器。
**** 协议不同：Tomcat 是基于 http 协议的 Web 服务器，而 Netty 能通过编程自定义各种协议，因为 Netty 本身自己能编码/解码字节流，所有 Netty 可以实现，HTTP 服务器、FTP 服务器、UDP 服务器、
     RPC 服务器、WebSocket 服务器、Redis 的 Proxy 服务器、MySQL 的 Proxy 服务器等等。
*** TODO [#B] NIOEventLoopGroup源码？
**** NioEventLoopGroup(其实是MultithreadEventExecutorGroup) 内部维护一个类型为 EventExecutor children [], 默认大小是处理器核数 * 2, 这样就构成了一个线程池，
     初始化EventExecutor时NioEventLoopGroup重载newChild方法，所以children元素的实际类型为NioEventLoop。
**** 线程启动时调用SingleThreadEventExecutor的构造方法，执行NioEventLoop类的run方法，首先会调用hasTasks()方法判断当前taskQueue是否有元素。如果taskQueue中有元素，执行 selectNow() 方法，
     最终执行selector.selectNow()，该方法会立即返回。如果taskQueue没有元素，执行 select(oldWakenUp) 方法
**** select ( oldWakenUp) 方法解决了 Nio 中的 bug，selectCnt 用来记录selector.select方法的执行次数和标识是否执行过selector.selectNow()，若触发了epoll的空轮询bug，
     则会反复执行selector.select(timeoutMillis)，变量selectCnt 会逐渐变大，当selectCnt 达到阈值（默认512），则执行rebuildSelector方法，进行selector重建，解决cpu占用100%的bug。
**** rebuildSelector方法先通过openSelector方法创建一个新的selector。然后将old selector的selectionKey执行cancel。最后将old selector的channel重新注册到新的selector中。
     rebuild后，需要重新执行方法selectNow，检查是否有已ready的selectionKey。
**** 接下来调用processSelectedKeys 方法（处理I/O任务），当selectedKeys != null时，调用processSelectedKeysOptimized方法，迭代 selectedKeys 获取就绪的 IO 事件的selectkey存放在
     数组selectedKeys中, 然后为每个事件都调用 processSelectedKey 来处理它，processSelectedKey 中分别处理OP_READ；OP_WRITE；OP_CONNECT事件。
**** 最后调用runAllTasks方法（非IO任务），该方法首先会调用fetchFromScheduledTaskQueue方法，把scheduledTaskQueue中已经超过延迟执行时间的任务移到taskQueue中等待被执行，
     然后依次从taskQueue中取任务执行，每执行64个任务，进行耗时检查，如果已执行时间超过预先设定的执行时间，则停止执行非IO任务，避免非IO任务太多，影响IO任务的执行。
**** 每个NioEventLoop对应一个线程和一个Selector，NioServerSocketChannel会主动注册到某一个NioEventLoop的Selector上，NioEventLoop负责事件轮询。
**** Outbound 事件都是请求事件, 发起者是 Channel，处理者是 unsafe，通过 Outbound 事件进行通知，传播方向是 tail到head。Inbound 事件发起者是 unsafe，事件的处理者是 Channel,
     是通知事件，传播方向是从头到尾。
**** 内存管理机制，首先会预申请一大块内存Arena，Arena由许多Chunk组成，而每个Chunk默认由2048个page组成。Chunk通过AVL树的形式组织Page，每个叶子节点表示一个Page，而中间节点表示内存区域，
     节点自己记录它在整个Arena中的偏移地址。当区域被分配出去后，中间节点上的标记位会被标记，这样就表示这个中间节点以下的所有节点都已被分配了。大于8k的内存分配在poolChunkList中，
     而PoolSubpage用于分配小于8k的内存，它会把一个page分割成多段，进行内存分配。
**** ByteBuf的特点：支持自动扩容（4M），保证put方法不会抛出异常、通过内置的复合缓冲类型，实现零拷贝（zero-copy）；不需要调用flip()来切换读/写模式，读取和写入索引分开；方法链；
     引用计数基于AtomicIntegerFieldUpdater用于内存回收；PooledByteBuf采用二叉树来实现一个内存池，集中管理内存的分配和释放，不用每次使用都新建一个缓冲区对象。
     UnpooledHeapByteBuf每次都会新建一个缓冲区对象。
*** Reactor线程模型(单Reactor单线程、单Reactor多线程、主从Reactor多线程)
**** Reactor是反应堆的意思，Reactor模型，是指通过一个或多个输入同时传递给服务处理器的服务请求的事件驱动处理模式。 服务端程序处理传入多路请求，并将它们同步分派给请求对应的处理线程，
     Reactor模式也叫Dispatcher模式，即I/O多路复用统一监听事件，收到事件后分发(Dispatch给某进程)，是编写高性能网络服务器的必备技术之一。
**** Reactor模型中有2个关键组成：
***** Reactor Reactor在一个单独的线程中运行，负责监听和分发事件，分发给适当的处理程序来对IO事件做出反应。 它就像公司的电话接线员，它接听来自客户的电话并将线路转移到适当的联系人
***** Handlers 处理程序执行I/O事件要完成的实际事件，类似于客户想要与之交谈的公司中的实际官员。Reactor通过调度适当的处理程序来响应I/O事件，处理程序执行非阻塞操作
**** 取决于Reactor的数量和Hanndler线程数量的不同，Reactor模型有3个变种
***** 单Reactor单线程
***** 单Reactor多线程
***** 主从Reactor多线程
**** 可以这样理解，Reactor就是一个执行while (true) { selector.select(); …}循环的线程，会源源不断的产生新的事件，称作反应堆很贴切。
*** Netty线程模型(主从Reactor多线程)
**** Netty主要基于主从Reactors多线程模型（如下图）做了一定的修改，其中主从Reactor多线程模型有多个Reactor：MainReactor和SubReactor：
***** MainReactor负责客户端的连接请求，并将请求转交给SubReactor
***** SubReactor负责相应通道的IO读写请求
***** 非IO请求（具体逻辑处理）的任务则会直接写入队列，等待worker threads进行处理
*** 模块组件(bootstrap客户端引导类，serverbootstrap服务端启动引导类，)
**** Bootstrap、ServerBootstrap
***** Bootstrap意思是引导，一个Netty应用通常由一个Bootstrap开始，主要作用是配置整个Netty程序，串联各个组件，Netty中Bootstrap类是客户端程序的启动引导类，ServerBootstrap是服务端启动引导类。
**** Future、ChannelFuture
***** 正如前面介绍，在Netty中所有的IO操作都是异步的，不能立刻得知消息是否被正确处理，但是可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过Future和ChannelFutures，
      他们可以注册一个监听，当操作执行成功或失败时监听会自动触发注册的监听事件。
**** Channel
***** Netty网络通信的组件，能够用于执行网络I/O操作。 Channel为用户提供：
****** 当前网络连接的通道的状态（例如是否打开？是否已连接？）
****** 网络连接的配置参数 （例如接收缓冲区大小）
****** 提供异步的网络I/O操作(如建立连接，读写，绑定端口)，异步调用意味着任何I / O调用都将立即返回，并且不保证在调用结束时所请求的I / O操作已完成。调用立即返回一个ChannelFuture实例，
       通过注册监听器到ChannelFuture上，可以I / O操作成功、失败或取消时回调通知调用方。
****** 支持关联I/O操作与对应的处理程序
***** 不同协议、不同的阻塞类型的连接都有不同的 Channel 类型与之对应，下面是一些常用的 Channel 类型
****** NioSocketChannel，异步的客户端 TCP Socket 连接
****** NioServerSocketChannel，异步的服务器端 TCP Socket 连接
****** NioDatagramChannel，异步的 UDP 连接
****** NioSctpChannel，异步的客户端 Sctp 连接
****** NioSctpServerChannel，异步的 Sctp 服务器端连接 这些通道涵盖了 UDP 和 TCP网络 IO以及文件 IO.
*** Selector(channel注册到selector，selector轮询channel是否有就绪io事件)
**** Netty基于Selector对象实现I/O多路复用，通过 Selector, 一个线程可以监听多个连接的Channel事件, 当向一个Selector中注册Channel 后，Selector 内部的机制就可以自动不断地查询(select)
     这些注册的Channel是否有已就绪的I/O事件(例如可读, 可写, 网络连接完成等)，这样程序就可以很简单地使用一个线程高效地管理多个 Channel 。
*** NioEventLoop(有一个线程+任务队列，线程启动调用loop.run()，执行io任务【如accept、connect、read、write】【processSelectedKey执行】和非io任务【runAllTasks】)
**** NioEventLoop中维护了一个线程和任务队列，支持异步提交执行任务，线程启动时会调用NioEventLoop的run方法，执行I/O任务和非I/O任务：
***** I/O任务 即selectionKey中ready的事件，如accept、connect、read、write等，由processSelectedKeys方法触发。
***** 非IO任务 添加到taskQueue中的任务，如register0、bind0等任务，由runAllTasks方法触发。
**** 两种任务的执行时间比由变量ioRatio控制，默认为50，则表示允许非IO任务执行的时间与IO任务的执行时间相等。
*** NioEventLoopGroup(管理loop的生命周期，相当于线程池，维护loop这个线程，一个loop负责多个channel上的事件，一个channel对应一个线程)
**** NioEventLoopGroup，主要管理eventLoop的生命周期，可以理解为一个线程池，内部维护了一组线程，每个线程(NioEventLoop)负责处理多个Channel上的事件，而一个Channel只对应于一个线程。
*** ChannelHandler(处理io事件)
**** ChannelHandler是一个接口，处理I / O事件或拦截I / O操作，并将其转发到其ChannelPipeline(业务处理链)中的下一个处理程序。
**** ChannelHandler本身并没有提供很多方法，因为这个接口有许多的方法需要实现，方便使用期间，可以继承它的子类：
***** ChannelInboundHandler用于处理入站I / O事件
***** ChannelOutboundHandler用于处理出站I / O操作
**** 或者使用以下适配器类：
***** ChannelInboundHandlerAdapter用于处理入站I / O事件
***** ChannelOutboundHandlerAdapter用于处理出站I / O操作
***** ChannelDuplexHandler用于处理入站和出站事件
*** ChannelHandlerContext(保存channel上下文，关联一个channelhandler对象)
**** 保存Channel相关的所有上下文信息，同时关联一个ChannelHandler对象
*** ChannelPipline(保存channelhandler的list，一个 Channel 包含了一个 ChannelPipeline, 而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表)
**** 保存ChannelHandler的List，用于处理或拦截Channel的入站事件和出站操作。 ChannelPipeline实现了一种高级形式的拦截过滤器模式，使用户可以完全控制事件的处理方式，
     以及Channel中各个的ChannelHandler如何相互交互。
**** 下图引用Netty的Javadoc4.1中ChannelPipline的说明，描述了ChannelPipeline中ChannelHandler通常如何处理I/O事件。 I/O事件由ChannelInboundHandler或ChannelOutboundHandler处理，
     并通过调用ChannelHandlerContext中定义的事件传播方法（例如ChannelHandlerContext.fireChannelRead（Object）和ChannelOutboundInvoker.write（Object））转发到其最近的处理程序。
**** 一个 Channel 包含了一个 ChannelPipeline, 而 ChannelPipeline 中又维护了一个由 ChannelHandlerContext 组成的双向链表, 并且每个 ChannelHandlerContext 中又关联着一个 ChannelHandler。
     入站事件和出站事件在一个双向链表中，入站事件会从链表head往后传递到最后一个入站的handler，出站事件会从链表tail往前传递到最前一个出站的handler，两种类型的handler互不干扰。
*** [#A] 总结：nioeventloopgroup【线程组】 -> 多个nioeventloop -> 一个selector -> 多个channel -> 一个channelpipline -> 多个channelhandlercontext -> 一个channelhandler + channel上下文
*** 代码
**** server
        EventLoopGroup bossGroup = new NioEventLoopGroup();
        EventLoopGroup workerGroup = new NioEventLoopGroup();
        try {
            //创建服务端的启动对象，设置参数
            ServerBootstrap bootstrap = new ServerBootstrap();
            //设置两个线程组boosGroup和workerGroup
            bootstrap.group(bossGroup, workerGroup)
                //设置服务端通道实现类型    
                .channel(NioServerSocketChannel.class)
                //设置线程队列得到连接个数    
                .option(ChannelOption.SO_BACKLOG, 128)
                //设置保持活动连接状态    
                .childOption(ChannelOption.SO_KEEPALIVE, true)
                //使用匿名内部类的形式初始化通道对象    
                .childHandler(new ChannelInitializer<SocketChannel>() {
                        @Override
                        protected void initChannel(SocketChannel socketChannel) throws Exception {
                            //给pipeline管道设置处理器
                            socketChannel.pipeline().addLast(new MyServerHandler());
                        }
                    });//给workerGroup的EventLoop对应的管道设置处理器
            System.out.println("java技术爱好者的服务端已经准备就绪...");
            //绑定端口号，启动服务端
            ChannelFuture channelFuture = bootstrap.bind(6666).sync();
            //对关闭通道进行监听
            channelFuture.channel().closeFuture().sync();
        } finally {
            bossGroup.shutdownGracefully();
            workerGroup.shutdownGracefully();
        }

public class MyServerHandler extends ChannelInboundHandlerAdapter {

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
        //获取客户端发送过来的消息
        ByteBuf byteBuf = (ByteBuf) msg;
        System.out.println("收到客户端" + ctx.channel().remoteAddress() + "发送的消息：" + byteBuf.toString(CharsetUtil.UTF_8));
    }

    @Override
    public void channelReadComplete(ChannelHandlerContext ctx) throws Exception {
        //发送消息给客户端
        ctx.writeAndFlush(Unpooled.copiedBuffer("服务端已收到消息，并给你发送一个问号?", CharsetUtil.UTF_8));
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
        //发生异常，关闭通道
        ctx.close();
    }
}

public class MyClient {

    public static void main(String[] args) throws Exception {
        NioEventLoopGroup eventExecutors = new NioEventLoopGroup();
        try {
            //创建bootstrap对象，配置参数
            Bootstrap bootstrap = new Bootstrap();
            //设置线程组
            bootstrap.group(eventExecutors)
                //设置客户端的通道实现类型    
                .channel(NioSocketChannel.class)
                //使用匿名内部类初始化通道
                .handler(new ChannelInitializer<SocketChannel>() {
                        @Override
                        protected void initChannel(SocketChannel ch) throws Exception {
                            //添加客户端通道的处理器
                            ch.pipeline().addLast(new MyClientHandler());
                        }
                    });
            System.out.println("客户端准备就绪，随时可以起飞~");
            //连接服务端
            ChannelFuture channelFuture = bootstrap.connect("127.0.0.1", 6666).sync();
            //可添加监听
            channelFuture.addListener(new ChannelFutureListener(){...})
            //对通道关闭进行监听
            channelFuture.channel().closeFuture().sync();
        } finally {
            //关闭线程组
            eventExecutors.shutdownGracefully();
        }
    }
}

public class MyClientHandler extends ChannelInboundHandlerAdapter {

    @Override
    public void channelActive(ChannelHandlerContext ctx) throws Exception {
        //发送消息到服务端
        ctx.writeAndFlush(Unpooled.copiedBuffer("歪比巴卜~茉莉~Are you good~马来西亚~", CharsetUtil.UTF_8));
    }

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
        //接收服务端发送过来的消息
        ByteBuf byteBuf = (ByteBuf) msg;
        System.out.println("收到服务端" + ctx.channel().remoteAddress() + "的消息：" + byteBuf.toString(CharsetUtil.UTF_8));
    }
}
** DONE 20 设计模式
   CLOSED: [2021-04-14 Wed 23:27]
*** 创建型模式，结构型模式，行为型模式
*** 单例模式(runtime, calendar)
*** 工厂模式(jdbc链接、线程池使用)
**** 简单（静态）工厂
***** 根据类型创建实例，switch选择要创建的类型
public class AnimalFactory {
    public static Dog createDog() {
        return new Dog();
    }
    public static Cat createCat() {
        return new Cat();
    }
    public static Animal createAnimal(String type) {
        if ("dog".equals(type)) {
            return new Dog();
        } else if ("cat".equals(type)) {
            return new Cat();
        } else {
            return null;
        }
    }
    }
**** 工厂方法
***** dogfactory、catfactory、dog、cat、factory、animal
   AnimalFactory ff = new CatFactory();
   Animal aa = ff.createAnimal();
   aa.eat();
   System.out.println("关注公众号：Java3y");
**** 抽象工厂
***** 猫狗有性别区分
***** 抽象出一个animalfactory接口，而后有maleanimalfactory和femaleanimalfactory继承
***** public class FemaleAnimalFactory implements AnimalFactory {
    // 生产母狗和母猫
    @Override
    public Animal createDog() {
        return  new FemaleDog();
    }
    @Override
    public Animal createCat() {
        return new FemaleCat();
    }
}
*** 建造者模式
**** 角色
***** Product: 最终要生成的对象，例如 Computer实例。
***** Builder： 构建者的抽象基类（有时会使用接口代替）。其定义了构建Product的抽象步骤，其实体类需要实现这些步骤。其会包含一个用来返回最终产品的方法Product getProduct()。
***** ConcreteBuilder: Builder的实现类。
***** Director: 决定如何构建最终产品的算法. 其会包含一个负责组装的方法void Construct(Builder builder)， 在这个方法中通过调用builder的方法，就可以设置builder，等设置完成后，就可以通过builder的 getProduct() 方法获得最终的产品。
**** 简单使用
public class Computer {
    private final String cpu;//必须
    private final String ram;//必须
    private final int usbCount;//可选
    private final String keyboard;//可选
    private final String display;//可选

    private Computer(Builder builder){
        this.cpu=builder.cpu;
        this.ram=builder.ram;
        this.usbCount=builder.usbCount;
        this.keyboard=builder.keyboard;
        this.display=builder.display;
    }
    public static class Builder{
        private String cpu;//必须
        private String ram;//必须
        private int usbCount;//可选
        private String keyboard;//可选
        private String display;//可选

        public Builder(String cup,String ram){
            this.cpu=cup;
            this.ram=ram;
        }

        public Builder setUsbCount(int usbCount) {
            this.usbCount = usbCount;
            return this;
        }
        public Builder setKeyboard(String keyboard) {
            this.keyboard = keyboard;
            return this;
        }
        public Builder setDisplay(String display) {
            this.display = display;
            return this;
        }        
        public Computer build(){
            return new Computer(this);
        }
    }
  //省略getter方法
}
Computer computer=new Computer.Builder("因特尔","三星")
                .setDisplay("三星24寸")
                .setKeyboard("罗技")
                .setUsbCount(2)
                .build();
**** 传统使用
***** 第一步：我们的目标Computer类：
public class Computer {
    private String cpu;//必须
    private String ram;//必须
    private int usbCount;//可选
    private String keyboard;//可选
    private String display;//可选

    public Computer(String cpu, String ram) {
        this.cpu = cpu;
        this.ram = ram;
    }
    public void setUsbCount(int usbCount) {
        this.usbCount = usbCount;
    }
    public void setKeyboard(String keyboard) {
        this.keyboard = keyboard;
    }
    public void setDisplay(String display) {
        this.display = display;
    }
    @Override
    public String toString() {
        return "Computer{" +
                "cpu='" + cpu + '\'' +
                ", ram='" + ram + '\'' +
                ", usbCount=" + usbCount +
                ", keyboard='" + keyboard + '\'' +
                ", display='" + display + '\'' +
                '}';
    }
}
***** 第二步：抽象构建者类
public abstract class ComputerBuilder {
    public abstract void setUsbCount();
    public abstract void setKeyboard();
    public abstract void setDisplay();

    public abstract Computer getComputer();
}
***** 苹果电脑构建者类
public class MacComputerBuilder extends ComputerBuilder {
    private Computer computer;
    public MacComputerBuilder(String cpu, String ram) {
        computer = new Computer(cpu, ram);
    }
    @Override
    public void setUsbCount() {
        computer.setUsbCount(2);
    }
    @Override
    public void setKeyboard() {
        computer.setKeyboard("苹果键盘");
    }
    @Override
    public void setDisplay() {
        computer.setDisplay("苹果显示器");
    }
    @Override
    public Computer getComputer() {
        return computer;
    }
}
***** 联想电脑构建者类（省略）
***** 第四步：指导者类（Director）
public class ComputerDirector {
    public void makeComputer(ComputerBuilder builder){
        builder.setUsbCount();
        builder.setDisplay();
        builder.setKeyboard();
    }
}
***** 使用
public static void main(String[] args) {
        ComputerDirector director=new ComputerDirector();//1
        ComputerBuilder builder=new MacComputerBuilder("I5处理器","三星125");//2
        director.makeComputer(builder);//3
        Computer macComputer=builder.getComputer();//4
        System.out.println("mac computer:"+macComputer.toString());

        ComputerBuilder lenovoBuilder=new LenovoComputerBuilder("I7处理器","海力士222");
        director.makeComputer(lenovoBuilder);
        Computer lenovoComputer=lenovoBuilder.getComputer();
        System.out.println("lenovo computer:"+lenovoComputer.toString());
}
*** 原型模式
**** 原型模式实际上就是实现 Cloneable 接口，重写 clone（）方法。
*** 代理模式(proxy)
**** 代理对象和被代理对象继承同一类，同时代理对象包含被代理对象实例
*** 观察者模式（行为型）
**** 观察者模式使用三个类 Subject、Observer 和 Client。Subject 对象带有绑定观察者到 Client 对象和从 Client 对象解绑观察者的方法。我们创建 Subject 类、Observer 抽象类和扩展了抽象类 Observer 的实体类。
**** public class Subject {
        private List<Observer> observers = new ArrayList<>();
        private int state;
        public int getState() {
            return state;
        }
        public void setState(int state) {
            this.state = state;
            notifyAllObservers();
        }
        public void attach(Observer observer) {
            observers.add(observer);
        }
        public void notifyAllObservers() {
            for (Observer observer : observers) {
                observer.update();
            }
        }
     
    }
*** 模版方法
**** 父类实现部分步骤，子类实现剩余步骤
public  abstract class Abstract Class {  
//模板方法，用来控制炒菜的流程 （炒菜的流程是一样的-复用）
//申明为final，不希望子类覆盖这个方法，防止更改流程的执行顺序 
        final void cookProcess(){  
        //第一步：倒油
        this.pourOil()；
        //第二步：热油
         this.HeatOil();
        //第三步：倒蔬菜
         this.pourVegetable();
        //第四步：倒调味料
         this.pourSauce（）；
        //第五步：翻炒
         this.fry();
    } 
//第一步：倒油是一样的，所以直接实现
void pourOil(){  
        System.out.println("倒油");  
    }  

//第二步：热油是一样的，所以直接实现
    void  HeatOil(){  
        System.out.println("热油");  
    }  

//第三步：倒蔬菜是不一样的（一个下包菜，一个是下菜心）
//所以声明为抽象方法，具体由子类实现 
    abstract void  pourVegetable()；

//第四步：倒调味料是不一样的（一个下辣椒，一个是下蒜蓉）
//所以声明为抽象方法，具体由子类实现 
    abstract void  pourSauce（）；


//第五步：翻炒是一样的，所以直接实现
    void fry();{  
        System.out.println("炒啊炒啊炒到熟啊");  
    }  
}
*** 装饰器模式(结构型)
**** 装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。
**** 意图：动态地给一个对象添加一些额外的职责。就增加功能来说，装饰器模式相比生成子类更为灵活。
**** 主要解决：一般的，我们为了扩展一个类经常使用继承方式实现，由于继承为类引入静态特征，并且随着扩展功能的增多，子类会很膨胀。
**** 实现
***** 我们将创建一个 Shape 接口和实现了 Shape 接口的实体类。然后我们创建一个实现了 Shape 接口的抽象装饰类 ShapeDecorator，并把 Shape 对象作为它的实例变量。
***** RedShapeDecorator 是实现了 ShapeDecorator 的实体类。
***** DecoratorPatternDemo，我们的演示类使用 RedShapeDecorator 来装饰 Shape 对象。
**** 装饰器模式强调的是增强自身，在被装饰之后你能够在被增强的类上使用增强后的功能。增强后你还是你，只不过能力更强了而已；
**** 代理模式强调要让别人帮你去做一些本身与你业务没有太多关系的职责（记录日志、设置缓存）。
*** 适配器模式(futruetask中的futuretaskadapter，将runnable转为callable调用)
**** 将一个类的接口变换成客户端所期待的另一种接口，从而使原本因接口不匹配而无法在一起工作的两个类能够在一起工作。
*** 中介者模式
**** 中介者模式适用于多个对象之间紧密耦合的情况，紧密耦合的标准是：在类图中出现了蜘蛛网状结构，即每个类都与其他的类有直接的联系。
**** 用一个中介对象封装一系列的对象交互，中介者使各对象不需要显示地相互作用，从而使其耦合松散，而且可以独立地改变它们之间的交互。
**** Mediator 抽象中介者角色
***** 抽象中介者角色定义统一的接口，用于各同事角色之间的通信。
**** Concrete Mediator 具体中介者角色
***** 具体中介者角色通过协调各同事角色实现协作行为，因此它必须依赖于各个同事角色。
**** Colleague 同事角色
***** 每一个同事角色都知道中介者角色，而且与其他的同事角色通信的时候，一定要通
过中介者角色协作。每个同事类的行为分为两种：一种是同事本身的行为，比如改
变对象本身的状态，处理自己的行为等，这种行为叫做自发行为（SelfMethod），与其他的同事类或中介者没有任何的依赖；第二种是必须依赖中介者
才能完成的行为，叫做依赖方法（Dep-Method）。
*** 命令模式
**** 在 GUI 开发中，一个按钮的点击是一个命令，可以采用命令模式；
**** 代码
public class Client {
    public static void main(String[] args) {
        //创建接收者
        Receiver receiver = new Receiver();
        //创建命令对象，设定其接收者
        Command command = new ConcreteCommand(receiver);
        //创建请求者，把命令对象设置进去
        Invoker invoker = new Invoker(command);
        //执行方法
        invoker.action();
    }
    }
*** 责任链模式(异常抛出、android点击事件)
**** 使多个对象都有机会处理请求，从而避免了请求的发送者和接受者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有对象处理它为止。
**** 抽象的处理者实现三个职责：
***** 一是定义一个请求的处理方法 handleMessage，唯一对外开放的方法；
***** 二是定义一个链的编排方法 setNext，设置下一个处理者；
***** 三是定义了具体的请求者必须实现的两个方法：定义自己能够处理的级别getHandlerLevel 和具体的处理任务 echo。
*** 策略模式
**** strategy接口
public interface Strategy {
    int calculate(int a, int b);
}
**** 具体strategy类
// 加法算法
public class AddStrategy implements Strategy{
    @Override
    public int calculate(int a, int b) {
        return a + b;
    }
}
// 减法算法
public class SubtractStrategy implements Strategy{
    @Override
    public int calculate(int a, int b) {
        return a - b;
    }
}
**** Context类
public class Context {
    private Strategy strategy;

    public Context(Strategy strategy) {
        this.strategy = strategy;
    }

    // 动态替换算法(策略)
    public void replaceStrategy(Strategy strategy) {
        this.strategy = strategy;
    }

    public int calculate(int a, int b) {
        return strategy.calculate(a, b);
    }
}
**** 测试结果
public static void main(String[] args) {
    Strategy addStrategy = new AddStrategy();
    Context context = new Context(addStrategy);
    // 输出3
    System.out.println(context.calculate(1, 2));

    Strategy subStrategy = new SubtractStrategy();
    // 动态替换算法(策略)
    context.replaceStrategy(subStrategy);
    // 输出-1
    System.out.println(context.calculate(1, 2));
}
**** 策略枚举
public enum  StrategyEnum {
    ADD {
        @Override
        public int calculate(int a, int b) {
            return a + b;
        }
    },

    SUBTRACT {
        @Override
        public int calculate(int a, int b) {
            return a - b;
        }
    };

    public abstract int calculate(int a, int b);

    public static void main(String[] args) {
        // 3
        int addResult = StrategyEnum.ADD.calculate(1, 2);
        System.out.println(addResult);

        // -1
        int subResult = StrategyEnum.SUBTRACT.calculate(1, 2);
        System.out.println(subResult);

    }
}
*** 迭代器模式
**** 它提供一种方法访问一个容器对象中各个元素，而又不需暴露该对象的内部细节。
**** 迭代器模式已经被淘汰，java 中已经把迭代器运用到各个聚集类
*** 组合模式
**** 将对象组合成树形结构以表示“部分-整体”的层次结构，使得用户对单个对象和组合对象的使用具有一致性。
**** 代码
public class Composite extends Component {
//构件容器
private ArrayList<Component> componentArrayList = new
ArrayList<Component>();
//增加一个叶子构件或树枝构件
public void add(Component component){
this.componentArrayList.add(component);
}
//删除一个叶子构件或树枝构件
public void remove(Component component){
this.componentArrayList.remove(component);
}
//获得分支下的所有叶子构件和树枝构件
public ArrayList<Component> getChildren(){
return this.componentArrayList;
} }
